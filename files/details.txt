
# ------------------------------------------------------------------ #
       ##### Recommender System 2019 Challenge Polimi #####
# ------------------------------------------------------------------ #

# App domain: Online store. The dataset contains 4 months of transactions collected from an online supermarket.
# ----------


# Train-test split : leave-one-out
# --------------------------------
            - Sampling one random interaction from each user profile (remove one random interaction)
            - Therefore there is only one positive interaction for each user in the test set

            ?? for every user one item-interaction is randomly selected to be part of the test set.

            ==> leaveone-out technique to obtain the training set and test set,
            which means for every user, there is a test item her has not
            interacted with?


# Goal: build a top-10 recommender
# --------------------------------
- recommend a list of 10 potentially relevant items for each user (discover which item a user will interact with.)


# Evaluation metric: MAP@10
# -------------------------
- Compared the performance of our proposed recommender with 10 baselines


Any kind of recommender algorithm you wish (e.g., collaborative-filtering, content-based, hybrid, etc.) written in Python or R
# ----------------------------------------


# Dataset description:
# -------------------
each file contains coordinates and the values of the non-zero cells of the sparse matrix
that represent the relative information


# Interactions files (URM)
# ------------------------

###### data_train.csv ######  training set describing interactions

- implicit ratings: what items a user purchased
    # user-item pair means the user interacted with the item (the user did buy a product)

- contains both cold items and cold users


    row       |     col       |       data
______________|_______________|__________________

  user_id        item_id        value of the preference (rating)



# Item content files (ICM): additional info about items
# ------------------------

The values of asset and sub-class refer to a categorization of the items:

###### data_ICM_asset.csv ######  description of the item (id)
###### data_ICM_sub_class.csv ######  categorization of the item (number)

    row       |     col       |       data
______________|_______________|__________________

   item_id       feature_id     value of the cell

    asset       |     prize       |       subclass
________________|_________________|__________________

  value of the cell


###### data_ICM_price.csv ######  price of each item (already normalized)


# User content files (User Content Matrix)  additional info about users (demographics)
# ----------------------------------------

###### data_UCM_age.csv ######  already normalized
###### data_UCM_region.csv ######  one-hot encoding of the user's region

    row       |     col       |       data
______________|_______________|__________________

   user_id       feature_id     value of the cell


###### data_target_users_test.csv ######
# --------------------------------------

This file contains the list of users that should appear in your submission file (users in the test data)



###### alg_sample_submission.csv ######  submission file
# ------------------------------------------------------

HEADERS [user_id],[items_list]

Each row is a user [user_id] , [ordered list of 10 recommended items separated by a space]

- Order is important in MAP: The items are ordered by relevance (most important first)
                             Different orders may provide different MAP



# Important: avoid overfitting the public test set
# ------------------------------------------------

- Download train set and perform local and split train/test locally before making a submission
- compare local score with score given on the public test. If there are close,
    the model is not overfitting the local test set

    - the difference in the leaderboard MAP vs the MAP of the runs against my test set is due to the
    sampling strategy for splitting training and local test sets



# Cold items, cold users and cold features
# ----------------------------------------

# NOTE:
# Usually to deal with cold items you use a content-collaborative hybrid
# URM, ICM = masks.refactor_URM_ICM(URM, ICM)

# The issue with cold users and cold items is that a personalized collaborative recommender
# is not able to model them.
# Even if you train a model like a matrix factorization or even an itemKNN and
# you get a recommendation list, those are just random recommendations.

# In order to overcome this, you have to look for other models that allow you to provide
# a meaningful result for cold items and users (e.g., TopPop, content-based â€¦)
# and build a hybrid. The easiest solution is to average those models or to switch among
# them depending on the user of interest. You may find some hints in the practice sessions for
# hybrid and collaborative boosted FW