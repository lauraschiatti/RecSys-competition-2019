{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset/data_ICM_sub_class.csv\n",
      "/kaggle/input/dataset/data_target_users_test.csv\n",
      "/kaggle/input/dataset/data_ICM_asset.csv\n",
      "/kaggle/input/dataset/data_UCM_age.csv\n",
      "/kaggle/input/dataset/data_train.csv\n",
      "/kaggle/input/dataset/data_UCM_region.csv\n",
      "/kaggle/input/dataset/data_ICM_price.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "dataset_dir = \"../input/dataset/\"\n",
    "\n",
    "# Interactions files (URM)\n",
    "data_train = dataset_dir + \"data_train.csv\"\n",
    "data_target_users = dataset_dir + \"data_target_users_test.csv\"\n",
    "\n",
    "# Item content files (ICM)\n",
    "data_ICM_asset = dataset_dir + \"/data_ICM_asset.csv\"  # description of the item (id)\n",
    "data_ICM_price = dataset_dir + \"/data_ICM_price.csv\"  # price of each item (already normalized)\n",
    "data_ICM_sub_class = dataset_dir + \"/data_ICM_sub_class.csv\"  # categorization of the item (number)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# data_manager.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# global vars\n",
    "user_list = []\n",
    "item_list = []\n",
    "n_interactions = 0\n",
    "n_users = 0\n",
    "n_items = 0\n",
    "n_subclass = 0\n",
    "\n",
    "\n",
    "def build_URM():\n",
    "    global user_list, item_list, n_interactions\n",
    "\n",
    "    matrix_tuples = []\n",
    "\n",
    "    with open(data_train, 'r') as file:  # read file's content\n",
    "        next(file)  # skip header row\n",
    "        for line in file:\n",
    "            if len(line.strip()) != 0:  # ignore lines with only whitespace\n",
    "                n_interactions += 1\n",
    "\n",
    "                # Create a tuple for each interaction (line in the file)\n",
    "                matrix_tuples.append(row_split(line))\n",
    "\n",
    "    # Separate user_id, item_id and rating\n",
    "    user_list, item_list, rating_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n",
    "\n",
    "    # Create lists of all users, items and contents (ratings)\n",
    "    user_list = list(user_list)  # row\n",
    "    item_list = list(item_list)  # col\n",
    "    rating_list = list(rating_list)  # data\n",
    "\n",
    "    URM = csr_sparse_matrix(rating_list, user_list, item_list)\n",
    "    \n",
    "    print(\"URM built!\")\n",
    "\n",
    "    return URM\n",
    "\n",
    "def build_ICM():\n",
    "    # features = [‘asset’, ’price’, ’subclass’] info about products\n",
    "    global n_subclass\n",
    "\n",
    "    # Load subclass data\n",
    "    matrix_tuples = []\n",
    "\n",
    "    with open(data_ICM_sub_class, 'r') as file:  # read file's content\n",
    "        next(file)  # skip header row\n",
    "        for line in file:\n",
    "            n_subclass += 1\n",
    "\n",
    "            # Create a tuple for each interaction (line in the file)\n",
    "            matrix_tuples.append(row_split(line))\n",
    "\n",
    "    # Separate user_id, item_id and rating\n",
    "    item_list, class_list, col_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n",
    "\n",
    "    # Convert values to list# Create lists of all users, items and contents (ratings)\n",
    "    item_list_icm = list(item_list)\n",
    "    class_list_icm = list(class_list)\n",
    "    col_list_icm = np.zeros(len(col_list))\n",
    "\n",
    "    # Number of items that are in the subclass list\n",
    "    num_items = max(item_list_icm) + 1\n",
    "    ICM_shape = (num_items, 1)\n",
    "    ICM_subclass = csr_sparse_matrix(class_list_icm, item_list_icm, col_list_icm, shape=ICM_shape)\n",
    "\n",
    "    # Load price data\n",
    "    matrix_tuples = []\n",
    "    n_prices = 0\n",
    "\n",
    "    with open(data_ICM_price, 'r') as file:  # read file's content\n",
    "        next(file)  # skip header row\n",
    "        for line in file:\n",
    "            n_prices += 1\n",
    "\n",
    "            # Create a tuple for each interaction (line in the file)\n",
    "            matrix_tuples.append(row_split(line))\n",
    "\n",
    "    # Separate user_id, item_id and rating\n",
    "    item_list, col_list, price_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n",
    "\n",
    "    # Convert values to list# Create lists of all users, items and contents (ratings)\n",
    "    item_list_icm = list(item_list)\n",
    "    col_list_icm = list(col_list)\n",
    "    price_list_icm = list(price_list)\n",
    "\n",
    "    ICM_price = csr_sparse_matrix(price_list_icm, item_list_icm, col_list_icm)\n",
    "\n",
    "    # Load asset data\n",
    "    matrix_tuples = []\n",
    "    n_assets = 0\n",
    "\n",
    "    with open(data_ICM_asset, 'r') as file:  # read file's content\n",
    "        next(file)  # skip header row\n",
    "        for line in file:\n",
    "            n_assets += 1\n",
    "\n",
    "            # Create a tuple for each interaction (line in the file)\n",
    "            matrix_tuples.append(row_split(line))\n",
    "\n",
    "    # Separate user_id, item_id and rating\n",
    "    item_list, col_list, asset_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n",
    "\n",
    "    # Convert values to list# Create lists of all users, items and contents (ratings)\n",
    "    item_list_icm = list(item_list)\n",
    "    col_list_icm = list(col_list)\n",
    "    asset_list_icm = list(asset_list)\n",
    "\n",
    "    ICM_asset = csr_sparse_matrix(asset_list_icm, item_list_icm, col_list_icm)\n",
    "\n",
    "    ICM_all = sps.hstack([ICM_price, ICM_asset, ICM_subclass], format='csr')\n",
    "\n",
    "    # item_feature_ratios(ICM_all)\n",
    "\n",
    "    print(\"ICM built!\\n\")\n",
    "\n",
    "    return ICM_all\n",
    "\n",
    "def row_split(row_string):\n",
    "    # file format: 0,3568,1.0\n",
    "\n",
    "    split = row_string.split(\",\")\n",
    "    split[2] = split[2].replace(\"\\n\", \"\")\n",
    "\n",
    "    split[0] = int(split[0])\n",
    "    split[1] = int(split[1])\n",
    "    split[2] = float(split[2])  # rating is a float\n",
    "\n",
    "    result = tuple(split)\n",
    "    return result\n",
    "\n",
    "def csr_sparse_matrix(data, row, col, shape=None):\n",
    "    csr_matrix = sps.coo_matrix((data, (row, col)), shape=shape)\n",
    "    csr_matrix = csr_matrix.tocsr()\n",
    "\n",
    "    return csr_matrix\n",
    "\n",
    "\n",
    "def get_target_users():\n",
    "    target_user_id_list = []\n",
    "\n",
    "    with open(data_target_users, 'r') as file:  # read file's content\n",
    "        next(file)  # skip header row\n",
    "        for line in file:\n",
    "            # each line is a user_id\n",
    "            target_user_id_list.append(int(line.strip()))  # remove trailing space\n",
    "\n",
    "    return target_user_id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Train, validation and test splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_splitter.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Random holdout split: take interactions randomly\n",
    "# and do not care about which users were involved in that interaction\n",
    "\n",
    "def split_train_validation_random_holdout(URM, train_split):\n",
    "    number_interactions = URM.nnz  # number of nonzero values\n",
    "    URM = URM.tocoo()  # Coordinate list matrix (COO)\n",
    "    shape = URM.shape\n",
    "\n",
    "    #  URM.row: user_list, URM.col: item_list, URM.data: rating_list\n",
    "\n",
    "    # Sampling strategy: take random samples of data using a boolean mask\n",
    "    train_mask = np.random.choice(\n",
    "        [True, False],\n",
    "        number_interactions,\n",
    "        p=[train_split, 1 - train_split])  # train_perc for True, 1-train_perc for False\n",
    "\n",
    "    URM_train = csr_sparse_matrix(URM.data[train_mask],\n",
    "                                  URM.row[train_mask],\n",
    "                                  URM.col[train_mask],\n",
    "                                  shape=shape)\n",
    "\n",
    "    test_mask = np.logical_not(train_mask)  # remaining samples\n",
    "    URM_test = csr_sparse_matrix(URM.data[test_mask],\n",
    "                                 URM.row[test_mask],\n",
    "                                 URM.col[test_mask],\n",
    "                                 shape=shape)\n",
    "\n",
    "    return URM_train, URM_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Performance evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 26/06/18\n",
    "\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import time, sys, copy\n",
    "\n",
    "from enum import Enum\n",
    "# from utils.Evaluation.Utils.seconds_to_biggest_unit import seconds_to_biggest_unit\n",
    "\n",
    "# from utils.Evaluation.metrics import roc_auc, precision, precision_recall_min_denominator, recall, MAP, MRR, ndcg, arhr, \\\n",
    "#     rmse, \\\n",
    "#     Novelty, Coverage_Item, Metrics_Object, Coverage_User, Gini_Diversity, Shannon_Entropy, Diversity_MeanInterList, \\\n",
    "#     Diversity_Herfindahl, AveragePopularity\n",
    "\n",
    "\n",
    "class EvaluatorMetrics(Enum):\n",
    "    ROC_AUC = \"ROC_AUC\"\n",
    "    PRECISION = \"PRECISION\"\n",
    "    PRECISION_RECALL_MIN_DEN = \"PRECISION_RECALL_MIN_DEN\"\n",
    "    RECALL = \"RECALL\"\n",
    "    MAP = \"MAP\"\n",
    "    MRR = \"MRR\"\n",
    "    NDCG = \"NDCG\"\n",
    "    F1 = \"F1\"\n",
    "    HIT_RATE = \"HIT_RATE\"\n",
    "    ARHR = \"ARHR\"\n",
    "    RMSE = \"RMSE\"\n",
    "    NOVELTY = \"NOVELTY\"\n",
    "    AVERAGE_POPULARITY = \"AVERAGE_POPULARITY\"\n",
    "    DIVERSITY_SIMILARITY = \"DIVERSITY_SIMILARITY\"\n",
    "    DIVERSITY_MEAN_INTER_LIST = \"DIVERSITY_MEAN_INTER_LIST\"\n",
    "    DIVERSITY_HERFINDAHL = \"DIVERSITY_HERFINDAHL\"\n",
    "    COVERAGE_ITEM = \"COVERAGE_ITEM\"\n",
    "    COVERAGE_USER = \"COVERAGE_USER\"\n",
    "    DIVERSITY_GINI = \"DIVERSITY_GINI\"\n",
    "    SHANNON_ENTROPY = \"SHANNON_ENTROPY\"\n",
    "\n",
    "\n",
    "def create_empty_metrics_dict(n_items, n_users, URM_train, ignore_items, ignore_users, cutoff,\n",
    "                              diversity_similarity_object):\n",
    "    empty_dict = {}\n",
    "\n",
    "    # from Base.Evaluation.ResultMetric import ResultMetric\n",
    "    # empty_dict = ResultMetric()\n",
    "\n",
    "    for metric in EvaluatorMetrics:\n",
    "        if metric == EvaluatorMetrics.COVERAGE_ITEM:\n",
    "            empty_dict[metric.value] = Coverage_Item(n_items, ignore_items)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.DIVERSITY_GINI:\n",
    "            empty_dict[metric.value] = Gini_Diversity(n_items, ignore_items)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.SHANNON_ENTROPY:\n",
    "            empty_dict[metric.value] = Shannon_Entropy(n_items, ignore_items)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.COVERAGE_USER:\n",
    "            empty_dict[metric.value] = Coverage_User(n_users, ignore_users)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.DIVERSITY_MEAN_INTER_LIST:\n",
    "            empty_dict[metric.value] = Diversity_MeanInterList(n_items, cutoff)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.DIVERSITY_HERFINDAHL:\n",
    "            empty_dict[metric.value] = Diversity_Herfindahl(n_items, ignore_items)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.NOVELTY:\n",
    "            empty_dict[metric.value] = Novelty(URM_train)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.AVERAGE_POPULARITY:\n",
    "            empty_dict[metric.value] = AveragePopularity(URM_train)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.MAP:\n",
    "            empty_dict[metric.value] = MAP()\n",
    "\n",
    "        elif metric == EvaluatorMetrics.MRR:\n",
    "            empty_dict[metric.value] = MRR()\n",
    "\n",
    "        elif metric == EvaluatorMetrics.DIVERSITY_SIMILARITY:\n",
    "            if diversity_similarity_object is not None:\n",
    "                empty_dict[metric.value] = copy.deepcopy(diversity_similarity_object)\n",
    "        else:\n",
    "            empty_dict[metric.value] = 0.0\n",
    "\n",
    "    return empty_dict\n",
    "\n",
    "\n",
    "def get_result_string(results_run, n_decimals=7):\n",
    "    output_str = \"\"\n",
    "\n",
    "    for cutoff in results_run.keys():\n",
    "\n",
    "        results_run_current_cutoff = results_run[cutoff]\n",
    "\n",
    "        output_str += \"CUTOFF: {} - \".format(cutoff)\n",
    "\n",
    "        for metric in results_run_current_cutoff.keys():\n",
    "            output_str += \"{}: {:.{n_decimals}f}, \".format(metric, results_run_current_cutoff[metric],\n",
    "                                                           n_decimals=n_decimals)\n",
    "        output_str += \"\\n\"\n",
    "\n",
    "    return output_str\n",
    "\n",
    "\n",
    "def _remove_item_interactions(URM, item_list):\n",
    "    URM = sps.csc_matrix(URM.copy())\n",
    "\n",
    "    for item_index in item_list:\n",
    "        start_pos = URM.indptr[item_index]\n",
    "        end_pos = URM.indptr[item_index + 1]\n",
    "\n",
    "        URM.data[start_pos:end_pos] = np.zeros_like(URM.data[start_pos:end_pos])\n",
    "\n",
    "    URM.eliminate_zeros()\n",
    "    URM = sps.csr_matrix(URM)\n",
    "\n",
    "    return URM\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "    \"\"\"Abstract Evaluator\"\"\"\n",
    "\n",
    "    EVALUATOR_NAME = \"Evaluator_Base_Class\"\n",
    "\n",
    "    def __init__(self, URM_test_list, cutoff_list, minRatingsPerUser=1, exclude_seen=True,\n",
    "                 diversity_object=None,\n",
    "                 ignore_items=None,\n",
    "                 ignore_users=None,\n",
    "                 verbose=True):\n",
    "\n",
    "        super(Evaluator, self).__init__()\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if ignore_items is None:\n",
    "            self.ignore_items_flag = False\n",
    "            self.ignore_items_ID = np.array([])\n",
    "        else:\n",
    "            self._print(\"Ignoring {} Items\".format(len(ignore_items)))\n",
    "            self.ignore_items_flag = True\n",
    "            self.ignore_items_ID = np.array(ignore_items)\n",
    "\n",
    "        self.cutoff_list = cutoff_list.copy()\n",
    "        self.max_cutoff = max(self.cutoff_list)\n",
    "\n",
    "        self.minRatingsPerUser = minRatingsPerUser\n",
    "        self.exclude_seen = exclude_seen\n",
    "\n",
    "        if not isinstance(URM_test_list, list):\n",
    "            self.URM_test = URM_test_list.copy()\n",
    "            URM_test_list = [URM_test_list]\n",
    "        else:\n",
    "            raise ValueError(\"List of URM_test not supported\")\n",
    "\n",
    "        self.diversity_object = diversity_object\n",
    "\n",
    "        self.n_users, self.n_items = URM_test_list[0].shape\n",
    "\n",
    "        # Prune users with an insufficient number of ratings\n",
    "        # During testing CSR is faster\n",
    "        self.URM_test_list = []\n",
    "        usersToEvaluate_mask = np.zeros(self.n_users, dtype=np.bool)\n",
    "\n",
    "        for URM_test in URM_test_list:\n",
    "            URM_test = _remove_item_interactions(URM_test, self.ignore_items_ID)\n",
    "\n",
    "            URM_test = sps.csr_matrix(URM_test)\n",
    "            self.URM_test_list.append(URM_test)\n",
    "\n",
    "            rows = URM_test.indptr\n",
    "            numRatings = np.ediff1d(rows)\n",
    "            new_mask = numRatings >= minRatingsPerUser\n",
    "\n",
    "            usersToEvaluate_mask = np.logical_or(usersToEvaluate_mask, new_mask)\n",
    "\n",
    "        self.usersToEvaluate = np.arange(self.n_users)[usersToEvaluate_mask]\n",
    "\n",
    "        if ignore_users is not None:\n",
    "            self._print(\"Ignoring {} Users\".format(len(ignore_users)))\n",
    "            self.ignore_users_ID = np.array(ignore_users)\n",
    "            self.usersToEvaluate = set(self.usersToEvaluate) - set(ignore_users)\n",
    "        else:\n",
    "            self.ignore_users_ID = np.array([])\n",
    "\n",
    "        self.usersToEvaluate = list(self.usersToEvaluate)\n",
    "\n",
    "    def _print(self, string):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"{}: {}\".format(self.EVALUATOR_NAME, string))\n",
    "\n",
    "    def evaluateRecommender(self, recommender_object):\n",
    "        \"\"\"\n",
    "        :param recommender_object: the trained recommender object, a BaseRecommender subclass\n",
    "        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n",
    "        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"The method evaluateRecommender not implemented for this evaluator class\")\n",
    "\n",
    "    def get_user_relevant_items(self, user_id):\n",
    "\n",
    "        assert self.URM_test.getformat() == \"csr\", \"Evaluator_Base_Class: URM_test is not CSR, this will cause errors in getting relevant items\"\n",
    "\n",
    "        return self.URM_test.indices[self.URM_test.indptr[user_id]:self.URM_test.indptr[user_id + 1]]\n",
    "\n",
    "    def get_user_test_ratings(self, user_id):\n",
    "\n",
    "        assert self.URM_test.getformat() == \"csr\", \"Evaluator_Base_Class: URM_test is not CSR, this will cause errors in relevant items ratings\"\n",
    "\n",
    "        return self.URM_test.data[self.URM_test.indptr[user_id]:self.URM_test.indptr[user_id + 1]]\n",
    "\n",
    "\n",
    "class EvaluatorHoldout(Evaluator):\n",
    "    \"\"\"EvaluatorHoldout\"\"\"\n",
    "\n",
    "    EVALUATOR_NAME = \"EvaluatorHoldout\"\n",
    "\n",
    "    def __init__(self, URM_test_list, cutoff_list, minRatingsPerUser=1, exclude_seen=True,\n",
    "                 diversity_object=None,\n",
    "                 ignore_items=None,\n",
    "                 ignore_users=None,\n",
    "                 verbose=True):\n",
    "\n",
    "        super(EvaluatorHoldout, self).__init__(URM_test_list, cutoff_list,\n",
    "                                               diversity_object=diversity_object,\n",
    "                                               minRatingsPerUser=minRatingsPerUser, exclude_seen=exclude_seen,\n",
    "                                               ignore_items=ignore_items, ignore_users=ignore_users,\n",
    "                                               verbose=verbose)\n",
    "\n",
    "    def _run_evaluation_on_selected_users(self, recommender_object, usersToEvaluate, block_size=None):\n",
    "\n",
    "        if block_size is None:\n",
    "            block_size = min(1000, int(1e8 / self.n_items))\n",
    "            block_size = min(block_size, len(usersToEvaluate))\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_print = time.time()\n",
    "\n",
    "        results_dict = {}\n",
    "\n",
    "        for cutoff in self.cutoff_list:\n",
    "            results_dict[cutoff] = create_empty_metrics_dict(self.n_items, self.n_users,\n",
    "                                                             recommender_object.get_URM_train(),\n",
    "                                                             self.ignore_items_ID,\n",
    "                                                             self.ignore_users_ID,\n",
    "                                                             cutoff,\n",
    "                                                             self.diversity_object)\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.set_items_to_ignore(self.ignore_items_ID)\n",
    "\n",
    "        n_users_evaluated = 0\n",
    "\n",
    "        # Start from -block_size to ensure it to be 0 at the first block\n",
    "        user_batch_start = 0\n",
    "        user_batch_end = 0\n",
    "\n",
    "        while user_batch_start < len(usersToEvaluate):\n",
    "\n",
    "            user_batch_end = user_batch_start + block_size\n",
    "            user_batch_end = min(user_batch_end, len(usersToEvaluate))\n",
    "\n",
    "            test_user_batch_array = np.array(usersToEvaluate[user_batch_start:user_batch_end])\n",
    "            user_batch_start = user_batch_end\n",
    "\n",
    "            # Compute predictions for a batch of users using vectorization, much more efficient than computing it one at a time\n",
    "            recommended_items_batch_list, scores_batch = recommender_object.recommend(test_user_batch_array,\n",
    "                                                                                      remove_seen_flag=self.exclude_seen,\n",
    "                                                                                      cutoff=self.max_cutoff,\n",
    "                                                                                      remove_top_pop_flag=False,\n",
    "                                                                                      remove_custom_items_flag=self.ignore_items_flag,\n",
    "                                                                                      return_scores=True\n",
    "                                                                                      )\n",
    "\n",
    "            assert len(recommended_items_batch_list) == len(\n",
    "                test_user_batch_array), \"{}: recommended_items_batch_list contained recommendations for {} users, expected was {}\".format(\n",
    "                self.EVALUATOR_NAME, len(recommended_items_batch_list), len(test_user_batch_array))\n",
    "\n",
    "            assert scores_batch.shape[0] == len(\n",
    "                test_user_batch_array), \"{}: scores_batch contained scores for {} users, expected was {}\".format(\n",
    "                self.EVALUATOR_NAME, scores_batch.shape[0], len(test_user_batch_array))\n",
    "\n",
    "            assert scores_batch.shape[\n",
    "                       1] == self.n_items, \"{}: scores_batch contained scores for {} items, expected was {}\".format(\n",
    "                self.EVALUATOR_NAME, scores_batch.shape[1], self.n_items)\n",
    "\n",
    "            # Compute recommendation quality for each user in batch\n",
    "            for batch_user_index in range(len(recommended_items_batch_list)):\n",
    "\n",
    "                test_user = test_user_batch_array[batch_user_index]\n",
    "\n",
    "                relevant_items = self.get_user_relevant_items(test_user)\n",
    "                relevant_items_rating = self.get_user_test_ratings(test_user)\n",
    "\n",
    "                all_items_predicted_ratings = scores_batch[batch_user_index]\n",
    "                user_rmse = rmse(all_items_predicted_ratings, relevant_items, relevant_items_rating)\n",
    "\n",
    "                # Being the URM CSR, the indices are the non-zero column indexes\n",
    "                recommended_items = recommended_items_batch_list[batch_user_index]\n",
    "                is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "                n_users_evaluated += 1\n",
    "\n",
    "                for cutoff in self.cutoff_list:\n",
    "\n",
    "                    results_current_cutoff = results_dict[cutoff]\n",
    "\n",
    "                    is_relevant_current_cutoff = is_relevant[0:cutoff]\n",
    "                    recommended_items_current_cutoff = recommended_items[0:cutoff]\n",
    "\n",
    "                    results_current_cutoff[EvaluatorMetrics.ROC_AUC.value] += roc_auc(is_relevant_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.PRECISION.value] += precision(is_relevant_current_cutoff)\n",
    "                    results_current_cutoff[\n",
    "                        EvaluatorMetrics.PRECISION_RECALL_MIN_DEN.value] += precision_recall_min_denominator(\n",
    "                        is_relevant_current_cutoff, len(relevant_items))\n",
    "                    results_current_cutoff[EvaluatorMetrics.RECALL.value] += recall(is_relevant_current_cutoff,\n",
    "                                                                                    relevant_items)\n",
    "                    results_current_cutoff[EvaluatorMetrics.NDCG.value] += ndcg(recommended_items_current_cutoff,\n",
    "                                                                                relevant_items,\n",
    "                                                                                relevance=self.get_user_test_ratings(\n",
    "                                                                                    test_user), at=cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.HIT_RATE.value] += is_relevant_current_cutoff.sum()\n",
    "                    results_current_cutoff[EvaluatorMetrics.ARHR.value] += arhr(is_relevant_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.RMSE.value] += user_rmse\n",
    "\n",
    "                    results_current_cutoff[EvaluatorMetrics.MRR.value].add_recommendations(is_relevant_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.MAP.value].add_recommendations(is_relevant_current_cutoff,\n",
    "                                                                                           relevant_items)\n",
    "                    results_current_cutoff[EvaluatorMetrics.NOVELTY.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.AVERAGE_POPULARITY.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.DIVERSITY_GINI.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.SHANNON_ENTROPY.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.COVERAGE_ITEM.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.COVERAGE_USER.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff, test_user)\n",
    "                    results_current_cutoff[EvaluatorMetrics.DIVERSITY_MEAN_INTER_LIST.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.DIVERSITY_HERFINDAHL.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "\n",
    "                    if EvaluatorMetrics.DIVERSITY_SIMILARITY.value in results_current_cutoff:\n",
    "                        results_current_cutoff[EvaluatorMetrics.DIVERSITY_SIMILARITY.value].add_recommendations(\n",
    "                            recommended_items_current_cutoff)\n",
    "\n",
    "                if time.time() - start_time_print > 30 or n_users_evaluated == len(self.usersToEvaluate):\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n",
    "\n",
    "                    self._print(\"Processed {} ( {:.2f}% ) in {:.2f} {}. Users per second: {:.0f}\".format(\n",
    "                        n_users_evaluated,\n",
    "                        100.0 * float(n_users_evaluated) / len(self.usersToEvaluate),\n",
    "                        new_time_value, new_time_unit,\n",
    "                        float(n_users_evaluated) / elapsed_time))\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "                    sys.stderr.flush()\n",
    "\n",
    "                    start_time_print = time.time()\n",
    "\n",
    "        return results_dict, n_users_evaluated\n",
    "\n",
    "    def evaluateRecommender(self, recommender_object):\n",
    "        \"\"\"\n",
    "        :param recommender_object: the trained recommender object, a BaseRecommender subclass\n",
    "        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n",
    "        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n",
    "        \"\"\"\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.set_items_to_ignore(self.ignore_items_ID)\n",
    "\n",
    "        results_dict, n_users_evaluated = self._run_evaluation_on_selected_users(recommender_object,\n",
    "                                                                                 self.usersToEvaluate)\n",
    "\n",
    "        if (n_users_evaluated > 0):\n",
    "\n",
    "            for cutoff in self.cutoff_list:\n",
    "\n",
    "                results_current_cutoff = results_dict[cutoff]\n",
    "\n",
    "                for key in results_current_cutoff.keys():\n",
    "\n",
    "                    value = results_current_cutoff[key]\n",
    "\n",
    "                    if isinstance(value, Metrics_Object):\n",
    "                        results_current_cutoff[key] = value.get_metric_value()\n",
    "                    else:\n",
    "                        results_current_cutoff[key] = value / n_users_evaluated\n",
    "\n",
    "                precision_ = results_current_cutoff[EvaluatorMetrics.PRECISION.value]\n",
    "                recall_ = results_current_cutoff[EvaluatorMetrics.RECALL.value]\n",
    "\n",
    "                if precision_ + recall_ != 0:\n",
    "                    # F1 micro averaged: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
    "                    results_current_cutoff[EvaluatorMetrics.F1.value] = 2 * (precision_ * recall_) / (\n",
    "                            precision_ + recall_)\n",
    "        else:\n",
    "            self._print(\"WARNING: No users had a sufficient number of relevant items\")\n",
    "\n",
    "        results_run_string = get_result_string(results_dict)\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.reset_items_to_ignore()\n",
    "\n",
    "        return (results_dict, results_run_string)\n",
    "    \n",
    "    \n",
    "    \n",
    "# metrics.py \n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Maurizio Ferrari Dacrema, Massimo Quadrana\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "class Metrics_Object(object):\n",
    "    \"\"\"\n",
    "    Abstract class that should be used as superclass of all metrics requiring an object, therefore a state, to be computed\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Coverage_Item(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Item coverage represents the percentage of the overall items which were recommended\n",
    "    https://gab41.lab41.org/recommender-systems-its-not-all-about-the-accuracy-562c7dceeaff\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_items, ignore_items):\n",
    "        super(Coverage_Item, self).__init__()\n",
    "        self.recommended_mask = np.zeros(n_items, dtype=np.bool)\n",
    "        self.n_ignore_items = len(ignore_items)\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        if len(recommended_items_ids) > 0:\n",
    "            self.recommended_mask[recommended_items_ids] = True\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.recommended_mask.sum()/(len(self.recommended_mask)-self.n_ignore_items)\n",
    "\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Coverage_Item, \"Coverage_Item: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.recommended_mask = np.logical_or(self.recommended_mask, other_metric_object.recommended_mask)\n",
    "\n",
    "class Coverage_User(Metrics_Object):\n",
    "    \"\"\"\n",
    "    User coverage represents the percentage of the overall users for which we can make recommendations.\n",
    "    If there is at least one recommendation the user is considered as covered\n",
    "    https://gab41.lab41.org/recommender-systems-its-not-all-about-the-accuracy-562c7dceeaff\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_users, ignore_users):\n",
    "        super(Coverage_User, self).__init__()\n",
    "        self.users_mask = np.zeros(n_users, dtype=np.bool)\n",
    "        self.n_ignore_users = len(ignore_users)\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids, user_id):\n",
    "        self.users_mask[user_id] = len(recommended_items_ids)>0\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.users_mask.sum()/(len(self.users_mask)-self.n_ignore_users)\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Coverage_User, \"Coverage_User: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.users_mask = np.logical_or(self.users_mask, other_metric_object.users_mask)\n",
    "\n",
    "class MAP(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Mean Average Precision, defined as the mean of the AveragePrecision over all users\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MAP, self).__init__()\n",
    "        self.cumulative_AP = 0.0\n",
    "        self.n_users = 0\n",
    "\n",
    "    def add_recommendations(self, is_relevant, pos_items):\n",
    "        self.cumulative_AP += average_precision(is_relevant, pos_items)\n",
    "        self.n_users += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.cumulative_AP/self.n_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is MAP, \"MAP: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.cumulative_AP += other_metric_object.cumulative_AP\n",
    "        self.n_users += other_metric_object.n_users\n",
    "\n",
    "class MRR(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank, defined as the mean of the Reciprocal Rank over all users\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MRR, self).__init__()\n",
    "        self.cumulative_RR = 0.0\n",
    "        self.n_users = 0\n",
    "\n",
    "    def add_recommendations(self, is_relevant):\n",
    "        self.cumulative_RR += rr(is_relevant)\n",
    "        self.n_users += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.cumulative_RR/self.n_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is MAP, \"MRR: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.cumulative_RR += other_metric_object.cumulative_RR\n",
    "        self.n_users += other_metric_object.n_users\n",
    "\n",
    "class Gini_Diversity(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Gini diversity index, computed from the Gini Index but with inverted range, such that high values mean higher diversity\n",
    "    This implementation ignores zero-occurrence items\n",
    "    # From https://github.com/oliviaguest/gini\n",
    "    # based on bottom eq: http://www.statsdirect.com/help/content/image/stat0206_wmf.gif\n",
    "    # from: http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    #\n",
    "    # http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.8174&rep=rep1&type=pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_items, ignore_items):\n",
    "        super(Gini_Diversity, self).__init__()\n",
    "        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n",
    "        self.ignore_items = ignore_items.astype(np.int).copy()\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        if len(recommended_items_ids) > 0:\n",
    "            self.recommended_counter[recommended_items_ids] += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        recommended_counter = self.recommended_counter.copy()\n",
    "\n",
    "        recommended_counter_mask = np.ones_like(recommended_counter, dtype = np.bool)\n",
    "        recommended_counter_mask[self.ignore_items] = False\n",
    "        recommended_counter_mask[recommended_counter == 0] = False\n",
    "\n",
    "        recommended_counter = recommended_counter[recommended_counter_mask]\n",
    "\n",
    "        n_items = len(recommended_counter)\n",
    "\n",
    "        recommended_counter_sorted = np.sort(recommended_counter)       # values must be sorted\n",
    "        index = np.arange(1, n_items+1)                                 # index per array element\n",
    "\n",
    "        #gini_index = (np.sum((2 * index - n_items  - 1) * recommended_counter_sorted)) / (n_items * np.sum(recommended_counter_sorted))\n",
    "        gini_diversity = 2*np.sum((n_items + 1 - index)/(n_items+1) * recommended_counter_sorted/np.sum(recommended_counter_sorted))\n",
    "\n",
    "        return gini_diversity\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Gini_Diversity, \"Gini_Diversity: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.recommended_counter += other_metric_object.recommended_counter\n",
    "\n",
    "class Diversity_Herfindahl(Metrics_Object):\n",
    "    \"\"\"\n",
    "    The Herfindahl index is also known as Concentration index, it is used in economy to determine whether the market quotas\n",
    "    are such that an excessive concentration exists. It is here used as a diversity index, if high means high diversity.\n",
    "    It is known to have a small value range in recommender systems, between 0.9 and 1.0\n",
    "    The Herfindahl index is a function of the square of the probability an item has been recommended to any user, hence\n",
    "    The Herfindahl index is equivalent to MeanInterList diversity as they measure the same quantity.\n",
    "    # http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.8174&rep=rep1&type=pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_items, ignore_items):\n",
    "        super(Diversity_Herfindahl, self).__init__()\n",
    "        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n",
    "        self.ignore_items = ignore_items.astype(np.int).copy()\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        if len(recommended_items_ids) > 0:\n",
    "            self.recommended_counter[recommended_items_ids] += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        recommended_counter = self.recommended_counter.copy()\n",
    "\n",
    "        recommended_counter_mask = np.ones_like(recommended_counter, dtype = np.bool)\n",
    "        recommended_counter_mask[self.ignore_items] = False\n",
    "\n",
    "        recommended_counter = recommended_counter[recommended_counter_mask]\n",
    "\n",
    "        if recommended_counter.sum() != 0:\n",
    "            herfindahl_index = 1 - np.sum((recommended_counter / recommended_counter.sum()) ** 2)\n",
    "        else:\n",
    "            herfindahl_index = np.nan\n",
    "\n",
    "        return herfindahl_index\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Diversity_Herfindahl, \"Diversity_Herfindahl: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.recommended_counter += other_metric_object.recommended_counter\n",
    "\n",
    "class Shannon_Entropy(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Shannon Entropy is a well known metric to measure the amount of information of a certain string of data.\n",
    "    Here is applied to the global number of times an item has been recommended.\n",
    "    It has a lower bound and can reach values over 12.0 for random recommenders.\n",
    "    A high entropy means that the distribution is random uniform across all users.\n",
    "    Note that while a random uniform distribution\n",
    "    (hence all items with SIMILAR number of occurrences)\n",
    "    will be highly diverse and have high entropy, a perfectly uniform distribution\n",
    "    (hence all items with EXACTLY IDENTICAL number of occurrences)\n",
    "    will have 0.0 entropy while being the most diverse possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_items, ignore_items):\n",
    "        super(Shannon_Entropy, self).__init__()\n",
    "        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n",
    "        self.ignore_items = ignore_items.astype(np.int).copy()\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        if len(recommended_items_ids) > 0:\n",
    "            self.recommended_counter[recommended_items_ids] += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        assert np.all(self.recommended_counter >= 0.0), \"Shannon_Entropy: self.recommended_counter contains negative counts\"\n",
    "\n",
    "        recommended_counter = self.recommended_counter.copy()\n",
    "\n",
    "        # Ignore from the computation both ignored items and items with zero occurrence.\n",
    "        # Zero occurrence items will have zero probability and will not change the result, butt will generate nans if used in the log\n",
    "        recommended_counter_mask = np.ones_like(recommended_counter, dtype = np.bool)\n",
    "        recommended_counter_mask[self.ignore_items] = False\n",
    "        recommended_counter_mask[recommended_counter == 0] = False\n",
    "\n",
    "        recommended_counter = recommended_counter[recommended_counter_mask]\n",
    "\n",
    "        n_recommendations = recommended_counter.sum()\n",
    "\n",
    "        recommended_probability = recommended_counter/n_recommendations\n",
    "\n",
    "        shannon_entropy = -np.sum(recommended_probability * np.log2(recommended_probability))\n",
    "\n",
    "        return shannon_entropy\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Gini_Diversity, \"Shannon_Entropy: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        assert np.all(self.recommended_counter >= 0.0), \"Shannon_Entropy: self.recommended_counter contains negative counts\"\n",
    "        assert np.all(other_metric_object.recommended_counter >= 0.0), \"Shannon_Entropy: other.recommended_counter contains negative counts\"\n",
    "\n",
    "        self.recommended_counter += other_metric_object.recommended_counter\n",
    "\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "class Novelty(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Novelty measures how \"novel\" a recommendation is in terms of how popular the item was in the train set.\n",
    "    Due to this definition, the novelty of a cold item (i.e. with no interactions in the train set) is not defined,\n",
    "    in this implementation cold items are ignored and their contribution to the novelty is 0.\n",
    "    A recommender with high novelty will be able to recommend also long queue (i.e. unpopular) items.\n",
    "    Mean self-information  (Zhou 2010)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, URM_train):\n",
    "        super(Novelty, self).__init__()\n",
    "\n",
    "        URM_train = sps.csc_matrix(URM_train)\n",
    "        URM_train.eliminate_zeros()\n",
    "        self.item_popularity = np.ediff1d(URM_train.indptr)\n",
    "\n",
    "        self.novelty = 0.0\n",
    "        self.n_evaluated_users = 0\n",
    "        self.n_items = len(self.item_popularity)\n",
    "        self.n_interactions = self.item_popularity.sum()\n",
    "\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "\n",
    "        self.n_evaluated_users += 1\n",
    "\n",
    "        if len(recommended_items_ids)>0:\n",
    "            recommended_items_popularity = self.item_popularity[recommended_items_ids]\n",
    "\n",
    "            probability = recommended_items_popularity/self.n_interactions\n",
    "            probability = probability[probability!=0]\n",
    "\n",
    "            self.novelty += np.sum(-np.log2(probability)/self.n_items)\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        if self.n_evaluated_users == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return self.novelty/self.n_evaluated_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Novelty, \"Novelty: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.novelty = self.novelty + other_metric_object.novelty\n",
    "        self.n_evaluated_users = self.n_evaluated_users + other_metric_object.n_evaluated_users\n",
    "\n",
    "class AveragePopularity(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Average popularity the recommended items have in the train data.\n",
    "    The popularity is normalized by setting as 1 the item with the highest popularity in the train data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, URM_train):\n",
    "        super(AveragePopularity, self).__init__()\n",
    "\n",
    "        URM_train = sps.csc_matrix(URM_train)\n",
    "        URM_train.eliminate_zeros()\n",
    "        item_popularity = np.ediff1d(URM_train.indptr)\n",
    "\n",
    "\n",
    "        self.cumulative_popularity = 0.0\n",
    "        self.n_evaluated_users = 0\n",
    "        self.n_items = URM_train.shape[0]\n",
    "        self.n_interactions = item_popularity.sum()\n",
    "\n",
    "        self.item_popularity_normalized = item_popularity/item_popularity.max()\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "\n",
    "        self.n_evaluated_users += 1\n",
    "\n",
    "        if len(recommended_items_ids)>0:\n",
    "            recommended_items_popularity = self.item_popularity_normalized[recommended_items_ids]\n",
    "\n",
    "            self.cumulative_popularity += np.sum(recommended_items_popularity)/len(recommended_items_ids)\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        if self.n_evaluated_users == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return self.cumulative_popularity/self.n_evaluated_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Novelty, \"AveragePopularity: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.cumulative_popularity = self.cumulative_popularity + other_metric_object.cumulative_popularity\n",
    "        self.n_evaluated_users = self.n_evaluated_users + other_metric_object.n_evaluated_users\n",
    "\n",
    "\n",
    "class Diversity_similarity(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Intra list diversity computes the diversity of items appearing in the recommendations received by each single user, by using an item_diversity_matrix.\n",
    "    It can be used, for example, to compute the diversity in terms of features for a collaborative recommender.\n",
    "    A content-based recommender will have low IntraList diversity if that is computed on the same features the recommender uses.\n",
    "    A TopPopular recommender may exhibit high IntraList diversity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, item_diversity_matrix):\n",
    "        super(Diversity_similarity, self).__init__()\n",
    "\n",
    "        assert np.all(item_diversity_matrix >= 0.0) and np.all(item_diversity_matrix <= 1.0), \\\n",
    "            \"item_diversity_matrix contains value greated than 1.0 or lower than 0.0\"\n",
    "\n",
    "        self.item_diversity_matrix = item_diversity_matrix\n",
    "\n",
    "        self.n_evaluated_users = 0\n",
    "        self.diversity = 0.0\n",
    "\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "\n",
    "        current_recommended_items_diversity = 0.0\n",
    "\n",
    "        for item_index in range(len(recommended_items_ids)-1):\n",
    "\n",
    "            item_id = recommended_items_ids[item_index]\n",
    "\n",
    "            item_other_diversity = self.item_diversity_matrix[item_id, recommended_items_ids]\n",
    "            item_other_diversity[item_index] = 0.0\n",
    "\n",
    "            current_recommended_items_diversity += np.sum(item_other_diversity)\n",
    "\n",
    "\n",
    "        self.diversity += current_recommended_items_diversity/(len(recommended_items_ids)*(len(recommended_items_ids)-1))\n",
    "\n",
    "        self.n_evaluated_users += 1\n",
    "\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        if self.n_evaluated_users == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return self.diversity/self.n_evaluated_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Diversity_similarity, \"Diversity: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.diversity = self.diversity + other_metric_object.diversity\n",
    "        self.n_evaluated_users = self.n_evaluated_users + other_metric_object.n_evaluated_users\n",
    "\n",
    "\n",
    "class Diversity_MeanInterList(Metrics_Object):\n",
    "    \"\"\"\n",
    "    MeanInterList diversity measures the uniqueness of different users' recommendation lists.\n",
    "    It can be used to measure how \"diversified\" are the recommendations different users receive.\n",
    "    While the original proposal called this metric \"Personalization\", we do not use this name since the highest MeanInterList diversity\n",
    "    is exhibited by a non personalized Random recommender.\n",
    "    It can be demonstrated that this metric does not require to compute the common items all possible couples of users have in common\n",
    "    but rather it is only sensitive to the total amount of time each item has been recommended.\n",
    "    MeanInterList diversity is a function of the square of the probability an item has been recommended to any user, hence\n",
    "    MeanInterList diversity is equivalent to the Herfindahl index as they measure the same quantity.\n",
    "    A TopPopular recommender that does not remove seen items will have 0.0 MeanInterList diversity.\n",
    "    pag. 3, http://www.pnas.org/content/pnas/107/10/4511.full.pdf\n",
    "    @article{zhou2010solving,\n",
    "      title={Solving the apparent diversity-accuracy dilemma of recommender systems},\n",
    "      author={Zhou, Tao and Kuscsik, Zolt{\\'a}n and Liu, Jian-Guo and Medo, Mat{\\'u}{\\v{s}} and Wakeling, Joseph Rushton and Zhang, Yi-Cheng},\n",
    "      journal={Proceedings of the National Academy of Sciences},\n",
    "      volume={107},\n",
    "      number={10},\n",
    "      pages={4511--4515},\n",
    "      year={2010},\n",
    "      publisher={National Acad Sciences}\n",
    "    }\n",
    "    # The formula is diversity_cumulative += 1 - common_recommendations(user1, user2)/cutoff\n",
    "    # for each couple of users, except the diagonal. It is VERY computationally expensive\n",
    "    # We can move the 1 and cutoff outside of the summation. Remember to exclude the diagonal\n",
    "    # co_counts = URM_predicted.dot(URM_predicted.T)\n",
    "    # co_counts[np.arange(0, n_user, dtype=np.int):np.arange(0, n_user, dtype=np.int)] = 0\n",
    "    # diversity = (n_user**2 - n_user) - co_counts.sum()/self.cutoff\n",
    "    # If we represent the summation of co_counts separating it for each item, we will have:\n",
    "    # co_counts.sum() = co_counts_item1.sum()  + co_counts_item2.sum() ...\n",
    "    # If we know how many times an item has been recommended, co_counts_item1.sum() can be computed as how many couples of\n",
    "    # users have item1 in common. If item1 has been recommended n times, the number of couples is n*(n-1)\n",
    "    # Therefore we can compute co_counts.sum() value as:\n",
    "    # np.sum(np.multiply(item-occurrence, item-occurrence-1))\n",
    "    # The naive implementation URM_predicted.dot(URM_predicted.T) might require an hour of computation\n",
    "    # The last implementation has a negligible computational time even for very big datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_items, cutoff):\n",
    "        super(Diversity_MeanInterList, self).__init__()\n",
    "\n",
    "        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n",
    "\n",
    "        self.n_evaluated_users = 0\n",
    "        self.n_items = n_items\n",
    "        self.diversity = 0.0\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "\n",
    "        assert len(recommended_items_ids) <= self.cutoff, \"Diversity_MeanInterList: recommended list is contains more elements than cutoff\"\n",
    "\n",
    "        self.n_evaluated_users += 1\n",
    "\n",
    "        if len(recommended_items_ids) > 0:\n",
    "            self.recommended_counter[recommended_items_ids] += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        # Requires to compute the number of common elements for all couples of users\n",
    "        if self.n_evaluated_users == 0:\n",
    "            return 1.0\n",
    "\n",
    "        cooccurrences_cumulative = np.sum(self.recommended_counter**2) - self.n_evaluated_users*self.cutoff\n",
    "\n",
    "        # All user combinations except diagonal\n",
    "        all_user_couples_count = self.n_evaluated_users**2 - self.n_evaluated_users\n",
    "\n",
    "        diversity_cumulative = all_user_couples_count - cooccurrences_cumulative/self.cutoff\n",
    "\n",
    "        self.diversity = diversity_cumulative/all_user_couples_count\n",
    "\n",
    "        return self.diversity\n",
    "\n",
    "    def get_theoretical_max(self):\n",
    "\n",
    "        global_co_occurrence_count = (self.n_evaluated_users*self.cutoff)**2/self.n_items - self.n_evaluated_users*self.cutoff\n",
    "\n",
    "        mild = 1 - 1/(self.n_evaluated_users**2 - self.n_evaluated_users)*(global_co_occurrence_count/self.cutoff)\n",
    "\n",
    "        return mild\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "\n",
    "        assert other_metric_object is Diversity_MeanInterList, \"Diversity_MeanInterList: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        assert np.all(self.recommended_counter >= 0.0), \"Diversity_MeanInterList: self.recommended_counter contains negative counts\"\n",
    "        assert np.all(other_metric_object.recommended_counter >= 0.0), \"Diversity_MeanInterList: other.recommended_counter contains negative counts\"\n",
    "\n",
    "        self.recommended_counter += other_metric_object.recommended_counter\n",
    "        self.n_evaluated_users += other_metric_object.n_evaluated_users\n",
    "\n",
    "def roc_auc(is_relevant):\n",
    "\n",
    "    ranks = np.arange(len(is_relevant))\n",
    "    pos_ranks = ranks[is_relevant]\n",
    "    neg_ranks = ranks[~is_relevant]\n",
    "    auc_score = 0.0\n",
    "\n",
    "    if len(neg_ranks) == 0:\n",
    "        return 1.0\n",
    "\n",
    "    if len(pos_ranks) > 0:\n",
    "        for pos_pred in pos_ranks:\n",
    "            auc_score += np.sum(pos_pred < neg_ranks, dtype=np.float32)\n",
    "        auc_score /= (pos_ranks.shape[0] * neg_ranks.shape[0])\n",
    "\n",
    "    assert 0 <= auc_score <= 1, auc_score\n",
    "    return auc_score\n",
    "\n",
    "def arhr(is_relevant):\n",
    "    # average reciprocal hit-rank (ARHR) of all relevant items\n",
    "    # As opposed to MRR, ARHR takes into account all relevant items and not just the first\n",
    "    # pag 17\n",
    "    # http://glaros.dtc.umn.edu/gkhome/fetch/papers/itemrsTOIS04.pdf\n",
    "    # https://emunix.emich.edu/~sverdlik/COSC562/ItemBasedTopTen.pdf\n",
    "\n",
    "    p_reciprocal = 1/np.arange(1,len(is_relevant)+1, 1.0, dtype=np.float64)\n",
    "    arhr_score = is_relevant.dot(p_reciprocal)\n",
    "\n",
    "    #assert 0 <= arhr_score <= p_reciprocal.sum(), \"arhr_score {} should be between 0 and {}\".format(arhr_score, p_reciprocal.sum())\n",
    "    assert not np.isnan(arhr_score), \"ARHR is NaN\"\n",
    "    return arhr_score\n",
    "\n",
    "def precision(is_relevant):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        precision_score = 0.0\n",
    "    else:\n",
    "        precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "\n",
    "    assert 0 <= precision_score <= 1, precision_score\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "def precision_recall_min_denominator(is_relevant, n_test_items):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        precision_score = 0.0\n",
    "    else:\n",
    "        precision_score = np.sum(is_relevant, dtype=np.float32) / min(n_test_items, len(is_relevant))\n",
    "\n",
    "    assert 0 <= precision_score <= 1, precision_score\n",
    "    return precision_score\n",
    "\n",
    "def rmse(all_items_predicted_ratings, relevant_items, relevant_items_rating):\n",
    "\n",
    "    # Important, some items will have -np.inf score and are treated as if they did not exist\n",
    "\n",
    "    # RMSE with test items\n",
    "    relevant_items_error = (all_items_predicted_ratings[relevant_items]-relevant_items_rating)**2\n",
    "\n",
    "    finite_prediction_mask = np.isfinite(relevant_items_error)\n",
    "\n",
    "    if finite_prediction_mask.sum() == 0:\n",
    "        rmse = np.nan\n",
    "\n",
    "    else:\n",
    "        relevant_items_error = relevant_items_error[finite_prediction_mask]\n",
    "\n",
    "        squared_error = np.sum(relevant_items_error)\n",
    "\n",
    "        # # Second the RMSE against all non-test items assumed having true rating 0\n",
    "        # # In order to avoid the need of explicitly indexing all non-relevant items, use a difference\n",
    "        # squared_error += np.sum(all_items_predicted_ratings[np.isfinite(all_items_predicted_ratings)]**2) - \\\n",
    "        #                  np.sum(all_items_predicted_ratings[relevant_items][np.isfinite(all_items_predicted_ratings[relevant_items])]**2)\n",
    "\n",
    "        mean_squared_error = squared_error/finite_prediction_mask.sum()\n",
    "        rmse = np.sqrt(mean_squared_error)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def recall(is_relevant, pos_items):\n",
    "\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / pos_items.shape[0]\n",
    "\n",
    "    assert 0 <= recall_score <= 1, recall_score\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "def rr(is_relevant):\n",
    "    # reciprocal rank of the FIRST relevant item in the ranked list (0 if none)\n",
    "\n",
    "    ranks = np.arange(1, len(is_relevant) + 1)[is_relevant]\n",
    "\n",
    "    if len(ranks) > 0:\n",
    "        return 1. / ranks[0]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def average_precision(is_relevant, pos_items):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        a_p = 0.0\n",
    "    else:\n",
    "        p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "        a_p = np.sum(p_at_k) / np.min([pos_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    assert 0 <= a_p <= 1, a_p\n",
    "    return a_p\n",
    "\n",
    "\n",
    "def ndcg(ranked_list, pos_items, relevance=None, at=None):\n",
    "\n",
    "    if relevance is None:\n",
    "        relevance = np.ones_like(pos_items)\n",
    "    assert len(relevance) == pos_items.shape[0]\n",
    "\n",
    "    # Create a dictionary associating item_id to its relevance\n",
    "    # it2rel[item] -> relevance[item]\n",
    "    it2rel = {it: r for it, r in zip(pos_items, relevance)}\n",
    "\n",
    "    # Creates array of length \"at\" with the relevance associated to the item in that position\n",
    "    rank_scores = np.asarray([it2rel.get(it, 0.0) for it in ranked_list[:at]], dtype=np.float32)\n",
    "\n",
    "    # IDCG has all relevances to 1, up to the number of items in the test set\n",
    "    ideal_dcg = dcg(np.sort(relevance)[::-1])\n",
    "\n",
    "    # DCG uses the relevance of the recommended items\n",
    "    rank_dcg = dcg(rank_scores)\n",
    "\n",
    "    if rank_dcg == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    ndcg_ = rank_dcg / ideal_dcg\n",
    "    # assert 0 <= ndcg_ <= 1, (rank_dcg, ideal_dcg, ndcg_)\n",
    "    return ndcg_\n",
    "\n",
    "\n",
    "def dcg(scores):\n",
    "    return np.sum(np.divide(np.power(2, scores) - 1, np.log(np.arange(scores.shape[0], dtype=np.float32) + 2)),\n",
    "                  dtype=np.float32)\n",
    "\n",
    "\n",
    "metrics = ['AUC', 'Precision' 'Recall', 'MAP', 'NDCG']\n",
    "\n",
    "\n",
    "def pp_metrics(metric_names, metric_values, metric_at):\n",
    "    \"\"\"\n",
    "    Pretty-prints metric values\n",
    "    :param metrics_arr:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(metric_names) == len(metric_values)\n",
    "    if isinstance(metric_at, int):\n",
    "        metric_at = [metric_at] * len(metric_values)\n",
    "    return ' '.join(['{}: {:.4f}'.format(mname, mvalue) if mcutoff is None or mcutoff == 0 else\n",
    "                     '{}@{}: {:.4f}'.format(mname, mcutoff, mvalue)\n",
    "                     for mname, mcutoff, mvalue in zip(metric_names, metric_at, metric_values)])\n",
    "\n",
    "\n",
    "class TestAUC(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4])\n",
    "        ranked_list = np.asarray([1, 2, 3, 4, 5])\n",
    "        self.assertTrue(np.allclose(roc_auc(ranked_list, pos_items),\n",
    "                                    (2. / 3 + 1. / 3) / 2))\n",
    "\n",
    "\n",
    "class TestRecall(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4, 5, 10])\n",
    "        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n",
    "        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n",
    "        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n",
    "        self.assertTrue(np.allclose(recall(ranked_list_1, pos_items), 3. / 4))\n",
    "        self.assertTrue(np.allclose(recall(ranked_list_2, pos_items), 1.0))\n",
    "        self.assertTrue(np.allclose(recall(ranked_list_3, pos_items), 0.0))\n",
    "\n",
    "        thresholds = [1, 2, 3, 4, 5]\n",
    "        values = [0.0, 1. / 4, 1. / 4, 2. / 4, 3. / 4]\n",
    "        for at, val in zip(thresholds, values):\n",
    "            self.assertTrue(np.allclose(np.asarray(recall(ranked_list_1, pos_items, at=at)), val))\n",
    "\n",
    "\n",
    "class TestPrecision(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4, 5, 10])\n",
    "        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n",
    "        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n",
    "        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n",
    "        self.assertTrue(np.allclose(precision(ranked_list_1, pos_items), 3. / 5))\n",
    "        self.assertTrue(np.allclose(precision(ranked_list_2, pos_items), 4. / 5))\n",
    "        self.assertTrue(np.allclose(precision(ranked_list_3, pos_items), 0.0))\n",
    "\n",
    "        thresholds = [1, 2, 3, 4, 5]\n",
    "        values = [0.0, 1. / 2, 1. / 3, 2. / 4, 3. / 5]\n",
    "        for at, val in zip(thresholds, values):\n",
    "            self.assertTrue(np.allclose(np.asarray(precision(ranked_list_1, pos_items, at=at)), val))\n",
    "\n",
    "\n",
    "class TestRR(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4, 5, 10])\n",
    "        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n",
    "        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n",
    "        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n",
    "        self.assertTrue(np.allclose(rr(ranked_list_1, pos_items), 1. / 2))\n",
    "        self.assertTrue(np.allclose(rr(ranked_list_2, pos_items), 1.))\n",
    "        self.assertTrue(np.allclose(rr(ranked_list_3, pos_items), 0.0))\n",
    "\n",
    "        thresholds = [1, 2, 3, 4, 5]\n",
    "        values = [0.0, 1. / 2, 1. / 2, 1. / 2, 1. / 2]\n",
    "        for at, val in zip(thresholds, values):\n",
    "            self.assertTrue(np.allclose(np.asarray(rr(ranked_list_1, pos_items, at=at)), val))\n",
    "\n",
    "\n",
    "class TestMAP(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4, 5, 10])\n",
    "        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n",
    "        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n",
    "        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n",
    "        ranked_list_4 = np.asarray([11, 12, 13, 14, 15, 16, 2, 4, 5, 10])\n",
    "        ranked_list_5 = np.asarray([2, 11, 12, 13, 14, 15, 4, 5, 10, 16])\n",
    "        self.assertTrue(np.allclose(map(ranked_list_1, pos_items), (1. / 2 + 2. / 4 + 3. / 5) / 4))\n",
    "        self.assertTrue(np.allclose(map(ranked_list_2, pos_items), 1.0))\n",
    "        self.assertTrue(np.allclose(map(ranked_list_3, pos_items), 0.0))\n",
    "        self.assertTrue(np.allclose(map(ranked_list_4, pos_items), (1. / 7 + 2. / 8 + 3. / 9 + 4. / 10) / 4))\n",
    "        self.assertTrue(np.allclose(map(ranked_list_5, pos_items), (1. + 2. / 7 + 3. / 8 + 4. / 9) / 4))\n",
    "\n",
    "        thresholds = [1, 2, 3, 4, 5]\n",
    "        values = [\n",
    "            0.0,\n",
    "            1. / 2 / 2,\n",
    "            1. / 2 / 3,\n",
    "            (1. / 2 + 2. / 4) / 4,\n",
    "            (1. / 2 + 2. / 4 + 3. / 5) / 4\n",
    "        ]\n",
    "        for at, val in zip(thresholds, values):\n",
    "            self.assertTrue(np.allclose(np.asarray(map(ranked_list_1, pos_items, at)), val))\n",
    "\n",
    "\n",
    "class TestNDCG(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4, 5, 10])\n",
    "        pos_relevances = np.asarray([5, 4, 3, 2])\n",
    "        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])  # rel = 0, 5, 0, 4, 3\n",
    "        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])  # rel = 2, 3, 5, 4, 0\n",
    "        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])  # rel = 0, 0, 0, 0, 0\n",
    "        idcg = ((2 ** 5 - 1) / np.log(2) +\n",
    "                (2 ** 4 - 1) / np.log(3) +\n",
    "                (2 ** 3 - 1) / np.log(4) +\n",
    "                (2 ** 2 - 1) / np.log(5))\n",
    "        self.assertTrue(np.allclose(dcg(np.sort(pos_relevances)[::-1]), idcg))\n",
    "        self.assertTrue(np.allclose(ndcg(ranked_list_1, pos_items, pos_relevances),\n",
    "                                    ((2 ** 5 - 1) / np.log(3) +\n",
    "                                     (2 ** 4 - 1) / np.log(5) +\n",
    "                                     (2 ** 3 - 1) / np.log(6)) / idcg))\n",
    "        self.assertTrue(np.allclose(ndcg(ranked_list_2, pos_items, pos_relevances),\n",
    "                                    ((2 ** 2 - 1) / np.log(2) +\n",
    "                                     (2 ** 3 - 1) / np.log(3) +\n",
    "                                     (2 ** 5 - 1) / np.log(4) +\n",
    "                                     (2 ** 4 - 1) / np.log(5)) / idcg))\n",
    "        self.assertTrue(np.allclose(ndcg(ranked_list_3, pos_items, pos_relevances), 0.0))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     unittest.main()\n",
    "\n",
    "\n",
    "# seconds_to_biggest_unit.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 30/03/2019\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def seconds_to_biggest_unit(time_in_seconds, data_array = None):\n",
    "\n",
    "    conversion_factor = [\n",
    "        (\"sec\", 60),\n",
    "        (\"min\", 60),\n",
    "        (\"hour\", 24),\n",
    "        (\"day\", 365),\n",
    "    ]\n",
    "\n",
    "    terminate = False\n",
    "    unit_index = 0\n",
    "\n",
    "    new_time_value = time_in_seconds\n",
    "    new_time_unit = \"sec\"\n",
    "\n",
    "    while not terminate:\n",
    "\n",
    "        next_time = new_time_value/conversion_factor[unit_index][1]\n",
    "\n",
    "        if next_time >= 1.0:\n",
    "            new_time_value = next_time\n",
    "\n",
    "            if data_array is not None:\n",
    "                data_array /= conversion_factor[unit_index][1]\n",
    "\n",
    "            unit_index += 1\n",
    "            new_time_unit = conversion_factor[unit_index][0]\n",
    "\n",
    "        else:\n",
    "            terminate = True\n",
    "\n",
    "\n",
    "    if data_array is not None:\n",
    "        return new_time_value, new_time_unit, data_array\n",
    "\n",
    "    else:\n",
    "        return new_time_value, new_time_unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Hyperparameters tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# SearchAbstractClass.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 10/03/2018\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "import time, os, traceback\n",
    "# from utils.Evaluation.Incremental_Training_Early_Stopping import Incremental_Training_Early_Stopping\n",
    "import numpy as np\n",
    "# from utils.DataIO import DataIO\n",
    "\n",
    "class SearchInputRecommenderArgs(object):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                   # Dictionary of parameters needed by the constructor\n",
    "                   CONSTRUCTOR_POSITIONAL_ARGS = None,\n",
    "                   CONSTRUCTOR_KEYWORD_ARGS = None,\n",
    "\n",
    "                   # List containing all positional arguments needed by the fit function\n",
    "                   FIT_POSITIONAL_ARGS = None,\n",
    "                   FIT_KEYWORD_ARGS = None\n",
    "                   ):\n",
    "\n",
    "\n",
    "          super(SearchInputRecommenderArgs, self).__init__()\n",
    "\n",
    "          if CONSTRUCTOR_POSITIONAL_ARGS is None:\n",
    "              CONSTRUCTOR_POSITIONAL_ARGS = []\n",
    "\n",
    "          if CONSTRUCTOR_KEYWORD_ARGS is None:\n",
    "              CONSTRUCTOR_KEYWORD_ARGS = {}\n",
    "\n",
    "          if FIT_POSITIONAL_ARGS is None:\n",
    "              FIT_POSITIONAL_ARGS = []\n",
    "\n",
    "          if FIT_KEYWORD_ARGS is None:\n",
    "              FIT_KEYWORD_ARGS = {}\n",
    "\n",
    "\n",
    "          assert isinstance(CONSTRUCTOR_POSITIONAL_ARGS, list), \"CONSTRUCTOR_POSITIONAL_ARGS must be a list\"\n",
    "          assert isinstance(CONSTRUCTOR_KEYWORD_ARGS, dict), \"CONSTRUCTOR_KEYWORD_ARGS must be a dict\"\n",
    "\n",
    "          assert isinstance(FIT_POSITIONAL_ARGS, list), \"FIT_POSITIONAL_ARGS must be a list\"\n",
    "          assert isinstance(FIT_KEYWORD_ARGS, dict), \"FIT_KEYWORD_ARGS must be a dict\"\n",
    "\n",
    "\n",
    "          self.CONSTRUCTOR_POSITIONAL_ARGS = CONSTRUCTOR_POSITIONAL_ARGS\n",
    "          self.CONSTRUCTOR_KEYWORD_ARGS = CONSTRUCTOR_KEYWORD_ARGS\n",
    "\n",
    "          self.FIT_POSITIONAL_ARGS = FIT_POSITIONAL_ARGS\n",
    "          self.FIT_KEYWORD_ARGS = FIT_KEYWORD_ARGS\n",
    "\n",
    "\n",
    "    def copy(self):\n",
    "\n",
    "\n",
    "        clone_object = SearchInputRecommenderArgs(\n",
    "                            CONSTRUCTOR_POSITIONAL_ARGS = self.CONSTRUCTOR_POSITIONAL_ARGS.copy(),\n",
    "                            CONSTRUCTOR_KEYWORD_ARGS = self.CONSTRUCTOR_KEYWORD_ARGS.copy(),\n",
    "                            FIT_POSITIONAL_ARGS = self.FIT_POSITIONAL_ARGS.copy(),\n",
    "                            FIT_KEYWORD_ARGS = self.FIT_KEYWORD_ARGS.copy()\n",
    "                            )\n",
    "\n",
    "\n",
    "        return clone_object\n",
    "\n",
    "\n",
    "\n",
    "def _compute_avg_time_non_none_values(data_list):\n",
    "\n",
    "    non_none_values = sum([value is not None for value in data_list])\n",
    "    total_value = sum([value if value is not None else 0.0 for value in data_list])\n",
    "\n",
    "    return total_value, \\\n",
    "           total_value/non_none_values if non_none_values != 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def get_result_string_evaluate_on_validation(results_run_single_cutoff, n_decimals=7):\n",
    "\n",
    "    output_str = \"\"\n",
    "\n",
    "    for metric in results_run_single_cutoff.keys():\n",
    "        output_str += \"{}: {:.{n_decimals}f}, \".format(metric, results_run_single_cutoff[metric], n_decimals = n_decimals)\n",
    "\n",
    "    return output_str\n",
    "\n",
    "\n",
    "\n",
    "class SearchAbstractClass(object):\n",
    "\n",
    "    ALGORITHM_NAME = \"SearchAbstractClass\"\n",
    "\n",
    "    # Available values for the save_model attribute\n",
    "    _SAVE_MODEL_VALUES = [\"all\", \"best\", \"last\", \"no\"]\n",
    "\n",
    "\n",
    "    # Value to be assigned to invalid configuration or if an Exception is raised\n",
    "    INVALID_CONFIG_VALUE = np.finfo(np.float16).max\n",
    "\n",
    "    def __init__(self, recommender_class,\n",
    "                 evaluator_validation = None,\n",
    "                 evaluator_test = None,\n",
    "                 verbose = True):\n",
    "\n",
    "        super(SearchAbstractClass, self).__init__()\n",
    "\n",
    "        self.recommender_class = recommender_class\n",
    "        self.verbose = verbose\n",
    "        self.log_file = None\n",
    "\n",
    "        self.results_test_best = {}\n",
    "        self.parameter_dictionary_best = {}\n",
    "\n",
    "        self.evaluator_validation = evaluator_validation\n",
    "\n",
    "        if evaluator_test is None:\n",
    "            self.evaluator_test = None\n",
    "        else:\n",
    "            self.evaluator_test = evaluator_test\n",
    "\n",
    "\n",
    "    def search(self, recommender_input_args,\n",
    "               parameter_search_space,\n",
    "               metric_to_optimize = \"MAP\",\n",
    "               n_cases = None,\n",
    "               output_folder_path = None,\n",
    "               output_file_name_root = None,\n",
    "               parallelize = False,\n",
    "               save_model = \"best\",\n",
    "               evaluate_on_test_each_best_solution = True,\n",
    "               save_metadata = True,\n",
    "               ):\n",
    "\n",
    "        raise NotImplementedError(\"Function search not implemented for this class\")\n",
    "\n",
    "\n",
    "    def _set_search_attributes(self, recommender_input_args,\n",
    "                               recommender_input_args_last_test,\n",
    "                               metric_to_optimize,\n",
    "                               output_folder_path,\n",
    "                               output_file_name_root,\n",
    "                               resume_from_saved,\n",
    "                               save_metadata,\n",
    "                               save_model,\n",
    "                               evaluate_on_test_each_best_solution,\n",
    "                               n_cases):\n",
    "\n",
    "\n",
    "        if save_model not in self._SAVE_MODEL_VALUES:\n",
    "           raise ValueError(\"{}: parameter save_model must be in '{}', provided was '{}'.\".format(self.ALGORITHM_NAME, self._SAVE_MODEL_VALUES, save_model))\n",
    "\n",
    "        self.output_folder_path = output_folder_path\n",
    "        self.output_file_name_root = output_file_name_root\n",
    "\n",
    "        # If directory does not exist, create\n",
    "        if not os.path.exists(self.output_folder_path):\n",
    "            os.makedirs(self.output_folder_path)\n",
    "\n",
    "        self.log_file = open(self.output_folder_path + self.output_file_name_root + \"_{}.txt\".format(self.ALGORITHM_NAME), \"a\")\n",
    "\n",
    "        if save_model == \"last\" and recommender_input_args_last_test is None:\n",
    "            self._write_log(\"{}: parameter save_model is 'last' but no recommender_input_args_last_test provided, saving best model on train data alone.\".format(self.ALGORITHM_NAME))\n",
    "            save_model = \"best\"\n",
    "\n",
    "\n",
    "\n",
    "        self.recommender_input_args = recommender_input_args\n",
    "        self.recommender_input_args_last_test = recommender_input_args_last_test\n",
    "        self.metric_to_optimize = metric_to_optimize\n",
    "        self.save_model = save_model\n",
    "        self.resume_from_saved = resume_from_saved\n",
    "        self.save_metadata = save_metadata\n",
    "        self.evaluate_on_test_each_best_solution = evaluate_on_test_each_best_solution\n",
    "\n",
    "        self.model_counter = 0\n",
    "        self._init_metadata_dict(n_cases = n_cases)\n",
    "\n",
    "        if self.save_metadata:\n",
    "            self.dataIO = DataIO(folder_path = self.output_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "    def _init_metadata_dict(self, n_cases):\n",
    "\n",
    "        self.metadata_dict = {\"algorithm_name_search\": self.ALGORITHM_NAME,\n",
    "                              \"algorithm_name_recommender\": self.recommender_class.RECOMMENDER_NAME,\n",
    "                              \"exception_list\": [None]*n_cases,\n",
    "\n",
    "                              \"hyperparameters_list\": [None]*n_cases,\n",
    "                              \"hyperparameters_best\": None,\n",
    "                              \"hyperparameters_best_index\": None,\n",
    "\n",
    "                              \"result_on_validation_list\": [None]*n_cases,\n",
    "                              \"result_on_validation_best\": None,\n",
    "                              \"result_on_test_list\": [None]*n_cases,\n",
    "                              \"result_on_test_best\": None,\n",
    "\n",
    "                              \"time_on_train_list\": [None]*n_cases,\n",
    "                              \"time_on_train_total\": 0.0,\n",
    "                              \"time_on_train_avg\": 0.0,\n",
    "\n",
    "                              \"time_on_validation_list\": [None]*n_cases,\n",
    "                              \"time_on_validation_total\": 0.0,\n",
    "                              \"time_on_validation_avg\": 0.0,\n",
    "\n",
    "                              \"time_on_test_list\": [None]*n_cases,\n",
    "                              \"time_on_test_total\": 0.0,\n",
    "                              \"time_on_test_avg\": 0.0,\n",
    "\n",
    "                              \"result_on_last\": None,\n",
    "                              \"time_on_last_train\": None,\n",
    "                              \"time_on_last_test\": None,\n",
    "                              }\n",
    "\n",
    "\n",
    "    def _print(self, string):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(string)\n",
    "\n",
    "\n",
    "    def _write_log(self, string):\n",
    "\n",
    "        self._print(string)\n",
    "\n",
    "        if self.log_file is not None:\n",
    "            self.log_file.write(string)\n",
    "            self.log_file.flush()\n",
    "\n",
    "\n",
    "    def _fit_model(self, current_fit_parameters):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Construct a new recommender instance\n",
    "        recommender_instance = self.recommender_class(*self.recommender_input_args.CONSTRUCTOR_POSITIONAL_ARGS,\n",
    "                                                      **self.recommender_input_args.CONSTRUCTOR_KEYWORD_ARGS)\n",
    "\n",
    "\n",
    "        self._print(\"{}: Testing config: {}\".format(self.ALGORITHM_NAME, current_fit_parameters))\n",
    "\n",
    "\n",
    "        recommender_instance.fit(*self.recommender_input_args.FIT_POSITIONAL_ARGS,\n",
    "                                 **self.recommender_input_args.FIT_KEYWORD_ARGS,\n",
    "                                 **current_fit_parameters)\n",
    "\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        return recommender_instance, train_time\n",
    "\n",
    "\n",
    "\n",
    "    def _evaluate_on_validation(self, current_fit_parameters):\n",
    "\n",
    "        recommender_instance, train_time = self._fit_model(current_fit_parameters)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Evaluate recommender and get results for the first cutoff\n",
    "        result_dict, _ = self.evaluator_validation.evaluateRecommender(recommender_instance)\n",
    "        result_dict = result_dict[list(result_dict.keys())[0]]\n",
    "\n",
    "        evaluation_time = time.time() - start_time\n",
    "\n",
    "        result_string = get_result_string_evaluate_on_validation(result_dict, n_decimals=7)\n",
    "\n",
    "        return result_dict, result_string, recommender_instance, train_time, evaluation_time\n",
    "\n",
    "\n",
    "\n",
    "    def _evaluate_on_test(self, recommender_instance, current_fit_parameters_dict, print_log = True):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Evaluate recommender and get results for the first cutoff\n",
    "        result_dict, result_string = self.evaluator_test.evaluateRecommender(recommender_instance)\n",
    "\n",
    "        evaluation_test_time = time.time() - start_time\n",
    "\n",
    "        if print_log:\n",
    "            self._write_log(\"{}: Best config evaluated with evaluator_test. Config: {} - results:\\n{}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                                                         current_fit_parameters_dict,\n",
    "                                                                                                         result_string))\n",
    "\n",
    "        return result_dict, result_string, evaluation_test_time\n",
    "\n",
    "\n",
    "\n",
    "    def _evaluate_on_test_with_data_last(self):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Construct a new recommender instance\n",
    "        recommender_instance = self.recommender_class(*self.recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS,\n",
    "                                                      **self.recommender_input_args_last_test.CONSTRUCTOR_KEYWORD_ARGS)\n",
    "\n",
    "        # Check if last was already evaluated\n",
    "        if self.resume_from_saved:\n",
    "            result_on_last_saved_flag = self.metadata_dict[\"result_on_last\"] is not None and \\\n",
    "                                        self.metadata_dict[\"time_on_last_train\"] is not None and \\\n",
    "                                        self.metadata_dict[\"time_on_last_test\"] is not None\n",
    "\n",
    "            if result_on_last_saved_flag:\n",
    "                self._print(\"{}: Resuming '{}'... Result on last already available.\".format(self.ALGORITHM_NAME, self.output_file_name_root))\n",
    "                return\n",
    "\n",
    "\n",
    "\n",
    "        self._print(\"{}: Evaluation with constructor data for final test. Using best config: {}\".format(self.ALGORITHM_NAME, self.metadata_dict[\"hyperparameters_best\"]))\n",
    "\n",
    "\n",
    "        # Use the hyperparameters that have been saved\n",
    "        assert self.metadata_dict[\"hyperparameters_best\"] is not None, \"{}: Best hyperparameters not available, the search might have failed.\".format(self.ALGORITHM_NAME)\n",
    "        fit_keyword_args = self.metadata_dict[\"hyperparameters_best\"].copy()\n",
    "\n",
    "\n",
    "        recommender_instance.fit(*self.recommender_input_args_last_test.FIT_POSITIONAL_ARGS,\n",
    "                                 **fit_keyword_args)\n",
    "\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        result_dict_test, result_string, evaluation_test_time = self._evaluate_on_test(recommender_instance, fit_keyword_args, print_log = False)\n",
    "\n",
    "        self._write_log(\"{}: Best config evaluated with evaluator_test with constructor data for final test. Config: {} - results:\\n{}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                                                                                          self.metadata_dict[\"hyperparameters_best\"],\n",
    "                                                                                                                                          result_string))\n",
    "\n",
    "        self.metadata_dict[\"result_on_last\"] = result_dict_test\n",
    "        self.metadata_dict[\"time_on_last_train\"] = train_time\n",
    "        self.metadata_dict[\"time_on_last_test\"] = evaluation_test_time\n",
    "\n",
    "        if self.save_metadata:\n",
    "            self.dataIO.save_data(data_dict_to_save = self.metadata_dict.copy(),\n",
    "                                  file_name = self.output_file_name_root + \"_metadata\")\n",
    "\n",
    "        if self.save_model in [\"all\", \"best\", \"last\"]:\n",
    "            self._print(\"{}: Saving model in {}\\n\".format(self.ALGORITHM_NAME, self.output_folder_path + self.output_file_name_root))\n",
    "            recommender_instance.save_model(self.output_folder_path, file_name =self.output_file_name_root + \"_best_model_last\")\n",
    "\n",
    "\n",
    "    def _objective_function(self, current_fit_parameters_dict):\n",
    "\n",
    "        try:\n",
    "\n",
    "            self.metadata_dict[\"hyperparameters_list\"][self.model_counter] = current_fit_parameters_dict.copy()\n",
    "\n",
    "            result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\n",
    "\n",
    "            current_result = - result_dict[self.metric_to_optimize]\n",
    "\n",
    "            # If the recommender uses Earlystopping, get the selected number of epochs\n",
    "            if isinstance(recommender_instance, Incremental_Training_Early_Stopping):\n",
    "\n",
    "                n_epochs_early_stopping_dict = recommender_instance.get_early_stopping_final_epochs_dict()\n",
    "                current_fit_parameters_dict = current_fit_parameters_dict.copy()\n",
    "\n",
    "                for epoch_label in n_epochs_early_stopping_dict.keys():\n",
    "\n",
    "                    epoch_value = n_epochs_early_stopping_dict[epoch_label]\n",
    "                    current_fit_parameters_dict[epoch_label] = epoch_value\n",
    "\n",
    "\n",
    "\n",
    "            # Always save best model separately\n",
    "            if self.save_model in [\"all\"]:\n",
    "                self._print(\"{}: Saving model in {}\\n\".format(self.ALGORITHM_NAME, self.output_folder_path + self.output_file_name_root))\n",
    "                recommender_instance.save_model(self.output_folder_path, file_name = self.output_file_name_root + \"_model_{}\".format(self.model_counter))\n",
    "\n",
    "\n",
    "            if self.metadata_dict[\"result_on_validation_best\"] is None:\n",
    "                new_best_config_found = True\n",
    "            else:\n",
    "                best_solution_val = self.metadata_dict[\"result_on_validation_best\"][self.metric_to_optimize]\n",
    "                new_best_config_found = best_solution_val < result_dict[self.metric_to_optimize]\n",
    "\n",
    "\n",
    "            if new_best_config_found:\n",
    "\n",
    "                self._write_log(\"{}: New best config found. Config {}: {} - results: {}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                                           self.model_counter,\n",
    "                                                                                           current_fit_parameters_dict,\n",
    "                                                                                           result_string))\n",
    "\n",
    "                if self.save_model in [\"all\", \"best\"]:\n",
    "                    self._print(\"{}: Saving model in {}\\n\".format(self.ALGORITHM_NAME, self.output_folder_path + self.output_file_name_root))\n",
    "                    recommender_instance.save_model(self.output_folder_path, file_name =self.output_file_name_root + \"_best_model\")\n",
    "\n",
    "\n",
    "                if self.evaluator_test is not None and self.evaluate_on_test_each_best_solution:\n",
    "                    result_dict_test, _, evaluation_test_time = self._evaluate_on_test(recommender_instance, current_fit_parameters_dict, print_log = True)\n",
    "\n",
    "\n",
    "            else:\n",
    "                self._write_log(\"{}: Config {} is suboptimal. Config: {} - results: {}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                                          self.model_counter,\n",
    "                                                                                          current_fit_parameters_dict,\n",
    "                                                                                          result_string))\n",
    "\n",
    "\n",
    "\n",
    "            if current_result >= self.INVALID_CONFIG_VALUE:\n",
    "                self._write_log(\"{}: WARNING! Config {} returned a value equal or worse than the default value to be assigned to invalid configurations.\"\n",
    "                                \" If no better valid configuration is found, this parameter search may produce an invalid result.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "            self.metadata_dict[\"result_on_validation_list\"][self.model_counter] = result_dict.copy()\n",
    "\n",
    "            self.metadata_dict[\"time_on_train_list\"][self.model_counter] = train_time\n",
    "            self.metadata_dict[\"time_on_validation_list\"][self.model_counter] = evaluation_time\n",
    "\n",
    "            self.metadata_dict[\"time_on_train_total\"], self.metadata_dict[\"time_on_train_avg\"] = \\\n",
    "                _compute_avg_time_non_none_values(self.metadata_dict[\"time_on_train_list\"])\n",
    "            self.metadata_dict[\"time_on_validation_total\"], self.metadata_dict[\"time_on_validation_avg\"] = \\\n",
    "                _compute_avg_time_non_none_values(self.metadata_dict[\"time_on_validation_list\"])\n",
    "\n",
    "\n",
    "            if new_best_config_found:\n",
    "                self.metadata_dict[\"hyperparameters_best\"] = current_fit_parameters_dict.copy()\n",
    "                self.metadata_dict[\"hyperparameters_best_index\"] = self.model_counter\n",
    "                self.metadata_dict[\"result_on_validation_best\"] = result_dict.copy()\n",
    "\n",
    "                if self.evaluator_test is not None and self.evaluate_on_test_each_best_solution:\n",
    "                    self.metadata_dict[\"result_on_test_best\"] = result_dict_test.copy()\n",
    "                    self.metadata_dict[\"result_on_test_list\"][self.model_counter] = result_dict_test.copy()\n",
    "                    self.metadata_dict[\"time_on_test_list\"][self.model_counter] = evaluation_test_time\n",
    "\n",
    "                    self.metadata_dict[\"time_on_test_total\"], self.metadata_dict[\"time_on_test_avg\"] = \\\n",
    "                        _compute_avg_time_non_none_values(self.metadata_dict[\"time_on_test_list\"])\n",
    "\n",
    "\n",
    "        except (KeyboardInterrupt, SystemExit) as e:\n",
    "            # If getting a interrupt, terminate without saving the exception\n",
    "            raise e\n",
    "\n",
    "        except:\n",
    "            # Catch any error: Exception, Tensorflow errors etc...\n",
    "\n",
    "            traceback_string = traceback.format_exc()\n",
    "\n",
    "            self._write_log(\"{}: Config {} Exception. Config: {} - Exception: {}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                                  self.model_counter,\n",
    "                                                                                  current_fit_parameters_dict,\n",
    "                                                                                  traceback_string))\n",
    "\n",
    "            self.metadata_dict[\"exception_list\"][self.model_counter] = traceback_string\n",
    "\n",
    "\n",
    "            # Assign to this configuration the worst possible score\n",
    "            # Being a minimization problem, set it to the max value of a float\n",
    "            current_result = + self.INVALID_CONFIG_VALUE\n",
    "\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "        if self.save_metadata:\n",
    "            self.dataIO.save_data(data_dict_to_save = self.metadata_dict.copy(),\n",
    "                                  file_name = self.output_file_name_root + \"_metadata\")\n",
    "\n",
    "        self.model_counter += 1\n",
    "\n",
    "        return current_result\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# SearchBayesianSkopt.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 14/12/18\n",
    "@author: Emanuele Chioso, Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# from utils.ParameterTuning.SearchAbstractClass import SearchAbstractClass\n",
    "import traceback\n",
    "\n",
    "\n",
    "class SearchBayesianSkopt(SearchAbstractClass):\n",
    "\n",
    "    ALGORITHM_NAME = \"SearchBayesianSkopt\"\n",
    "\n",
    "    def __init__(self, recommender_class, evaluator_validation = None, evaluator_test = None, verbose = True):\n",
    "\n",
    "        assert evaluator_validation is not None, \"{}: evaluator_validation must be provided\".format(self.ALGORITHM_NAME)\n",
    "\n",
    "        super(SearchBayesianSkopt, self).__init__(recommender_class,\n",
    "                                                  evaluator_validation = evaluator_validation,\n",
    "                                                  evaluator_test = evaluator_test,\n",
    "                                                  verbose = verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def _set_skopt_params(self, n_calls = 70,\n",
    "                          n_random_starts = 20,\n",
    "                          n_points = 10000,\n",
    "                          n_jobs = 1,\n",
    "                          # noise = 'gaussian',\n",
    "                          noise = 1e-5,\n",
    "                          acq_func = 'gp_hedge',\n",
    "                          acq_optimizer = 'auto',\n",
    "                          random_state = None,\n",
    "                          verbose = True,\n",
    "                          n_restarts_optimizer = 10,\n",
    "                          xi = 0.01,\n",
    "                          kappa = 1.96,\n",
    "                          x0 = None,\n",
    "                          y0 = None):\n",
    "        \"\"\"\n",
    "        wrapper to change the params of the bayesian optimizator.\n",
    "        for further details:\n",
    "        https://scikit-optimize.github.io/#skopt.gp_minimize\n",
    "        \"\"\"\n",
    "        self.n_point = n_points\n",
    "        self.n_calls = n_calls\n",
    "        self.n_random_starts = n_random_starts\n",
    "        self.n_jobs = n_jobs\n",
    "        self.acq_func = acq_func\n",
    "        self.acq_optimizer = acq_optimizer\n",
    "        self.random_state = random_state\n",
    "        self.n_restarts_optimizer = n_restarts_optimizer\n",
    "        self.verbose = verbose\n",
    "        self.xi = xi\n",
    "        self.kappa = kappa\n",
    "        self.noise = noise\n",
    "        self.x0 = x0\n",
    "        self.y0 = y0\n",
    "\n",
    "\n",
    "\n",
    "    def _resume_from_saved(self):\n",
    "\n",
    "        try:\n",
    "            self.metadata_dict = self.dataIO.load_data(file_name = self.output_file_name_root + \"_metadata\")\n",
    "\n",
    "        except (KeyboardInterrupt, SystemExit) as e:\n",
    "            # If getting a interrupt, terminate without saving the exception\n",
    "            raise e\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            self._write_log(\"{}: Resuming '{}' Failed, no such file exists.\\n\".format(self.ALGORITHM_NAME, self.output_file_name_root))\n",
    "            self.resume_from_saved = False\n",
    "            return None, None\n",
    "\n",
    "        except Exception as e:\n",
    "            self._write_log(\"{}: Resuming '{}' Failed, generic exception: {}.\\n\".format(self.ALGORITHM_NAME, self.output_file_name_root, str(e)))\n",
    "            traceback.print_exc()\n",
    "            self.resume_from_saved = False\n",
    "            return None, None\n",
    "\n",
    "        # Get hyperparameter list and corresponding result\n",
    "        # Make sure that the hyperparameters only contain those given as input and not others like the number of epochs\n",
    "        # selected by earlystopping\n",
    "        hyperparameters_list_saved = self.metadata_dict['hyperparameters_list']\n",
    "        result_on_validation_list_saved = self.metadata_dict['result_on_validation_list']\n",
    "\n",
    "        hyperparameters_list_input = []\n",
    "        result_on_validation_list_input = []\n",
    "\n",
    "        # The hyperparameters are saved for all cases even if they throw an exception\n",
    "        while self.model_counter<len(hyperparameters_list_saved) and hyperparameters_list_saved[self.model_counter] is not None:\n",
    "\n",
    "            hyperparameters_config_saved = hyperparameters_list_saved[self.model_counter]\n",
    "\n",
    "            hyperparameters_config_input = []\n",
    "\n",
    "            # Add only those having a search space, in the correct ordering\n",
    "            for index in range(len(self.hyperparams_names)):\n",
    "                key = self.hyperparams_names[index]\n",
    "                value_saved = hyperparameters_config_saved[key]\n",
    "\n",
    "                # Check if single value categorical. It is aimed at intercepting\n",
    "                # Hyperparameters that are chosen via early stopping and set them as the\n",
    "                # maximum value as per hyperparameter search space. If not, the gp_minimize will return an error\n",
    "                # as some values will be outside (lower) than the search space\n",
    "\n",
    "                if isinstance(self.hyperparams_values[index], Categorical) and self.hyperparams_values[index].transformed_size == 1:\n",
    "                    value_input = self.hyperparams_values[index].bounds[0]\n",
    "                else:\n",
    "                    value_input = value_saved\n",
    "\n",
    "                hyperparameters_config_input.append(value_input)\n",
    "\n",
    "\n",
    "            hyperparameters_list_input.append(hyperparameters_config_input)\n",
    "\n",
    "            # Check if the hyperparameters have a valid result or an exception\n",
    "            validation_result = result_on_validation_list_saved[self.model_counter]\n",
    "\n",
    "            if validation_result is None:\n",
    "                # Exception detected\n",
    "                result_on_validation_list_input.append(+ self.INVALID_CONFIG_VALUE)\n",
    "\n",
    "                assert self.metadata_dict[\"exception_list\"][self.model_counter] is not None, \\\n",
    "                    \"{}: Resuming '{}' Failed due to inconsistent data. Invalid validation result found in position {} but no corresponding exception detected.\".format(self.ALGORITHM_NAME, self.output_file_name_root, self.model_counter)\n",
    "            else:\n",
    "                result_on_validation_list_input.append(- validation_result[self.metric_to_optimize])\n",
    "\n",
    "\n",
    "\n",
    "            self.model_counter += 1\n",
    "\n",
    "\n",
    "        self._print(\"{}: Resuming '{}'... Loaded {} configurations.\".format(self.ALGORITHM_NAME, self.output_file_name_root, self.model_counter))\n",
    "\n",
    "\n",
    "        # If the data structure exists but is empty, return None\n",
    "        if len(hyperparameters_list_input) == 0:\n",
    "            self.resume_from_saved = False\n",
    "            return None, None\n",
    "\n",
    "        # If loaded less configurations than desired ones\n",
    "        if self.model_counter < self.n_calls:\n",
    "            self.resume_from_saved = False\n",
    "\n",
    "\n",
    "        return hyperparameters_list_input, result_on_validation_list_input\n",
    "\n",
    "\n",
    "    def search(self, recommender_input_args,\n",
    "               parameter_search_space,\n",
    "               metric_to_optimize = \"MAP\",\n",
    "               n_cases = 20,\n",
    "               n_random_starts = 5,\n",
    "               output_folder_path = None,\n",
    "               output_file_name_root = None,\n",
    "               save_model = \"best\",\n",
    "               save_metadata = True,\n",
    "               resume_from_saved = False,\n",
    "               recommender_input_args_last_test = None,\n",
    "               evaluate_on_test_each_best_solution = True,\n",
    "               ):\n",
    "        \"\"\"\n",
    "        :param recommender_input_args:\n",
    "        :param parameter_search_space:\n",
    "        :param metric_to_optimize:\n",
    "        :param n_cases:\n",
    "        :param n_random_starts:\n",
    "        :param output_folder_path:\n",
    "        :param output_file_name_root:\n",
    "        :param save_model:          \"no\"    don't save anything\n",
    "                                    \"all\"   save every model\n",
    "                                    \"best\"  save the best model trained on train data alone and on last, if present\n",
    "                                    \"last\"  save only last, if present\n",
    "        :param save_metadata:\n",
    "        :param recommender_input_args_last_test:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self._set_skopt_params()    ### default parameters are set here\n",
    "\n",
    "        self._set_search_attributes(recommender_input_args,\n",
    "                                    recommender_input_args_last_test,\n",
    "                                    metric_to_optimize,\n",
    "                                    output_folder_path,\n",
    "                                    output_file_name_root,\n",
    "                                    resume_from_saved,\n",
    "                                    save_metadata,\n",
    "                                    save_model,\n",
    "                                    evaluate_on_test_each_best_solution,\n",
    "                                    n_cases)\n",
    "\n",
    "\n",
    "        self.parameter_search_space = parameter_search_space\n",
    "        self.n_random_starts = n_random_starts\n",
    "        self.n_calls = n_cases\n",
    "        self.n_jobs = 1\n",
    "        self.n_loaded_counter = 0\n",
    "\n",
    "\n",
    "        self.hyperparams = dict()\n",
    "        self.hyperparams_names = list()\n",
    "        self.hyperparams_values = list()\n",
    "\n",
    "        skopt_types = [Real, Integer, Categorical]\n",
    "\n",
    "        for name, hyperparam in self.parameter_search_space.items():\n",
    "\n",
    "            if any(isinstance(hyperparam, sko_type) for sko_type in skopt_types):\n",
    "                self.hyperparams_names.append(name)\n",
    "                self.hyperparams_values.append(hyperparam)\n",
    "                self.hyperparams[name] = hyperparam\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"{}: Unexpected parameter type: {} - {}\".format(self.ALGORITHM_NAME, str(name), str(hyperparam)))\n",
    "\n",
    "\n",
    "        if self.resume_from_saved:\n",
    "            hyperparameters_list_input, result_on_validation_list_saved = self._resume_from_saved()\n",
    "            self.x0 = hyperparameters_list_input\n",
    "            self.y0 = result_on_validation_list_saved\n",
    "\n",
    "            self.n_random_starts = max(0, self.n_random_starts - self.model_counter)\n",
    "            self.n_calls = max(0, self.n_calls - self.model_counter)\n",
    "            self.n_loaded_counter = self.model_counter\n",
    "\n",
    "\n",
    "\n",
    "        self.result = gp_minimize(self._objective_function_list_input,\n",
    "                                  self.hyperparams_values,\n",
    "                                  base_estimator=None,\n",
    "                                  n_calls=self.n_calls,\n",
    "                                  n_random_starts=self.n_random_starts,\n",
    "                                  acq_func=self.acq_func,\n",
    "                                  acq_optimizer=self.acq_optimizer,\n",
    "                                  x0=self.x0,\n",
    "                                  y0=self.y0,\n",
    "                                  random_state=self.random_state,\n",
    "                                  verbose=self.verbose,\n",
    "                                  callback=None,\n",
    "                                  n_points=self.n_point,\n",
    "                                  n_restarts_optimizer=self.n_restarts_optimizer,\n",
    "                                  xi=self.xi,\n",
    "                                  kappa=self.kappa,\n",
    "                                  noise=self.noise,\n",
    "                                  n_jobs=self.n_jobs)\n",
    "\n",
    "\n",
    "        if self.n_loaded_counter < self.model_counter:\n",
    "            self._write_log(\"{}: Search complete. Best config is {}: {}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                           self.metadata_dict[\"hyperparameters_best_index\"],\n",
    "                                                                           self.metadata_dict[\"hyperparameters_best\"]))\n",
    "\n",
    "\n",
    "        if self.recommender_input_args_last_test is not None:\n",
    "            self._evaluate_on_test_with_data_last()\n",
    "\n",
    "\n",
    "    def _objective_function_list_input(self, current_fit_parameters_list_of_values):\n",
    "\n",
    "        current_fit_parameters_dict = dict(zip(self.hyperparams_names, current_fit_parameters_list_of_values))\n",
    "\n",
    "        return self._objective_function(current_fit_parameters_dict)\n",
    "\n",
    "\n",
    "\n",
    "# hyperparameter_search.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 22/11/17\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "# Automate hyperparameter tuning\n",
    "# using bayesian optimization with scikit-optimize\n",
    "# -------------------------------------------------\n",
    "\n",
    "import os, multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "######################################################################\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import traceback\n",
    "# from Utils.PoolWithSubprocess import PoolWithSubprocess\n",
    "\n",
    "\n",
    "def run_KNNRecommender_on_similarity_type(similarity_type, parameterSearch,\n",
    "                                          parameter_search_space,\n",
    "                                          recommender_input_args,\n",
    "                                          n_cases,\n",
    "                                          n_random_starts,\n",
    "                                          resume_from_saved,\n",
    "                                          save_model,\n",
    "                                          output_folder_path,\n",
    "                                          output_file_name_root,\n",
    "                                          metric_to_optimize,\n",
    "                                          allow_weighting=False,\n",
    "                                          recommender_input_args_last_test=None):\n",
    "\n",
    "    original_parameter_search_space = parameter_search_space\n",
    "\n",
    "    hyperparameters_range_dictionary = {}\n",
    "    hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n",
    "    hyperparameters_range_dictionary[\"shrink\"] = Integer(0, 1000)\n",
    "    hyperparameters_range_dictionary[\"similarity\"] = Categorical([similarity_type])\n",
    "    hyperparameters_range_dictionary[\"normalize\"] = Categorical([True, False])\n",
    "\n",
    "    is_set_similarity = similarity_type in [\"tversky\", \"dice\", \"jaccard\", \"tanimoto\"]\n",
    "\n",
    "    if similarity_type == \"asymmetric\":\n",
    "        hyperparameters_range_dictionary[\"asymmetric_alpha\"] = Real(low=0, high=2, prior='uniform')\n",
    "        hyperparameters_range_dictionary[\"normalize\"] = Categorical([True])\n",
    "\n",
    "    elif similarity_type == \"tversky\":\n",
    "        hyperparameters_range_dictionary[\"tversky_alpha\"] = Real(low=0, high=2, prior='uniform')\n",
    "        hyperparameters_range_dictionary[\"tversky_beta\"] = Real(low=0, high=2, prior='uniform')\n",
    "        hyperparameters_range_dictionary[\"normalize\"] = Categorical([True])\n",
    "\n",
    "    elif similarity_type == \"euclidean\":\n",
    "        hyperparameters_range_dictionary[\"normalize\"] = Categorical([True, False])\n",
    "        hyperparameters_range_dictionary[\"normalize_avg_row\"] = Categorical([True, False])\n",
    "        hyperparameters_range_dictionary[\"similarity_from_distance_mode\"] = Categorical([\"lin\", \"log\", \"exp\"])\n",
    "\n",
    "    if not is_set_similarity:\n",
    "\n",
    "        if allow_weighting:\n",
    "            hyperparameters_range_dictionary[\"feature_weighting\"] = Categorical([\"none\", \"BM25\", \"TF-IDF\"])\n",
    "\n",
    "    local_parameter_search_space = {**hyperparameters_range_dictionary, **original_parameter_search_space}\n",
    "\n",
    "    parameterSearch.search(recommender_input_args,\n",
    "                           parameter_search_space=local_parameter_search_space,\n",
    "                           n_cases=n_cases,\n",
    "                           n_random_starts=n_random_starts,\n",
    "                           resume_from_saved=resume_from_saved,\n",
    "                           save_model=save_model,\n",
    "                           output_folder_path=output_folder_path,\n",
    "                           output_file_name_root=output_file_name_root + \"_\" + similarity_type,\n",
    "                           metric_to_optimize=metric_to_optimize,\n",
    "                           recommender_input_args_last_test=recommender_input_args_last_test)\n",
    "\n",
    "\n",
    "def runParameterSearch_Content(recommender_class, URM_train, ICM_object, ICM_name, URM_train_last_test=None,\n",
    "                               n_cases=30, n_random_starts=5, resume_from_saved=False, save_model=\"best\",\n",
    "                               evaluator_validation=None, evaluator_test=None, metric_to_optimize=\"PRECISION\",\n",
    "                               output_folder_path=\"result_experiments/\", parallelizeKNN=False, allow_weighting=True,\n",
    "                               similarity_type_list=None):\n",
    "    # If directory does not exist, create\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    URM_train = URM_train.copy()\n",
    "    ICM_object = ICM_object.copy()\n",
    "\n",
    "    if URM_train_last_test is not None:\n",
    "        URM_train_last_test = URM_train_last_test.copy()\n",
    "\n",
    "    ##########################################################################################################\n",
    "\n",
    "    output_file_name_root = recommender_class.RECOMMENDER_NAME + \"_{}\".format(ICM_name)\n",
    "\n",
    "    parameterSearch = SearchBayesianSkopt(recommender_class,\n",
    "                                          evaluator_validation=evaluator_validation,\n",
    "                                          evaluator_test=evaluator_test)\n",
    "\n",
    "    if similarity_type_list is None:\n",
    "        similarity_type_list = ['cosine', 'jaccard', \"asymmetric\", \"dice\", \"tversky\"]\n",
    "\n",
    "    recommender_input_args = SearchInputRecommenderArgs(\n",
    "        CONSTRUCTOR_POSITIONAL_ARGS=[URM_train, ICM_object],\n",
    "        CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        FIT_POSITIONAL_ARGS=[],\n",
    "        FIT_KEYWORD_ARGS={}\n",
    "    )\n",
    "\n",
    "    if URM_train_last_test is not None:\n",
    "        recommender_input_args_last_test = recommender_input_args.copy()\n",
    "        recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n",
    "    else:\n",
    "        recommender_input_args_last_test = None\n",
    "\n",
    "    run_KNNCBFRecommender_on_similarity_type_partial = partial(run_KNNRecommender_on_similarity_type,\n",
    "                                                               recommender_input_args=recommender_input_args,\n",
    "                                                               parameter_search_space={},\n",
    "                                                               parameterSearch=parameterSearch,\n",
    "                                                               n_cases=n_cases,\n",
    "                                                               n_random_starts=n_random_starts,\n",
    "                                                               resume_from_saved=resume_from_saved,\n",
    "                                                               save_model=save_model,\n",
    "                                                               output_folder_path=output_folder_path,\n",
    "                                                               output_file_name_root=output_file_name_root,\n",
    "                                                               metric_to_optimize=metric_to_optimize,\n",
    "                                                               allow_weighting=allow_weighting,\n",
    "                                                               recommender_input_args_last_test=recommender_input_args_last_test)\n",
    "\n",
    "    # if parallelizeKNN:\n",
    "    #     pool = multiprocessing.Pool(processes=int(multiprocessing.cpu_count()), maxtasksperchild=1)\n",
    "    #     pool.map(run_KNNCBFRecommender_on_similarity_type_partial, similarity_type_list)\n",
    "    #\n",
    "    #     pool.close()\n",
    "    #     pool.join()\n",
    "    #\n",
    "    # else:\n",
    "    #\n",
    "    for similarity_type in similarity_type_list:\n",
    "        run_KNNCBFRecommender_on_similarity_type_partial(similarity_type)\n",
    "\n",
    "\n",
    "def runParameterSearch_Collaborative(recommender_class, URM_train, URM_train_last_test=None,\n",
    "                                     metric_to_optimize=\"PRECISION\",\n",
    "                                     evaluator_validation=None, evaluator_test=None,\n",
    "                                     evaluator_validation_earlystopping=None,\n",
    "                                     output_folder_path=\"result_experiments/\", parallelizeKNN=True,\n",
    "                                     n_cases=35, n_random_starts=5, resume_from_saved=False, save_model=\"best\",\n",
    "                                     allow_weighting=True,\n",
    "                                     similarity_type_list=None):\n",
    "\n",
    "    # If directory does not exist, create\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    earlystopping_keywargs = {\"validation_every_n\": 5,\n",
    "                              \"stop_on_validation\": True,\n",
    "                              \"evaluator_object\": evaluator_validation_earlystopping,\n",
    "                              \"lower_validations_allowed\": 5,\n",
    "                              \"validation_metric\": metric_to_optimize,\n",
    "                              }\n",
    "\n",
    "    URM_train = URM_train.copy()\n",
    "\n",
    "    if URM_train_last_test is not None:\n",
    "        URM_train_last_test = URM_train_last_test.copy()\n",
    "\n",
    "    try:\n",
    "\n",
    "        output_file_name_root = recommender_class.RECOMMENDER_NAME\n",
    "\n",
    "        parameterSearch = SearchBayesianSkopt(recommender_class,\n",
    "                                              evaluator_validation=evaluator_validation,\n",
    "                                              evaluator_test=evaluator_test)\n",
    "\n",
    "        # if recommender_class in [TopPop, GlobalEffects, Random]:\n",
    "        #     \"\"\"\n",
    "        #     TopPop, GlobalEffects and Random have no parameters therefore only one evaluation is needed\n",
    "        #     \"\"\"\n",
    "        #\n",
    "        #     parameterSearch = SearchSingleCase(recommender_class,\n",
    "        #                                        evaluator_validation=evaluator_validation,\n",
    "        #                                        evaluator_test=evaluator_test)\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={}\n",
    "        #     )\n",
    "        #\n",
    "        #     if URM_train_last_test is not None:\n",
    "        #         recommender_input_args_last_test = recommender_input_args.copy()\n",
    "        #         recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n",
    "        #     else:\n",
    "        #         recommender_input_args_last_test = None\n",
    "        #\n",
    "        #     parameterSearch.search(recommender_input_args,\n",
    "        #                            recommender_input_args_last_test=recommender_input_args_last_test,\n",
    "        #                            fit_hyperparameters_values={},\n",
    "        #                            output_folder_path=output_folder_path,\n",
    "        #                            output_file_name_root=output_file_name_root,\n",
    "        #                            resume_from_saved=resume_from_saved,\n",
    "        #                            save_model=save_model,\n",
    "        #                            )\n",
    "        #\n",
    "        #     return\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "#         if recommender_class in [ItemKNNCFRecommender, UserKNNCFRecommender]:\n",
    "\n",
    "#             if similarity_type_list is None:\n",
    "#                 similarity_type_list = ['cosine', 'jaccard', \"asymmetric\", \"dice\", \"tversky\"]\n",
    "\n",
    "#             recommender_input_args = SearchInputRecommenderArgs(\n",
    "#                 CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "#                 CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "#                 FIT_POSITIONAL_ARGS=[],\n",
    "#                 FIT_KEYWORD_ARGS={}\n",
    "#             )\n",
    "\n",
    "#             if URM_train_last_test is not None:\n",
    "#                 recommender_input_args_last_test = recommender_input_args.copy()\n",
    "#                 recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n",
    "#             else:\n",
    "#                 recommender_input_args_last_test = None\n",
    "\n",
    "#             run_KNNCFRecommender_on_similarity_type_partial = partial(run_KNNRecommender_on_similarity_type,\n",
    "#                                                                       recommender_input_args=recommender_input_args,\n",
    "#                                                                       parameter_search_space={},\n",
    "#                                                                       parameterSearch=parameterSearch,\n",
    "#                                                                       n_cases=n_cases,\n",
    "#                                                                       n_random_starts=n_random_starts,\n",
    "#                                                                       resume_from_saved=resume_from_saved,\n",
    "#                                                                       save_model=save_model,\n",
    "#                                                                       output_folder_path=output_folder_path,\n",
    "#                                                                       output_file_name_root=output_file_name_root,\n",
    "#                                                                       metric_to_optimize=metric_to_optimize,\n",
    "#                                                                       allow_weighting=allow_weighting,\n",
    "#                                                                       recommender_input_args_last_test=recommender_input_args_last_test)\n",
    "\n",
    "            # if parallelizeKNN:\n",
    "            #     pool = multiprocessing.Pool(processes=multiprocessing.cpu_count(), maxtasksperchild=1)\n",
    "            #     pool.map(run_KNNCFRecommender_on_similarity_type_partial, similarity_type_list)\n",
    "            #\n",
    "            #     pool.close()\n",
    "            #     pool.join()\n",
    "            #\n",
    "            # else:\n",
    "\n",
    "#             for similarity_type in similarity_type_list:\n",
    "#                 run_KNNCFRecommender_on_similarity_type_partial(similarity_type)\n",
    "\n",
    "#             return\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        # if recommender_class is P3alphaRecommender:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n",
    "        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=0, high=2, prior='uniform')\n",
    "        #     hyperparameters_range_dictionary[\"normalize_similarity\"] = Categorical([True, False])\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={}\n",
    "        #     )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        # if recommender_class is RP3betaRecommender:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n",
    "        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=0, high=2, prior='uniform')\n",
    "        #     hyperparameters_range_dictionary[\"beta\"] = Real(low=0, high=2, prior='uniform')\n",
    "        #     hyperparameters_range_dictionary[\"normalize_similarity\"] = Categorical([True, False])\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={}\n",
    "        #     )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        if recommender_class is MatrixFactorization_FunkSVD_Cython:\n",
    "            hyperparameters_range_dictionary = {}\n",
    "            hyperparameters_range_dictionary[\"sgd_mode\"] = Categorical([\"sgd\", \"adagrad\", \"adam\"])\n",
    "            hyperparameters_range_dictionary[\"epochs\"] = Categorical([500])\n",
    "            hyperparameters_range_dictionary[\"use_bias\"] = Categorical([True, False])\n",
    "            hyperparameters_range_dictionary[\"batch_size\"] = Categorical([1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024])\n",
    "            hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n",
    "            hyperparameters_range_dictionary[\"item_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "            hyperparameters_range_dictionary[\"user_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "            hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n",
    "            hyperparameters_range_dictionary[\"negative_interactions_quota\"] = Real(low=0.0, high=0.5, prior='uniform')\n",
    "        \n",
    "            recommender_input_args = SearchInputRecommenderArgs(\n",
    "                CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "                CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "                FIT_POSITIONAL_ARGS=[],\n",
    "                FIT_KEYWORD_ARGS=earlystopping_keywargs\n",
    "            )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        # if recommender_class is MatrixFactorization_AsySVD_Cython:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"sgd_mode\"] = Categoricl([\"sgd\", \"adagrad\", \"adam\"])\n",
    "        #     hyperparameters_range_dictionary[\"epochs\"] = Categorical([500])\n",
    "        #     hyperparameters_range_dictionary[\"use_bias\"] = Categorical([True, False])\n",
    "        #     hyperparameters_range_dictionary[\"batch_size\"] = Categorical([1])\n",
    "        #     hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n",
    "        #     hyperparameters_range_dictionary[\"item_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"user_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"negative_interactions_quota\"] = Real(low=0.0, high=0.5, prior='uniform')\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS=earlystopping_keywargs\n",
    "        #     )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        if recommender_class is MatrixFactorization_BPR_Cython:\n",
    "            hyperparameters_range_dictionary = {}\n",
    "            hyperparameters_range_dictionary[\"sgd_mode\"] = Categorical([\"sgd\", \"adagrad\", \"adam\"])\n",
    "            hyperparameters_range_dictionary[\"epochs\"] = Categorical([1000])  # 1500\n",
    "            hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n",
    "            hyperparameters_range_dictionary[\"batch_size\"] = Categorical([1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024])\n",
    "            hyperparameters_range_dictionary[\"positive_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "            hyperparameters_range_dictionary[\"negative_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "            hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n",
    "        \n",
    "            recommender_input_args = SearchInputRecommenderArgs(\n",
    "                CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "                CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "                FIT_POSITIONAL_ARGS=[],\n",
    "                FIT_KEYWORD_ARGS={**earlystopping_keywargs,\n",
    "                                  \"positive_threshold_BPR\": None}\n",
    "            )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        # if recommender_class is IALSRecommender:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n",
    "        #     hyperparameters_range_dictionary[\"confidence_scaling\"] = Categorical([\"linear\", \"log\"])\n",
    "        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=1e-3, high=50.0, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"epsilon\"] = Real(low=1e-3, high=10.0, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS=earlystopping_keywargs\n",
    "        #     )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "#         if recommender_class is PureSVDRecommender:\n",
    "#             hyperparameters_range_dictionary = {}\n",
    "#             hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 350)\n",
    "\n",
    "#             recommender_input_args = SearchInputRecommenderArgs(\n",
    "#                 CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "#                 CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "#                 FIT_POSITIONAL_ARGS=[],\n",
    "#                 FIT_KEYWORD_ARGS={}\n",
    "#             )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        # if recommender_class is NMFRecommender:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 350)\n",
    "        #     hyperparameters_range_dictionary[\"solver\"] = Categorical([\"coordinate_descent\", \"multiplicative_update\"])\n",
    "        #     hyperparameters_range_dictionary[\"init_type\"] = Categorical([\"random\", \"nndsvda\"])\n",
    "        #     hyperparameters_range_dictionary[\"beta_loss\"] = Categorical([\"frobenius\", \"kullback-leibler\"])\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={}\n",
    "        #     )\n",
    "\n",
    "        #########################################################################################################\n",
    "\n",
    "        # if recommender_class is SLIM_BPR_Cython:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n",
    "        #     hyperparameters_range_dictionary[\"epochs\"] = Categorical([1500])\n",
    "        #     hyperparameters_range_dictionary[\"symmetric\"] = Categorical([True, False])\n",
    "        #     hyperparameters_range_dictionary[\"sgd_mode\"] = Categorical([\"sgd\", \"adagrad\", \"adam\"])\n",
    "        #     hyperparameters_range_dictionary[\"lambda_i\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"lambda_j\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={**earlystopping_keywargs,\n",
    "        #                           \"positive_threshold_BPR\": None,\n",
    "        #                           'train_with_sparse_weights': None}\n",
    "        #     )\n",
    "\n",
    "        ##########################################################################################################\n",
    "        #\n",
    "        # if recommender_class is SLIMElasticNetRecommender:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n",
    "        #     hyperparameters_range_dictionary[\"l1_ratio\"] = Real(low=1e-5, high=1.0, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=1e-3, high=1.0, prior='uniform')\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={}\n",
    "        #     )\n",
    "\n",
    "        #########################################################################################################\n",
    "\n",
    "        if URM_train_last_test is not None:\n",
    "            recommender_input_args_last_test = recommender_input_args.copy()\n",
    "            recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n",
    "        else:\n",
    "            recommender_input_args_last_test = None\n",
    "\n",
    "        ## Final step, after the hyperparameter range has been defined for each type of algorithm\n",
    "        parameterSearch.search(recommender_input_args,\n",
    "                               parameter_search_space=hyperparameters_range_dictionary,\n",
    "                               n_cases=n_cases,\n",
    "                               n_random_starts=n_random_starts,\n",
    "                               resume_from_saved=resume_from_saved,\n",
    "                               save_model=save_model,\n",
    "                               output_folder_path=output_folder_path,\n",
    "                               output_file_name_root=output_file_name_root,\n",
    "                               metric_to_optimize=metric_to_optimize,\n",
    "                               recommender_input_args_last_test=recommender_input_args_last_test)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(\"On recommender {} Exception {}\".format(recommender_class, str(e)))\n",
    "        traceback.print_exc()\n",
    "\n",
    "        error_file = open(output_folder_path + \"ErrorLog.txt\", \"a\")\n",
    "        error_file.write(\"On recommender {} Exception {}\\n\".format(recommender_class, str(e)))\n",
    "        error_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Matrix Factorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaseRecommender.py\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from utils.compute_similarity import check_matrix\n",
    "\n",
    "class BaseRecommender(object):\n",
    "    \"\"\"Abstract BaseRecommender\"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"Recommender_Base_Class\"\n",
    "\n",
    "    def __init__(self, URM_train, verbose=True):\n",
    "\n",
    "        super(BaseRecommender, self).__init__()\n",
    "\n",
    "        self.URM_train = check_matrix(URM_train.copy(), 'csr', dtype=np.float32)\n",
    "        self.URM_train.eliminate_zeros()\n",
    "\n",
    "        self.n_users, self.n_items = self.URM_train.shape\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.filterTopPop = False\n",
    "        self.filterTopPop_ItemsID = np.array([], dtype=np.int)\n",
    "\n",
    "        self.items_to_ignore_flag = False\n",
    "        self.items_to_ignore_ID = np.array([], dtype=np.int)\n",
    "\n",
    "        self._cold_user_mask = np.ediff1d(self.URM_train.indptr) == 0\n",
    "\n",
    "        if self._cold_user_mask.any():\n",
    "            self._print(\"URM Detected {} ({:.2f} %) cold users.\".format(\n",
    "                self._cold_user_mask.sum(), self._cold_user_mask.sum()/self.n_users*100))\n",
    "\n",
    "\n",
    "        self._cold_item_mask = np.ediff1d(self.URM_train.tocsc().indptr) == 0\n",
    "\n",
    "        if self._cold_item_mask.any():\n",
    "            self._print(\"URM Detected {} ({:.2f} %) cold items.\".format(\n",
    "                self._cold_item_mask.sum(), self._cold_item_mask.sum()/self.n_items*100))\n",
    "\n",
    "\n",
    "    def _get_cold_user_mask(self):\n",
    "        return self._cold_user_mask\n",
    "\n",
    "    def _get_cold_item_mask(self):\n",
    "        return self._cold_item_mask\n",
    "\n",
    "\n",
    "    def _print(self, string):\n",
    "        if self.verbose:\n",
    "            print(\"{}: {}\".format(self.RECOMMENDER_NAME, string))\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def get_URM_train(self):\n",
    "        return self.URM_train.copy()\n",
    "\n",
    "    def set_items_to_ignore(self, items_to_ignore):\n",
    "        self.items_to_ignore_flag = True\n",
    "        self.items_to_ignore_ID = np.array(items_to_ignore, dtype=np.int)\n",
    "\n",
    "    def reset_items_to_ignore(self):\n",
    "        self.items_to_ignore_flag = False\n",
    "        self.items_to_ignore_ID = np.array([], dtype=np.int)\n",
    "\n",
    "\n",
    "    #########################################################################################################\n",
    "    ##########                                                                                     ##########\n",
    "    ##########                     COMPUTE AND FILTER RECOMMENDATION LIST                          ##########\n",
    "    ##########                                                                                     ##########\n",
    "    #########################################################################################################\n",
    "\n",
    "\n",
    "    def _remove_TopPop_on_scores(self, scores_batch):\n",
    "        scores_batch[:, self.filterTopPop_ItemsID] = -np.inf\n",
    "        return scores_batch\n",
    "\n",
    "\n",
    "    def _remove_custom_items_on_scores(self, scores_batch):\n",
    "        scores_batch[:, self.items_to_ignore_ID] = -np.inf\n",
    "        return scores_batch\n",
    "\n",
    "\n",
    "    def _remove_seen_on_scores(self, user_id, scores):\n",
    "\n",
    "        assert self.URM_train.getformat() == \"csr\", \"Recommender_Base_Class: URM_train is not CSR, this will cause errors in filtering seen items\"\n",
    "\n",
    "        seen = self.URM_train.indices[self.URM_train.indptr[user_id]:self.URM_train.indptr[user_id + 1]]\n",
    "\n",
    "        scores[seen] = -np.inf\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _compute_item_score(self, user_id_array, items_to_compute = None):\n",
    "        \"\"\"\n",
    "        :param user_id_array:       array containing the user indices whose recommendations need to be computed\n",
    "        :param items_to_compute:    array containing the items whose scores are to be computed.\n",
    "                                        If None, all items are computed, otherwise discarded items will have as score -np.inf\n",
    "        :return:                    array (len(user_id_array), n_items) with the score.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"BaseRecommender: compute_item_score not assigned for current recommender, unable to compute prediction scores\")\n",
    "\n",
    "\n",
    "    def recommend(self, user_id_array, cutoff = None, remove_seen_flag=True, items_to_compute = None,\n",
    "                  remove_top_pop_flag = False, remove_custom_items_flag = False, return_scores = False):\n",
    "\n",
    "        # If is a scalar transform it in a 1-cell array\n",
    "        if np.isscalar(user_id_array):\n",
    "            user_id_array = np.atleast_1d(user_id_array)\n",
    "            single_user = True\n",
    "        else:\n",
    "            single_user = False\n",
    "\n",
    "        if cutoff is None:\n",
    "            cutoff = self.URM_train.shape[1] - 1\n",
    "\n",
    "        # Compute the scores using the model-specific function\n",
    "        # Vectorize over all users in user_id_array\n",
    "        scores_batch = self._compute_item_score(user_id_array, items_to_compute=items_to_compute)\n",
    "\n",
    "\n",
    "        for user_index in range(len(user_id_array)):\n",
    "\n",
    "            user_id = user_id_array[user_index]\n",
    "\n",
    "            if remove_seen_flag:\n",
    "                scores_batch[user_index,:] = self._remove_seen_on_scores(user_id, scores_batch[user_index, :])\n",
    "\n",
    "            # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "            # - Partition the data to extract the set of relevant items\n",
    "            # - Sort only the relevant items\n",
    "            # - Get the original item index\n",
    "            # relevant_items_partition = (-scores_user).argpartition(cutoff)[0:cutoff]\n",
    "            # relevant_items_partition_sorting = np.argsort(-scores_user[relevant_items_partition])\n",
    "            # ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "            #\n",
    "            # ranking_list.append(ranking)\n",
    "\n",
    "\n",
    "        if remove_top_pop_flag:\n",
    "            scores_batch = self._remove_TopPop_on_scores(scores_batch)\n",
    "\n",
    "        if remove_custom_items_flag:\n",
    "            scores_batch = self._remove_custom_items_on_scores(scores_batch)\n",
    "\n",
    "        # relevant_items_partition is block_size x cutoff\n",
    "        relevant_items_partition = (-scores_batch).argpartition(cutoff, axis=1)[:,0:cutoff]\n",
    "\n",
    "        # Get original value and sort it\n",
    "        # [:, None] adds 1 dimension to the array, from (block_size,) to (block_size,1)\n",
    "        # This is done to correctly get scores_batch value as [row, relevant_items_partition[row,:]]\n",
    "        relevant_items_partition_original_value = scores_batch[np.arange(scores_batch.shape[0])[:, None], relevant_items_partition]\n",
    "        relevant_items_partition_sorting = np.argsort(-relevant_items_partition_original_value, axis=1)\n",
    "        ranking = relevant_items_partition[np.arange(relevant_items_partition.shape[0])[:, None], relevant_items_partition_sorting]\n",
    "\n",
    "        ranking_list = [None] * ranking.shape[0]\n",
    "\n",
    "        # Remove from the recommendation list any item that has a -inf score\n",
    "        # Since -inf is a flag to indicate an item to remove\n",
    "        for user_index in range(len(user_id_array)):\n",
    "            user_recommendation_list = ranking[user_index]\n",
    "            user_item_scores = scores_batch[user_index, user_recommendation_list]\n",
    "\n",
    "            not_inf_scores_mask = np.logical_not(np.isinf(user_item_scores))\n",
    "\n",
    "            user_recommendation_list = user_recommendation_list[not_inf_scores_mask]\n",
    "            ranking_list[user_index] = user_recommendation_list.tolist()\n",
    "\n",
    "\n",
    "\n",
    "        # Return single list for one user, instead of list of lists\n",
    "        if single_user:\n",
    "            ranking_list = ranking_list[0]\n",
    "\n",
    "\n",
    "        if return_scores:\n",
    "            return ranking_list, scores_batch\n",
    "\n",
    "        else:\n",
    "            return ranking_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental_Training_Early_Stopping.py\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 06/07/2018\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "import time, sys\n",
    "import numpy as np\n",
    "# from Base.BaseTempFolder import BaseTempFolder\n",
    "# from utils.Evaluation.Utils.seconds_to_biggest_unit import seconds_to_biggest_unit\n",
    "\n",
    "\n",
    "class Incremental_Training_Early_Stopping(object):\n",
    "    \"\"\"\n",
    "    This class provides a function which trains a model applying early stopping\n",
    "    The term \"incremental\" refers to the model that is updated at every epoch\n",
    "    The term \"best\" refers to the incremental model which corresponded to the best validation score\n",
    "    The object must implement the following methods:\n",
    "    _run_epoch(self, num_epoch)                 : trains the model for one epoch (e.g. calling another object implementing the training cython, pyTorch...)\n",
    "    _prepare_model_for_validation(self)         : ensures the recommender being trained can compute the predictions needed for the validation step\n",
    "    _update_best_model(self)                    : updates the best model with the current incremental one\n",
    "    _train_with_early_stopping(.)               : Function that executes the training, validation and early stopping by using the previously implemented functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Incremental_Training_Early_Stopping, self).__init__()\n",
    "\n",
    "\n",
    "    def get_early_stopping_final_epochs_dict(self):\n",
    "        \"\"\"\n",
    "        This function returns a dictionary to be used as optimal parameters in the .fit() function\n",
    "        It provides the flexibility to deal with multiple early-stopping in a single algorithm\n",
    "        e.g. in NeuMF there are three model components each with its own optimal number of epochs\n",
    "        the return dict would be {\"epochs\": epochs_best_neumf, \"epochs_gmf\": epochs_best_gmf, \"epochs_mlp\": epochs_best_mlp}\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        return {\"epochs\": self.epochs_best}\n",
    "\n",
    "\n",
    "    def _run_epoch(self, num_epoch):\n",
    "        \"\"\"\n",
    "        This function should run a single epoch on the object you train. This may either involve calling a function to do an epoch\n",
    "        on a Cython object or a loop on the data points directly in python\n",
    "        :param num_epoch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _prepare_model_for_validation(self):\n",
    "        \"\"\"\n",
    "        This function is executed before the evaluation of the current model\n",
    "        It should ensure the current object \"self\" can be passed to the evaluator object\n",
    "        E.G. if the epoch is done via Cython or PyTorch, this function should get the new parameter values from\n",
    "        the cython or pytorch objects into the self. pyhon object\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _update_best_model(self):\n",
    "        \"\"\"\n",
    "        This function is called when the incremental model is found to have better validation score than the current best one\n",
    "        So the current best model should be replaced by the current incremental one.\n",
    "        Important, remember to clone the objects and NOT to create a pointer-reference, otherwise the best solution will be altered\n",
    "        by the next epoch\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "    def _train_with_early_stopping(self, epochs_max, epochs_min = 0,\n",
    "                                   validation_every_n = None, stop_on_validation = False,\n",
    "                                   validation_metric = None, lower_validations_allowed = None, evaluator_object = None,\n",
    "                                   algorithm_name = \"Incremental_Training_Early_Stopping\"):\n",
    "        \"\"\"\n",
    "        :param epochs_max:                  max number of epochs the training will last\n",
    "        :param epochs_min:                  min number of epochs the training will last\n",
    "        :param validation_every_n:          number of epochs after which the model will be evaluated and a best_model selected\n",
    "        :param stop_on_validation:          [True/False] whether to stop the training before the max number of epochs\n",
    "        :param validation_metric:           which metric to use when selecting the best model, higher values are better\n",
    "        :param lower_validations_allowed:    number of contiguous validation steps required for the tranining to early-stop\n",
    "        :param evaluator_object:            evaluator instance used to compute the validation metrics.\n",
    "                                                If multiple cutoffs are available, the first one is used\n",
    "        :param algorithm_name:              name of the algorithm to be displayed in the output updates\n",
    "        :return: -\n",
    "        Supported uses:\n",
    "        - Train for max number of epochs with no validation nor early stopping:\n",
    "            _train_with_early_stopping(epochs_max = 100,\n",
    "                                        evaluator_object = None\n",
    "                                        epochs_min,                 not used\n",
    "                                        validation_every_n,         not used\n",
    "                                        stop_on_validation,         not used\n",
    "                                        validation_metric,          not used\n",
    "                                        lower_validations_allowed,   not used\n",
    "                                        )\n",
    "        - Train for max number of epochs with validation but NOT early stopping:\n",
    "            _train_with_early_stopping(epochs_max = 100,\n",
    "                                        evaluator_object = evaluator\n",
    "                                        stop_on_validation = False\n",
    "                                        validation_every_n = int value\n",
    "                                        validation_metric = metric name string\n",
    "                                        epochs_min,                 not used\n",
    "                                        lower_validations_allowed,   not used\n",
    "                                        )\n",
    "        - Train for max number of epochs with validation AND early stopping:\n",
    "            _train_with_early_stopping(epochs_max = 100,\n",
    "                                        epochs_min = int value\n",
    "                                        evaluator_object = evaluator\n",
    "                                        stop_on_validation = True\n",
    "                                        validation_every_n = int value\n",
    "                                        validation_metric = metric name string\n",
    "                                        lower_validations_allowed = int value\n",
    "                                        )\n",
    "        \"\"\"\n",
    "\n",
    "        assert epochs_max >= 0, \"{}: Number of epochs_max must be >= 0, passed was {}\".format(algorithm_name, epochs_max)\n",
    "        assert epochs_min >= 0, \"{}: Number of epochs_min must be >= 0, passed was {}\".format(algorithm_name, epochs_min)\n",
    "        assert epochs_min <= epochs_max, \"{}: epochs_min must be <= epochs_max, passed are epochs_min {}, epochs_max {}\".format(algorithm_name, epochs_min, epochs_max)\n",
    "\n",
    "        # Train for max number of epochs with no validation nor early stopping\n",
    "        # OR Train for max number of epochs with validation but NOT early stopping\n",
    "        # OR Train for max number of epochs with validation AND early stopping\n",
    "        assert evaluator_object is None or\\\n",
    "               (evaluator_object is not None and not stop_on_validation and validation_every_n is not None and validation_metric is not None) or\\\n",
    "               (evaluator_object is not None and stop_on_validation and validation_every_n is not None and validation_metric is not None and lower_validations_allowed is not None),\\\n",
    "            \"{}: Inconsistent parameters passed, please check the supported uses\".format(algorithm_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.best_validation_metric = None\n",
    "        lower_validatons_count = 0\n",
    "        convergence = False\n",
    "\n",
    "        self.epochs_best = 0\n",
    "\n",
    "        epochs_current = 0\n",
    "\n",
    "        while epochs_current < epochs_max and not convergence:\n",
    "\n",
    "            self._run_epoch(epochs_current)\n",
    "\n",
    "            # If no validation required, always keep the latest\n",
    "            if evaluator_object is None:\n",
    "\n",
    "                self.epochs_best = epochs_current\n",
    "\n",
    "            # Determine whether a validaton step is required\n",
    "            elif (epochs_current + 1) % validation_every_n == 0:\n",
    "\n",
    "                print(\"{}: Validation begins...\".format(algorithm_name))\n",
    "\n",
    "                self._prepare_model_for_validation()\n",
    "\n",
    "                # If the evaluator validation has multiple cutoffs, choose the first one\n",
    "                results_run, results_run_string = evaluator_object.evaluateRecommender(self)\n",
    "                results_run = results_run[list(results_run.keys())[0]]\n",
    "\n",
    "                print(\"{}: {}\".format(algorithm_name, results_run_string))\n",
    "\n",
    "                # Update optimal model\n",
    "                current_metric_value = results_run[validation_metric]\n",
    "                #\n",
    "                # if not np.isfinite(current_metric_value):\n",
    "                #     if isinstance(self, BaseTempFolder):\n",
    "                #         # If the recommender uses BaseTempFolder, clean the temp folder\n",
    "                #         self._clean_temp_folder(temp_file_folder=self.temp_file_folder)\n",
    "                #\n",
    "                #     assert False, \"{}: metric value is not a finite number, terminating!\".format(self.RECOMMENDER_NAME)\n",
    "                #\n",
    "\n",
    "                if self.best_validation_metric is None or self.best_validation_metric < current_metric_value:\n",
    "\n",
    "                    print(\"{}: New best model found! Updating.\".format(algorithm_name))\n",
    "\n",
    "                    self.best_validation_metric = current_metric_value\n",
    "\n",
    "                    self._update_best_model()\n",
    "\n",
    "                    self.epochs_best = epochs_current +1\n",
    "                    lower_validatons_count = 0\n",
    "\n",
    "                else:\n",
    "                    lower_validatons_count += 1\n",
    "\n",
    "\n",
    "                if stop_on_validation and lower_validatons_count >= lower_validations_allowed and epochs_current >= epochs_min:\n",
    "                    convergence = True\n",
    "\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n",
    "\n",
    "                    print(\"{}: Convergence reached! Terminating at epoch {}. Best value for '{}' at epoch {} is {:.4f}. Elapsed time {:.2f} {}\".format(\n",
    "                        algorithm_name, epochs_current+1, validation_metric, self.epochs_best, self.best_validation_metric, new_time_value, new_time_unit))\n",
    "\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n",
    "\n",
    "            print(\"{}: Epoch {} of {}. Elapsed time {:.2f} {}\".format(\n",
    "                algorithm_name, epochs_current+1, epochs_max, new_time_value, new_time_unit))\n",
    "\n",
    "            epochs_current += 1\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            sys.stderr.flush()\n",
    "\n",
    "        # If no validation required, keep the latest\n",
    "        if evaluator_object is None:\n",
    "\n",
    "            self._prepare_model_for_validation()\n",
    "            self._update_best_model()\n",
    "\n",
    "\n",
    "        # Stop when max epochs reached and not early-stopping\n",
    "        if not convergence:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n",
    "\n",
    "            if evaluator_object is not None and self.best_validation_metric is not None:\n",
    "                print(\"{}: Terminating at epoch {}. Best value for '{}' at epoch {} is {:.4f}. Elapsed time {:.2f} {}\".format(\n",
    "                    algorithm_name, epochs_current, validation_metric, self.epochs_best, self.best_validation_metric, new_time_value, new_time_unit))\n",
    "            else:\n",
    "                print(\"{}: Terminating at epoch {}. Elapsed time {:.2f} {}\".format(\n",
    "                    algorithm_name, epochs_current, new_time_value, new_time_unit))\n",
    "\n",
    "                \n",
    "\n",
    "# seconds_to_biggest_unit.py\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 30/03/2019\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "def seconds_to_biggest_unit(time_in_seconds, data_array = None):\n",
    "\n",
    "    conversion_factor = [\n",
    "        (\"sec\", 60),\n",
    "        (\"min\", 60),\n",
    "        (\"hour\", 24),\n",
    "        (\"day\", 365),\n",
    "    ]\n",
    "\n",
    "    terminate = False\n",
    "    unit_index = 0\n",
    "\n",
    "    new_time_value = time_in_seconds\n",
    "    new_time_unit = \"sec\"\n",
    "\n",
    "    while not terminate:\n",
    "\n",
    "        next_time = new_time_value/conversion_factor[unit_index][1]\n",
    "\n",
    "        if next_time >= 1.0:\n",
    "            new_time_value = next_time\n",
    "\n",
    "            if data_array is not None:\n",
    "                data_array /= conversion_factor[unit_index][1]\n",
    "\n",
    "            unit_index += 1\n",
    "            new_time_unit = conversion_factor[unit_index][0]\n",
    "\n",
    "        else:\n",
    "            terminate = True\n",
    "\n",
    "\n",
    "    if data_array is not None:\n",
    "        return new_time_value, new_time_unit, data_array\n",
    "\n",
    "    else:\n",
    "        return new_time_value, new_time_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d606c1c376a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ---------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMF_MSE_PyTorch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# MF_MSE_PyTorch_model.py \n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class MF_MSE_PyTorch_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_users, n_items, n_factors):\n",
    "\n",
    "        super(MF_MSE_PyTorch_model, self).__init__()\n",
    "\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "\n",
    "        self.user_factors = torch.nn.Embedding(num_embeddings = self.n_users, embedding_dim = self.n_factors)\n",
    "        self.item_factors = torch.nn.Embedding(num_embeddings = self.n_items, embedding_dim = self.n_factors)\n",
    "        \n",
    "        \"\"\" We define a single layer and an activation function, which takes the result and \n",
    "            transforms it in the final prediction. The activation function can be used to restrict \n",
    "            the predicted values (e.g., sigmoid is between 0 and 1)\"\"\"\n",
    "        \n",
    "        self.layer_1 = torch.nn.Linear(in_features = self.n_factors, out_features = 1)\n",
    "\n",
    "        self.activation_function = torch.nn.ReLU()\n",
    "\n",
    "    # implement the forward function which computes the prediction as we did before\n",
    "    def forward(self, user_coordinates, item_coordinates):\n",
    "\n",
    "        current_user_factors = self.user_factors(user_coordinates)\n",
    "        current_item_factors = self.item_factors(item_coordinates)\n",
    "\n",
    "        prediction = torch.mul(current_user_factors, current_item_factors)\n",
    "\n",
    "        prediction = self.layer_1(prediction)\n",
    "        prediction = self.activation_function(prediction)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def get_W(self):\n",
    "        return self.user_factors.weight.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    def get_H(self):\n",
    "        return self.item_factors.weight.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# DatasetIterator_URM.py\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\"\"\" # Define the DatasetIterator, which will be used to iterate over the data\n",
    "    # A DatasetIterator will implement the Dataset class and provide the getitem(self, index) method, \n",
    "    # which allows to get the data points indexed by that index.\n",
    "    # Since we need the data to be a tensor, we pre inizialize everything as a tensor. \n",
    "    # In practice we save the URM in coordinate format (user, item, rating)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class DatasetIterator_URM(Dataset):\n",
    "\n",
    "    def __init__(self, URM):\n",
    "\n",
    "        URM = URM.tocoo()\n",
    "\n",
    "        self.n_data_points = URM.nnz\n",
    "\n",
    "        self.user_item_coordinates = np.empty((self.n_data_points, 2))\n",
    "\n",
    "        self.user_item_coordinates[:,0] = URM.row.copy()\n",
    "        self.user_item_coordinates[:,1] = URM.col.copy()\n",
    "        self.rating = URM.data.copy().astype(np.float)\n",
    "\n",
    "        self.user_item_coordinates = torch.Tensor(self.user_item_coordinates).type(torch.LongTensor)\n",
    "        self.rating = torch.Tensor(self.rating)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Format is (row, col, data)\n",
    "        :param index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        return self.user_item_coordinates[index, :], self.rating[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data_points\n",
    "    \n",
    "    \n",
    "    \n",
    "# MF_MSE_PyTorch\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 06/07/2018\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "# from Base.Incremental_Training_Early_Stopping import Incremental_Training_Early_Stopping\n",
    "# from Base.BaseRecommender import BaseRecommender\n",
    "import os, sys\n",
    "import numpy as np, pickle\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class MF_MSE_PyTorch(BaseRecommender, Incremental_Training_Early_Stopping):\n",
    "\n",
    "    RECOMMENDER_NAME = \"MF_MSE_PyTorch_Recommender\"\n",
    "\n",
    "    def __init__(self, URM_train, positive_threshold=4):\n",
    "\n",
    "#         super(MF_MSE_PyTorch, self).__init__()\n",
    "\n",
    "        self.URM_train = URM_train\n",
    "        self.n_users = URM_train.shape[0]\n",
    "        self.n_items = URM_train.shape[1]\n",
    "        self.normalize = False\n",
    "        \n",
    "        self.positive_threshold = positive_threshold\n",
    "        self.compute_item_score = self.compute_score_MF\n",
    "\n",
    "        \n",
    "    def compute_score_MF(self, user_id):\n",
    "        scores_array = np.dot(self.W[user_id], self.H.T)\n",
    "        return scores_array\n",
    "\n",
    "    \n",
    "    def fit(self, epochs=30, batch_size = 128, num_factors=10,\n",
    "            learning_rate = 0.001, use_cuda = True,\n",
    "            **earlystopping_kwargs):\n",
    "\n",
    "        self.n_factors = num_factors\n",
    "\n",
    "        # Select only positive interactions\n",
    "        URM_train_positive = self.URM_train.copy()\n",
    "\n",
    "        URM_train_positive.data = URM_train_positive.data >= self.positive_threshold\n",
    "        URM_train_positive.eliminate_zeros()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "        ########################################################################################################\n",
    "        #\n",
    "        #                                SETUP PYTORCH MODEL AND DATA READER\n",
    "        #\n",
    "        ########################################################################################################\n",
    "\n",
    "        use_cuda = False\n",
    "\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "            print(\"MF_MSE_PyTorch: Using CUDA\")\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            print(\"MF_MSE_PyTorch: Using CPU\")\n",
    "\n",
    "#         from MatrixFactorization.PyTorch.MF_MSE_PyTorch_model import MF_MSE_PyTorch_model, DatasetIterator_URM\n",
    "\n",
    "        n_users, n_items = self.URM_train.shape\n",
    "    \n",
    "        # Create an instance of the model and specify the device it should run on\n",
    "        self.pyTorchModel = MF_MSE_PyTorch_model(n_users, n_items, self.n_factors).to(self.device)\n",
    "\n",
    "        # Choose loss functions, there are quite a few to choose from\n",
    "        self.lossFunction = torch.nn.MSELoss(size_average=False)\n",
    "        # self.lossFunction = torch.nn.BCELoss(size_average=False)\n",
    "        \n",
    "        # Select the optimizer to be used for the model parameters: Adam, AdaGrad, RMSProp etc..\n",
    "        self.optimizer = torch.optim.Adagrad(self.pyTorchModel.parameters(), lr = self.learning_rate)\n",
    "        \n",
    "        # Define dataset iterator\n",
    "        dataset_iterator = DatasetIterator_URM(self.URM_train)\n",
    "        # pass the DatasetIterator to a DataLoader object which manages the use of batches and so on\n",
    "        self.train_data_loader = DataLoader(dataset = dataset_iterator,\n",
    "                                       batch_size = self.batch_size,\n",
    "                                       shuffle = True,\n",
    "                                       #num_workers = 2,\n",
    "                                       )\n",
    "\n",
    "        ########################################################################################################\n",
    "\n",
    "        self._train_with_early_stopping(epochs,\n",
    "                                        algorithm_name = self.RECOMMENDER_NAME,\n",
    "                                        **earlystopping_kwargs)\n",
    "\n",
    "        self.ITEM_factors = self.W_best.copy()\n",
    "        self.USER_factors = self.H_best.copy()\n",
    "\n",
    "        self._print(\"Computing NMF decomposition... Done!\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        \n",
    "    def _initialize_incremental_model(self):\n",
    "        self.W_incremental = self.pyTorchModel.get_W()\n",
    "        self.W_best = self.W_incremental.copy()\n",
    "\n",
    "        self.H_incremental = self.pyTorchModel.get_H()\n",
    "        self.H_best = self.H_incremental.copy()\n",
    "\n",
    "        \n",
    "    def _update_incremental_model(self):\n",
    "        self.W_incremental = self.pyTorchModel.get_W()\n",
    "        self.H_incremental = self.pyTorchModel.get_H()\n",
    "\n",
    "        self.W = self.W_incremental.copy()\n",
    "        self.H = self.H_incremental.copy()\n",
    "\n",
    "        \n",
    "    def _update_best_model(self):\n",
    "        self.W_best = self.W_incremental.copy()\n",
    "        self.H_best = self.H_incremental.copy()\n",
    "\n",
    "    \n",
    "    def _run_epoch(self, num_epoch):\n",
    "        \"\"\" And now we ran the usual epoch steps\n",
    "            ------------------------------------\n",
    "            Data point sampling\n",
    "            Prediction computation\n",
    "            Loss function computation\n",
    "            Gradient computation\n",
    "            Update \n",
    "        \"\"\"\n",
    "        \n",
    "        for num_batch, (input_data, label) in enumerate(self.train_data_loader, 0):\n",
    "\n",
    "            if num_batch % 1000 == 0:\n",
    "                print(\"num_batch: {}\".format(num_batch))\n",
    "\n",
    "            # On windows requires int64, on ubuntu int32\n",
    "            #input_data_tensor = Variable(torch.from_numpy(np.asarray(input_data, dtype=np.int64))).to(self.device)\n",
    "            input_data_tensor = Variable(input_data).to(self.device)\n",
    "\n",
    "            label_tensor = Variable(label).to(self.device)\n",
    "\n",
    "            user_coordinates = input_data_tensor[:,0]\n",
    "            item_coordinates = input_data_tensor[:,1]\n",
    "\n",
    "            # FORWARD pass\n",
    "            prediction = self.pyTorchModel(user_coordinates, item_coordinates)\n",
    "\n",
    "            # Pass prediction and label removing last empty dimension of prediction\n",
    "            loss = self.lossFunction(prediction.view(-1), label_tensor)\n",
    "\n",
    "            # BACKWARD pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URM built!\n",
      "ICM built!\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MF_MSE_PyTorch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c6978a753926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# print(H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mrecommender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMF_MSE_PyTorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURM_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#epochs=30, batch_size = 128, num_factors=10, learning_rate = 0.001, use_cuda = True,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mresult_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluateRecommender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MF_MSE_PyTorch' is not defined"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import os, traceback\n",
    "import numpy as np\n",
    "\n",
    "URM_all = build_URM()\n",
    "ICM_all = build_ICM()\n",
    "# data_manager.get_statistics_URM(URM)\n",
    "\n",
    "######################################################################\n",
    "##########                                                  ##########\n",
    "##########      TRAINING, EVALUATION AND PREDICTIONS        ##########\n",
    "##########                                                  ##########\n",
    "######################################################################\n",
    "\n",
    "\n",
    "# URM train/validation/test splitting\n",
    "# -----------------------------------\n",
    "\n",
    "# from Data_manager.Movielens1M.Movielens1MReader import Movielens1MReader\n",
    "# from Data_manager.DataSplitter_k_fold_stratified import DataSplitter_Warm_k_fold\n",
    "\n",
    "# dataset_object = Movielens1MReader()\n",
    "# dataSplitter = DataSplitter_Warm_k_fold(dataset_object)\n",
    "# dataSplitter.load_data()\n",
    "# URM_train, URM_validation, URM_test = dataSplitter.get_holdout_split()\n",
    "\n",
    "URM_train, URM_test = split_train_validation_random_holdout(URM_all, train_split=0.8)\n",
    "URM_train, URM_validation = split_train_validation_random_holdout(URM_train, train_split=0.9)\n",
    "\n",
    "# ---------------------------------\n",
    "# Train a MF MSE model with PyTorch\n",
    "# ---------------------------------\n",
    "\n",
    "# In order to compute the prediction you have to:\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Define a list of user and item indices\n",
    "# Create a tensor from it\n",
    "# Create a variable from the tensor\n",
    "# Get the user and item embedding\n",
    "# Compute the element-wise product of the embeddings\n",
    "# Pass the element-wise product to the single layer network\n",
    "# Pass the output of the single layer network to the activation function\n",
    "\n",
    "# To compute the prediction we have to multiply the user factors to the item factors, \n",
    "# which is a linear operation.\n",
    "\n",
    "\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# item_index = [15]\n",
    "# user_index = [42]\n",
    "\n",
    "# user_index = torch.Tensor(user_index).type(torch.LongTensor)\n",
    "# item_index = torch.Tensor(item_index).type(torch.LongTensor)\n",
    "\n",
    "# user_index = Variable(user_index)\n",
    "# item_index = Variable(item_index)\n",
    "\n",
    "# current_user_factors = user_factors(user_index)\n",
    "# current_item_factors = item_factors(item_index)\n",
    "\n",
    "# element_wise_product = torch.mul(current_user_factors, current_item_factors)\n",
    "\n",
    "# prediction = layer_1(element_wise_product)\n",
    "# prediction = activation_function(prediction)\n",
    "\n",
    "# prediction_numpy = prediction.detach().numpy()\n",
    "\n",
    "# print(\"Prediction is {}\".format(prediction_numpy))\n",
    "\n",
    "\n",
    "# After the train is complete (it may take a while and many epochs), \n",
    "# we can get the matrices in the usual numpy format\n",
    "# W = pyTorchModel.get_W()\n",
    "# H = pyTorchModel.get_H()\n",
    "\n",
    "# print(W)\n",
    "# print(H)\n",
    "\n",
    "recommender = MF_MSE_PyTorch(URM_train)\n",
    "recommender.fit() #epochs=30, batch_size = 128, num_factors=10, learning_rate = 0.001, use_cuda = True,)\n",
    "result_dict, _ = evaluator_test.evaluateRecommender(recommender)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
