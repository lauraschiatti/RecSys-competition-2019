{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset/data_UCM_age.csv\n",
      "/kaggle/input/dataset/data_ICM_sub_class.csv\n",
      "/kaggle/input/dataset/data_ICM_asset.csv\n",
      "/kaggle/input/dataset/data_ICM_price.csv\n",
      "/kaggle/input/dataset/data_UCM_region.csv\n",
      "/kaggle/input/dataset/data_target_users_test.csv\n",
      "/kaggle/input/dataset/data_train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "dataset_dir = \"../input/dataset/\"\n",
    "\n",
    "# Interactions files (URM)\n",
    "data_train = dataset_dir + \"data_train.csv\"\n",
    "data_target_users = dataset_dir + \"data_target_users_test.csv\"\n",
    "\n",
    "# Item content files (ICM)\n",
    "data_ICM_asset = dataset_dir + \"/data_ICM_asset.csv\"  # description of the item (id)\n",
    "data_ICM_price = dataset_dir + \"/data_ICM_price.csv\"  # price of each item (already normalized)\n",
    "data_ICM_sub_class = dataset_dir + \"/data_ICM_sub_class.csv\"  # categorization of the item (number)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_manager.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# global vars\n",
    "user_list = []\n",
    "item_list = []\n",
    "n_interactions = 0\n",
    "n_users = 0\n",
    "n_items = 0\n",
    "n_subclass = 0\n",
    "\n",
    "\n",
    "def build_URM():\n",
    "    global user_list, item_list, n_interactions\n",
    "\n",
    "    matrix_tuples = []\n",
    "\n",
    "    with open(data_train, 'r') as file:  # read file's content\n",
    "        next(file)  # skip header row\n",
    "        for line in file:\n",
    "            if len(line.strip()) != 0:  # ignore lines with only whitespace\n",
    "                n_interactions += 1\n",
    "\n",
    "                # Create a tuple for each interaction (line in the file)\n",
    "                matrix_tuples.append(row_split(line))\n",
    "\n",
    "    # Separate user_id, item_id and rating\n",
    "    user_list, item_list, rating_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n",
    "\n",
    "    # Create lists of all users, items and contents (ratings)\n",
    "    user_list = list(user_list)  # row\n",
    "    item_list = list(item_list)  # col\n",
    "    rating_list = list(rating_list)  # data\n",
    "\n",
    "    URM = csr_sparse_matrix(rating_list, user_list, item_list)\n",
    "    \n",
    "    print(\"URM built!\")\n",
    "\n",
    "    return URM\n",
    "\n",
    "def build_ICM():\n",
    "    # features = [‘asset’, ’price’, ’subclass’] info about products\n",
    "    global n_subclass\n",
    "\n",
    "    # Load subclass data\n",
    "    matrix_tuples = []\n",
    "\n",
    "    with open(data_ICM_sub_class, 'r') as file:  # read file's content\n",
    "        next(file)  # skip header row\n",
    "        for line in file:\n",
    "            n_subclass += 1\n",
    "\n",
    "            # Create a tuple for each interaction (line in the file)\n",
    "            matrix_tuples.append(row_split(line))\n",
    "\n",
    "    # Separate user_id, item_id and rating\n",
    "    item_list, class_list, col_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n",
    "\n",
    "    # Convert values to list# Create lists of all users, items and contents (ratings)\n",
    "    item_list_icm = list(item_list)\n",
    "    class_list_icm = list(class_list)\n",
    "    col_list_icm = np.zeros(len(col_list))\n",
    "\n",
    "    # Number of items that are in the subclass list\n",
    "    num_items = max(item_list_icm) + 1\n",
    "    ICM_shape = (num_items, 1)\n",
    "    ICM_subclass = csr_sparse_matrix(class_list_icm, item_list_icm, col_list_icm, shape=ICM_shape)\n",
    "\n",
    "    # Load price data\n",
    "    matrix_tuples = []\n",
    "    n_prices = 0\n",
    "\n",
    "    with open(data_ICM_price, 'r') as file:  # read file's content\n",
    "        next(file)  # skip header row\n",
    "        for line in file:\n",
    "            n_prices += 1\n",
    "\n",
    "            # Create a tuple for each interaction (line in the file)\n",
    "            matrix_tuples.append(row_split(line))\n",
    "\n",
    "    # Separate user_id, item_id and rating\n",
    "    item_list, col_list, price_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n",
    "\n",
    "    # Convert values to list# Create lists of all users, items and contents (ratings)\n",
    "    item_list_icm = list(item_list)\n",
    "    col_list_icm = list(col_list)\n",
    "    price_list_icm = list(price_list)\n",
    "\n",
    "    ICM_price = csr_sparse_matrix(price_list_icm, item_list_icm, col_list_icm)\n",
    "\n",
    "    # Load asset data\n",
    "    matrix_tuples = []\n",
    "    n_assets = 0\n",
    "\n",
    "    with open(data_ICM_asset, 'r') as file:  # read file's content\n",
    "        next(file)  # skip header row\n",
    "        for line in file:\n",
    "            n_assets += 1\n",
    "\n",
    "            # Create a tuple for each interaction (line in the file)\n",
    "            matrix_tuples.append(row_split(line))\n",
    "\n",
    "    # Separate user_id, item_id and rating\n",
    "    item_list, col_list, asset_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n",
    "\n",
    "    # Convert values to list# Create lists of all users, items and contents (ratings)\n",
    "    item_list_icm = list(item_list)\n",
    "    col_list_icm = list(col_list)\n",
    "    asset_list_icm = list(asset_list)\n",
    "\n",
    "    ICM_asset = csr_sparse_matrix(asset_list_icm, item_list_icm, col_list_icm)\n",
    "\n",
    "    ICM_all = sps.hstack([ICM_price, ICM_asset, ICM_subclass], format='csr')\n",
    "\n",
    "    # item_feature_ratios(ICM_all)\n",
    "\n",
    "    print(\"ICM built!\\n\")\n",
    "\n",
    "    return ICM_all\n",
    "\n",
    "def row_split(row_string):\n",
    "    # file format: 0,3568,1.0\n",
    "\n",
    "    split = row_string.split(\",\")\n",
    "    split[2] = split[2].replace(\"\\n\", \"\")\n",
    "\n",
    "    split[0] = int(split[0])\n",
    "    split[1] = int(split[1])\n",
    "    split[2] = float(split[2])  # rating is a float\n",
    "\n",
    "    result = tuple(split)\n",
    "    return result\n",
    "\n",
    "def csr_sparse_matrix(data, row, col, shape=None):\n",
    "    csr_matrix = sps.coo_matrix((data, (row, col)), shape=shape)\n",
    "    csr_matrix = csr_matrix.tocsr()\n",
    "\n",
    "    return csr_matrix\n",
    "\n",
    "\n",
    "def get_target_users():\n",
    "    target_user_id_list = []\n",
    "\n",
    "    with open(data_target_users, 'r') as file:  # read file's content\n",
    "        next(file)  # skip header row\n",
    "        for line in file:\n",
    "            # each line is a user_id\n",
    "            target_user_id_list.append(int(line.strip()))  # remove trailing space\n",
    "\n",
    "    return target_user_id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Train, validation and test splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_splitter.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Random holdout split: take interactions randomly\n",
    "# and do not care about which users were involved in that interaction\n",
    "\n",
    "def split_train_validation_random_holdout(URM, train_split):\n",
    "    number_interactions = URM.nnz  # number of nonzero values\n",
    "    URM = URM.tocoo()  # Coordinate list matrix (COO)\n",
    "    shape = URM.shape\n",
    "\n",
    "    #  URM.row: user_list, URM.col: item_list, URM.data: rating_list\n",
    "\n",
    "    # Sampling strategy: take random samples of data using a boolean mask\n",
    "    train_mask = np.random.choice(\n",
    "        [True, False],\n",
    "        number_interactions,\n",
    "        p=[train_split, 1 - train_split])  # train_perc for True, 1-train_perc for False\n",
    "\n",
    "    URM_train = csr_sparse_matrix(URM.data[train_mask],\n",
    "                                  URM.row[train_mask],\n",
    "                                  URM.col[train_mask],\n",
    "                                  shape=shape)\n",
    "\n",
    "    test_mask = np.logical_not(train_mask)  # remaining samples\n",
    "    URM_test = csr_sparse_matrix(URM.data[test_mask],\n",
    "                                 URM.row[test_mask],\n",
    "                                 URM.col[test_mask],\n",
    "                                 shape=shape)\n",
    "\n",
    "    return URM_train, URM_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Performance evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 26/06/18\n",
    "\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import time, sys, copy\n",
    "\n",
    "from enum import Enum\n",
    "# from utils.Evaluation.Utils.seconds_to_biggest_unit import seconds_to_biggest_unit\n",
    "\n",
    "# from utils.Evaluation.metrics import roc_auc, precision, precision_recall_min_denominator, recall, MAP, MRR, ndcg, arhr, \\\n",
    "#     rmse, \\\n",
    "#     Novelty, Coverage_Item, Metrics_Object, Coverage_User, Gini_Diversity, Shannon_Entropy, Diversity_MeanInterList, \\\n",
    "#     Diversity_Herfindahl, AveragePopularity\n",
    "\n",
    "\n",
    "class EvaluatorMetrics(Enum):\n",
    "    ROC_AUC = \"ROC_AUC\"\n",
    "    PRECISION = \"PRECISION\"\n",
    "    PRECISION_RECALL_MIN_DEN = \"PRECISION_RECALL_MIN_DEN\"\n",
    "    RECALL = \"RECALL\"\n",
    "    MAP = \"MAP\"\n",
    "    MRR = \"MRR\"\n",
    "    NDCG = \"NDCG\"\n",
    "    F1 = \"F1\"\n",
    "    HIT_RATE = \"HIT_RATE\"\n",
    "    ARHR = \"ARHR\"\n",
    "    RMSE = \"RMSE\"\n",
    "    NOVELTY = \"NOVELTY\"\n",
    "    AVERAGE_POPULARITY = \"AVERAGE_POPULARITY\"\n",
    "    DIVERSITY_SIMILARITY = \"DIVERSITY_SIMILARITY\"\n",
    "    DIVERSITY_MEAN_INTER_LIST = \"DIVERSITY_MEAN_INTER_LIST\"\n",
    "    DIVERSITY_HERFINDAHL = \"DIVERSITY_HERFINDAHL\"\n",
    "    COVERAGE_ITEM = \"COVERAGE_ITEM\"\n",
    "    COVERAGE_USER = \"COVERAGE_USER\"\n",
    "    DIVERSITY_GINI = \"DIVERSITY_GINI\"\n",
    "    SHANNON_ENTROPY = \"SHANNON_ENTROPY\"\n",
    "\n",
    "\n",
    "def create_empty_metrics_dict(n_items, n_users, URM_train, ignore_items, ignore_users, cutoff,\n",
    "                              diversity_similarity_object):\n",
    "    empty_dict = {}\n",
    "\n",
    "    # from Base.Evaluation.ResultMetric import ResultMetric\n",
    "    # empty_dict = ResultMetric()\n",
    "\n",
    "    for metric in EvaluatorMetrics:\n",
    "        if metric == EvaluatorMetrics.COVERAGE_ITEM:\n",
    "            empty_dict[metric.value] = Coverage_Item(n_items, ignore_items)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.DIVERSITY_GINI:\n",
    "            empty_dict[metric.value] = Gini_Diversity(n_items, ignore_items)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.SHANNON_ENTROPY:\n",
    "            empty_dict[metric.value] = Shannon_Entropy(n_items, ignore_items)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.COVERAGE_USER:\n",
    "            empty_dict[metric.value] = Coverage_User(n_users, ignore_users)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.DIVERSITY_MEAN_INTER_LIST:\n",
    "            empty_dict[metric.value] = Diversity_MeanInterList(n_items, cutoff)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.DIVERSITY_HERFINDAHL:\n",
    "            empty_dict[metric.value] = Diversity_Herfindahl(n_items, ignore_items)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.NOVELTY:\n",
    "            empty_dict[metric.value] = Novelty(URM_train)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.AVERAGE_POPULARITY:\n",
    "            empty_dict[metric.value] = AveragePopularity(URM_train)\n",
    "\n",
    "        elif metric == EvaluatorMetrics.MAP:\n",
    "            empty_dict[metric.value] = MAP()\n",
    "\n",
    "        elif metric == EvaluatorMetrics.MRR:\n",
    "            empty_dict[metric.value] = MRR()\n",
    "\n",
    "        elif metric == EvaluatorMetrics.DIVERSITY_SIMILARITY:\n",
    "            if diversity_similarity_object is not None:\n",
    "                empty_dict[metric.value] = copy.deepcopy(diversity_similarity_object)\n",
    "        else:\n",
    "            empty_dict[metric.value] = 0.0\n",
    "\n",
    "    return empty_dict\n",
    "\n",
    "\n",
    "def get_result_string(results_run, n_decimals=7):\n",
    "    output_str = \"\"\n",
    "\n",
    "    for cutoff in results_run.keys():\n",
    "\n",
    "        results_run_current_cutoff = results_run[cutoff]\n",
    "\n",
    "        output_str += \"CUTOFF: {} - \".format(cutoff)\n",
    "\n",
    "        for metric in results_run_current_cutoff.keys():\n",
    "            output_str += \"{}: {:.{n_decimals}f}, \".format(metric, results_run_current_cutoff[metric],\n",
    "                                                           n_decimals=n_decimals)\n",
    "        output_str += \"\\n\"\n",
    "\n",
    "    return output_str\n",
    "\n",
    "\n",
    "def _remove_item_interactions(URM, item_list):\n",
    "    URM = sps.csc_matrix(URM.copy())\n",
    "\n",
    "    for item_index in item_list:\n",
    "        start_pos = URM.indptr[item_index]\n",
    "        end_pos = URM.indptr[item_index + 1]\n",
    "\n",
    "        URM.data[start_pos:end_pos] = np.zeros_like(URM.data[start_pos:end_pos])\n",
    "\n",
    "    URM.eliminate_zeros()\n",
    "    URM = sps.csr_matrix(URM)\n",
    "\n",
    "    return URM\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "    \"\"\"Abstract Evaluator\"\"\"\n",
    "\n",
    "    EVALUATOR_NAME = \"Evaluator_Base_Class\"\n",
    "\n",
    "    def __init__(self, URM_test_list, cutoff_list, minRatingsPerUser=1, exclude_seen=True,\n",
    "                 diversity_object=None,\n",
    "                 ignore_items=None,\n",
    "                 ignore_users=None,\n",
    "                 verbose=True):\n",
    "\n",
    "        super(Evaluator, self).__init__()\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if ignore_items is None:\n",
    "            self.ignore_items_flag = False\n",
    "            self.ignore_items_ID = np.array([])\n",
    "        else:\n",
    "            self._print(\"Ignoring {} Items\".format(len(ignore_items)))\n",
    "            self.ignore_items_flag = True\n",
    "            self.ignore_items_ID = np.array(ignore_items)\n",
    "\n",
    "        self.cutoff_list = cutoff_list.copy()\n",
    "        self.max_cutoff = max(self.cutoff_list)\n",
    "\n",
    "        self.minRatingsPerUser = minRatingsPerUser\n",
    "        self.exclude_seen = exclude_seen\n",
    "\n",
    "        if not isinstance(URM_test_list, list):\n",
    "            self.URM_test = URM_test_list.copy()\n",
    "            URM_test_list = [URM_test_list]\n",
    "        else:\n",
    "            raise ValueError(\"List of URM_test not supported\")\n",
    "\n",
    "        self.diversity_object = diversity_object\n",
    "\n",
    "        self.n_users, self.n_items = URM_test_list[0].shape\n",
    "\n",
    "        # Prune users with an insufficient number of ratings\n",
    "        # During testing CSR is faster\n",
    "        self.URM_test_list = []\n",
    "        usersToEvaluate_mask = np.zeros(self.n_users, dtype=np.bool)\n",
    "\n",
    "        for URM_test in URM_test_list:\n",
    "            URM_test = _remove_item_interactions(URM_test, self.ignore_items_ID)\n",
    "\n",
    "            URM_test = sps.csr_matrix(URM_test)\n",
    "            self.URM_test_list.append(URM_test)\n",
    "\n",
    "            rows = URM_test.indptr\n",
    "            numRatings = np.ediff1d(rows)\n",
    "            new_mask = numRatings >= minRatingsPerUser\n",
    "\n",
    "            usersToEvaluate_mask = np.logical_or(usersToEvaluate_mask, new_mask)\n",
    "\n",
    "        self.usersToEvaluate = np.arange(self.n_users)[usersToEvaluate_mask]\n",
    "\n",
    "        if ignore_users is not None:\n",
    "            self._print(\"Ignoring {} Users\".format(len(ignore_users)))\n",
    "            self.ignore_users_ID = np.array(ignore_users)\n",
    "            self.usersToEvaluate = set(self.usersToEvaluate) - set(ignore_users)\n",
    "        else:\n",
    "            self.ignore_users_ID = np.array([])\n",
    "\n",
    "        self.usersToEvaluate = list(self.usersToEvaluate)\n",
    "\n",
    "    def _print(self, string):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"{}: {}\".format(self.EVALUATOR_NAME, string))\n",
    "\n",
    "    def evaluateRecommender(self, recommender_object):\n",
    "        \"\"\"\n",
    "        :param recommender_object: the trained recommender object, a BaseRecommender subclass\n",
    "        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n",
    "        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"The method evaluateRecommender not implemented for this evaluator class\")\n",
    "\n",
    "    def get_user_relevant_items(self, user_id):\n",
    "\n",
    "        assert self.URM_test.getformat() == \"csr\", \"Evaluator_Base_Class: URM_test is not CSR, this will cause errors in getting relevant items\"\n",
    "\n",
    "        return self.URM_test.indices[self.URM_test.indptr[user_id]:self.URM_test.indptr[user_id + 1]]\n",
    "\n",
    "    def get_user_test_ratings(self, user_id):\n",
    "\n",
    "        assert self.URM_test.getformat() == \"csr\", \"Evaluator_Base_Class: URM_test is not CSR, this will cause errors in relevant items ratings\"\n",
    "\n",
    "        return self.URM_test.data[self.URM_test.indptr[user_id]:self.URM_test.indptr[user_id + 1]]\n",
    "\n",
    "\n",
    "class EvaluatorHoldout(Evaluator):\n",
    "    \"\"\"EvaluatorHoldout\"\"\"\n",
    "\n",
    "    EVALUATOR_NAME = \"EvaluatorHoldout\"\n",
    "\n",
    "    def __init__(self, URM_test_list, cutoff_list, minRatingsPerUser=1, exclude_seen=True,\n",
    "                 diversity_object=None,\n",
    "                 ignore_items=None,\n",
    "                 ignore_users=None,\n",
    "                 verbose=True):\n",
    "\n",
    "        super(EvaluatorHoldout, self).__init__(URM_test_list, cutoff_list,\n",
    "                                               diversity_object=diversity_object,\n",
    "                                               minRatingsPerUser=minRatingsPerUser, exclude_seen=exclude_seen,\n",
    "                                               ignore_items=ignore_items, ignore_users=ignore_users,\n",
    "                                               verbose=verbose)\n",
    "\n",
    "    def _run_evaluation_on_selected_users(self, recommender_object, usersToEvaluate, block_size=None):\n",
    "\n",
    "        if block_size is None:\n",
    "            block_size = min(1000, int(1e8 / self.n_items))\n",
    "            block_size = min(block_size, len(usersToEvaluate))\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_print = time.time()\n",
    "\n",
    "        results_dict = {}\n",
    "\n",
    "        for cutoff in self.cutoff_list:\n",
    "            results_dict[cutoff] = create_empty_metrics_dict(self.n_items, self.n_users,\n",
    "                                                             recommender_object.get_URM_train(),\n",
    "                                                             self.ignore_items_ID,\n",
    "                                                             self.ignore_users_ID,\n",
    "                                                             cutoff,\n",
    "                                                             self.diversity_object)\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.set_items_to_ignore(self.ignore_items_ID)\n",
    "\n",
    "        n_users_evaluated = 0\n",
    "\n",
    "        # Start from -block_size to ensure it to be 0 at the first block\n",
    "        user_batch_start = 0\n",
    "        user_batch_end = 0\n",
    "\n",
    "        while user_batch_start < len(usersToEvaluate):\n",
    "\n",
    "            user_batch_end = user_batch_start + block_size\n",
    "            user_batch_end = min(user_batch_end, len(usersToEvaluate))\n",
    "\n",
    "            test_user_batch_array = np.array(usersToEvaluate[user_batch_start:user_batch_end])\n",
    "            user_batch_start = user_batch_end\n",
    "\n",
    "            # Compute predictions for a batch of users using vectorization, much more efficient than computing it one at a time\n",
    "            recommended_items_batch_list, scores_batch = recommender_object.recommend(test_user_batch_array,\n",
    "                                                                                      remove_seen_flag=self.exclude_seen,\n",
    "                                                                                      cutoff=self.max_cutoff,\n",
    "                                                                                      remove_top_pop_flag=False,\n",
    "                                                                                      remove_custom_items_flag=self.ignore_items_flag,\n",
    "                                                                                      return_scores=True\n",
    "                                                                                      )\n",
    "\n",
    "            assert len(recommended_items_batch_list) == len(\n",
    "                test_user_batch_array), \"{}: recommended_items_batch_list contained recommendations for {} users, expected was {}\".format(\n",
    "                self.EVALUATOR_NAME, len(recommended_items_batch_list), len(test_user_batch_array))\n",
    "\n",
    "            assert scores_batch.shape[0] == len(\n",
    "                test_user_batch_array), \"{}: scores_batch contained scores for {} users, expected was {}\".format(\n",
    "                self.EVALUATOR_NAME, scores_batch.shape[0], len(test_user_batch_array))\n",
    "\n",
    "            assert scores_batch.shape[\n",
    "                       1] == self.n_items, \"{}: scores_batch contained scores for {} items, expected was {}\".format(\n",
    "                self.EVALUATOR_NAME, scores_batch.shape[1], self.n_items)\n",
    "\n",
    "            # Compute recommendation quality for each user in batch\n",
    "            for batch_user_index in range(len(recommended_items_batch_list)):\n",
    "\n",
    "                test_user = test_user_batch_array[batch_user_index]\n",
    "\n",
    "                relevant_items = self.get_user_relevant_items(test_user)\n",
    "                relevant_items_rating = self.get_user_test_ratings(test_user)\n",
    "\n",
    "                all_items_predicted_ratings = scores_batch[batch_user_index]\n",
    "                user_rmse = rmse(all_items_predicted_ratings, relevant_items, relevant_items_rating)\n",
    "\n",
    "                # Being the URM CSR, the indices are the non-zero column indexes\n",
    "                recommended_items = recommended_items_batch_list[batch_user_index]\n",
    "                is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "                n_users_evaluated += 1\n",
    "\n",
    "                for cutoff in self.cutoff_list:\n",
    "\n",
    "                    results_current_cutoff = results_dict[cutoff]\n",
    "\n",
    "                    is_relevant_current_cutoff = is_relevant[0:cutoff]\n",
    "                    recommended_items_current_cutoff = recommended_items[0:cutoff]\n",
    "\n",
    "                    results_current_cutoff[EvaluatorMetrics.ROC_AUC.value] += roc_auc(is_relevant_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.PRECISION.value] += precision(is_relevant_current_cutoff)\n",
    "                    results_current_cutoff[\n",
    "                        EvaluatorMetrics.PRECISION_RECALL_MIN_DEN.value] += precision_recall_min_denominator(\n",
    "                        is_relevant_current_cutoff, len(relevant_items))\n",
    "                    results_current_cutoff[EvaluatorMetrics.RECALL.value] += recall(is_relevant_current_cutoff,\n",
    "                                                                                    relevant_items)\n",
    "                    results_current_cutoff[EvaluatorMetrics.NDCG.value] += ndcg(recommended_items_current_cutoff,\n",
    "                                                                                relevant_items,\n",
    "                                                                                relevance=self.get_user_test_ratings(\n",
    "                                                                                    test_user), at=cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.HIT_RATE.value] += is_relevant_current_cutoff.sum()\n",
    "                    results_current_cutoff[EvaluatorMetrics.ARHR.value] += arhr(is_relevant_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.RMSE.value] += user_rmse\n",
    "\n",
    "                    results_current_cutoff[EvaluatorMetrics.MRR.value].add_recommendations(is_relevant_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.MAP.value].add_recommendations(is_relevant_current_cutoff,\n",
    "                                                                                           relevant_items)\n",
    "                    results_current_cutoff[EvaluatorMetrics.NOVELTY.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.AVERAGE_POPULARITY.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.DIVERSITY_GINI.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.SHANNON_ENTROPY.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.COVERAGE_ITEM.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.COVERAGE_USER.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff, test_user)\n",
    "                    results_current_cutoff[EvaluatorMetrics.DIVERSITY_MEAN_INTER_LIST.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "                    results_current_cutoff[EvaluatorMetrics.DIVERSITY_HERFINDAHL.value].add_recommendations(\n",
    "                        recommended_items_current_cutoff)\n",
    "\n",
    "                    if EvaluatorMetrics.DIVERSITY_SIMILARITY.value in results_current_cutoff:\n",
    "                        results_current_cutoff[EvaluatorMetrics.DIVERSITY_SIMILARITY.value].add_recommendations(\n",
    "                            recommended_items_current_cutoff)\n",
    "\n",
    "                if time.time() - start_time_print > 30 or n_users_evaluated == len(self.usersToEvaluate):\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n",
    "\n",
    "                    self._print(\"Processed {} ( {:.2f}% ) in {:.2f} {}. Users per second: {:.0f}\".format(\n",
    "                        n_users_evaluated,\n",
    "                        100.0 * float(n_users_evaluated) / len(self.usersToEvaluate),\n",
    "                        new_time_value, new_time_unit,\n",
    "                        float(n_users_evaluated) / elapsed_time))\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "                    sys.stderr.flush()\n",
    "\n",
    "                    start_time_print = time.time()\n",
    "\n",
    "        return results_dict, n_users_evaluated\n",
    "\n",
    "    def evaluateRecommender(self, recommender_object):\n",
    "        \"\"\"\n",
    "        :param recommender_object: the trained recommender object, a BaseRecommender subclass\n",
    "        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n",
    "        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n",
    "        \"\"\"\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.set_items_to_ignore(self.ignore_items_ID)\n",
    "\n",
    "        results_dict, n_users_evaluated = self._run_evaluation_on_selected_users(recommender_object,\n",
    "                                                                                 self.usersToEvaluate)\n",
    "\n",
    "        if (n_users_evaluated > 0):\n",
    "\n",
    "            for cutoff in self.cutoff_list:\n",
    "\n",
    "                results_current_cutoff = results_dict[cutoff]\n",
    "\n",
    "                for key in results_current_cutoff.keys():\n",
    "\n",
    "                    value = results_current_cutoff[key]\n",
    "\n",
    "                    if isinstance(value, Metrics_Object):\n",
    "                        results_current_cutoff[key] = value.get_metric_value()\n",
    "                    else:\n",
    "                        results_current_cutoff[key] = value / n_users_evaluated\n",
    "\n",
    "                precision_ = results_current_cutoff[EvaluatorMetrics.PRECISION.value]\n",
    "                recall_ = results_current_cutoff[EvaluatorMetrics.RECALL.value]\n",
    "\n",
    "                if precision_ + recall_ != 0:\n",
    "                    # F1 micro averaged: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
    "                    results_current_cutoff[EvaluatorMetrics.F1.value] = 2 * (precision_ * recall_) / (\n",
    "                            precision_ + recall_)\n",
    "        else:\n",
    "            self._print(\"WARNING: No users had a sufficient number of relevant items\")\n",
    "\n",
    "        results_run_string = get_result_string(results_dict)\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.reset_items_to_ignore()\n",
    "\n",
    "        return (results_dict, results_run_string)\n",
    "    \n",
    "    \n",
    "    \n",
    "# metrics.py \n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Maurizio Ferrari Dacrema, Massimo Quadrana\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "class Metrics_Object(object):\n",
    "    \"\"\"\n",
    "    Abstract class that should be used as superclass of all metrics requiring an object, therefore a state, to be computed\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Coverage_Item(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Item coverage represents the percentage of the overall items which were recommended\n",
    "    https://gab41.lab41.org/recommender-systems-its-not-all-about-the-accuracy-562c7dceeaff\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_items, ignore_items):\n",
    "        super(Coverage_Item, self).__init__()\n",
    "        self.recommended_mask = np.zeros(n_items, dtype=np.bool)\n",
    "        self.n_ignore_items = len(ignore_items)\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        if len(recommended_items_ids) > 0:\n",
    "            self.recommended_mask[recommended_items_ids] = True\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.recommended_mask.sum()/(len(self.recommended_mask)-self.n_ignore_items)\n",
    "\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Coverage_Item, \"Coverage_Item: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.recommended_mask = np.logical_or(self.recommended_mask, other_metric_object.recommended_mask)\n",
    "\n",
    "class Coverage_User(Metrics_Object):\n",
    "    \"\"\"\n",
    "    User coverage represents the percentage of the overall users for which we can make recommendations.\n",
    "    If there is at least one recommendation the user is considered as covered\n",
    "    https://gab41.lab41.org/recommender-systems-its-not-all-about-the-accuracy-562c7dceeaff\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_users, ignore_users):\n",
    "        super(Coverage_User, self).__init__()\n",
    "        self.users_mask = np.zeros(n_users, dtype=np.bool)\n",
    "        self.n_ignore_users = len(ignore_users)\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids, user_id):\n",
    "        self.users_mask[user_id] = len(recommended_items_ids)>0\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.users_mask.sum()/(len(self.users_mask)-self.n_ignore_users)\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Coverage_User, \"Coverage_User: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.users_mask = np.logical_or(self.users_mask, other_metric_object.users_mask)\n",
    "\n",
    "class MAP(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Mean Average Precision, defined as the mean of the AveragePrecision over all users\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MAP, self).__init__()\n",
    "        self.cumulative_AP = 0.0\n",
    "        self.n_users = 0\n",
    "\n",
    "    def add_recommendations(self, is_relevant, pos_items):\n",
    "        self.cumulative_AP += average_precision(is_relevant, pos_items)\n",
    "        self.n_users += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.cumulative_AP/self.n_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is MAP, \"MAP: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.cumulative_AP += other_metric_object.cumulative_AP\n",
    "        self.n_users += other_metric_object.n_users\n",
    "\n",
    "class MRR(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Mean Reciprocal Rank, defined as the mean of the Reciprocal Rank over all users\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MRR, self).__init__()\n",
    "        self.cumulative_RR = 0.0\n",
    "        self.n_users = 0\n",
    "\n",
    "    def add_recommendations(self, is_relevant):\n",
    "        self.cumulative_RR += rr(is_relevant)\n",
    "        self.n_users += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        return self.cumulative_RR/self.n_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is MAP, \"MRR: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.cumulative_RR += other_metric_object.cumulative_RR\n",
    "        self.n_users += other_metric_object.n_users\n",
    "\n",
    "class Gini_Diversity(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Gini diversity index, computed from the Gini Index but with inverted range, such that high values mean higher diversity\n",
    "    This implementation ignores zero-occurrence items\n",
    "    # From https://github.com/oliviaguest/gini\n",
    "    # based on bottom eq: http://www.statsdirect.com/help/content/image/stat0206_wmf.gif\n",
    "    # from: http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    #\n",
    "    # http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.8174&rep=rep1&type=pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_items, ignore_items):\n",
    "        super(Gini_Diversity, self).__init__()\n",
    "        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n",
    "        self.ignore_items = ignore_items.astype(np.int).copy()\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        if len(recommended_items_ids) > 0:\n",
    "            self.recommended_counter[recommended_items_ids] += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        recommended_counter = self.recommended_counter.copy()\n",
    "\n",
    "        recommended_counter_mask = np.ones_like(recommended_counter, dtype = np.bool)\n",
    "        recommended_counter_mask[self.ignore_items] = False\n",
    "        recommended_counter_mask[recommended_counter == 0] = False\n",
    "\n",
    "        recommended_counter = recommended_counter[recommended_counter_mask]\n",
    "\n",
    "        n_items = len(recommended_counter)\n",
    "\n",
    "        recommended_counter_sorted = np.sort(recommended_counter)       # values must be sorted\n",
    "        index = np.arange(1, n_items+1)                                 # index per array element\n",
    "\n",
    "        #gini_index = (np.sum((2 * index - n_items  - 1) * recommended_counter_sorted)) / (n_items * np.sum(recommended_counter_sorted))\n",
    "        gini_diversity = 2*np.sum((n_items + 1 - index)/(n_items+1) * recommended_counter_sorted/np.sum(recommended_counter_sorted))\n",
    "\n",
    "        return gini_diversity\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Gini_Diversity, \"Gini_Diversity: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.recommended_counter += other_metric_object.recommended_counter\n",
    "\n",
    "class Diversity_Herfindahl(Metrics_Object):\n",
    "    \"\"\"\n",
    "    The Herfindahl index is also known as Concentration index, it is used in economy to determine whether the market quotas\n",
    "    are such that an excessive concentration exists. It is here used as a diversity index, if high means high diversity.\n",
    "    It is known to have a small value range in recommender systems, between 0.9 and 1.0\n",
    "    The Herfindahl index is a function of the square of the probability an item has been recommended to any user, hence\n",
    "    The Herfindahl index is equivalent to MeanInterList diversity as they measure the same quantity.\n",
    "    # http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.8174&rep=rep1&type=pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_items, ignore_items):\n",
    "        super(Diversity_Herfindahl, self).__init__()\n",
    "        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n",
    "        self.ignore_items = ignore_items.astype(np.int).copy()\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        if len(recommended_items_ids) > 0:\n",
    "            self.recommended_counter[recommended_items_ids] += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        recommended_counter = self.recommended_counter.copy()\n",
    "\n",
    "        recommended_counter_mask = np.ones_like(recommended_counter, dtype = np.bool)\n",
    "        recommended_counter_mask[self.ignore_items] = False\n",
    "\n",
    "        recommended_counter = recommended_counter[recommended_counter_mask]\n",
    "\n",
    "        if recommended_counter.sum() != 0:\n",
    "            herfindahl_index = 1 - np.sum((recommended_counter / recommended_counter.sum()) ** 2)\n",
    "        else:\n",
    "            herfindahl_index = np.nan\n",
    "\n",
    "        return herfindahl_index\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Diversity_Herfindahl, \"Diversity_Herfindahl: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.recommended_counter += other_metric_object.recommended_counter\n",
    "\n",
    "class Shannon_Entropy(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Shannon Entropy is a well known metric to measure the amount of information of a certain string of data.\n",
    "    Here is applied to the global number of times an item has been recommended.\n",
    "    It has a lower bound and can reach values over 12.0 for random recommenders.\n",
    "    A high entropy means that the distribution is random uniform across all users.\n",
    "    Note that while a random uniform distribution\n",
    "    (hence all items with SIMILAR number of occurrences)\n",
    "    will be highly diverse and have high entropy, a perfectly uniform distribution\n",
    "    (hence all items with EXACTLY IDENTICAL number of occurrences)\n",
    "    will have 0.0 entropy while being the most diverse possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_items, ignore_items):\n",
    "        super(Shannon_Entropy, self).__init__()\n",
    "        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n",
    "        self.ignore_items = ignore_items.astype(np.int).copy()\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "        if len(recommended_items_ids) > 0:\n",
    "            self.recommended_counter[recommended_items_ids] += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        assert np.all(self.recommended_counter >= 0.0), \"Shannon_Entropy: self.recommended_counter contains negative counts\"\n",
    "\n",
    "        recommended_counter = self.recommended_counter.copy()\n",
    "\n",
    "        # Ignore from the computation both ignored items and items with zero occurrence.\n",
    "        # Zero occurrence items will have zero probability and will not change the result, butt will generate nans if used in the log\n",
    "        recommended_counter_mask = np.ones_like(recommended_counter, dtype = np.bool)\n",
    "        recommended_counter_mask[self.ignore_items] = False\n",
    "        recommended_counter_mask[recommended_counter == 0] = False\n",
    "\n",
    "        recommended_counter = recommended_counter[recommended_counter_mask]\n",
    "\n",
    "        n_recommendations = recommended_counter.sum()\n",
    "\n",
    "        recommended_probability = recommended_counter/n_recommendations\n",
    "\n",
    "        shannon_entropy = -np.sum(recommended_probability * np.log2(recommended_probability))\n",
    "\n",
    "        return shannon_entropy\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Gini_Diversity, \"Shannon_Entropy: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        assert np.all(self.recommended_counter >= 0.0), \"Shannon_Entropy: self.recommended_counter contains negative counts\"\n",
    "        assert np.all(other_metric_object.recommended_counter >= 0.0), \"Shannon_Entropy: other.recommended_counter contains negative counts\"\n",
    "\n",
    "        self.recommended_counter += other_metric_object.recommended_counter\n",
    "\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "class Novelty(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Novelty measures how \"novel\" a recommendation is in terms of how popular the item was in the train set.\n",
    "    Due to this definition, the novelty of a cold item (i.e. with no interactions in the train set) is not defined,\n",
    "    in this implementation cold items are ignored and their contribution to the novelty is 0.\n",
    "    A recommender with high novelty will be able to recommend also long queue (i.e. unpopular) items.\n",
    "    Mean self-information  (Zhou 2010)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, URM_train):\n",
    "        super(Novelty, self).__init__()\n",
    "\n",
    "        URM_train = sps.csc_matrix(URM_train)\n",
    "        URM_train.eliminate_zeros()\n",
    "        self.item_popularity = np.ediff1d(URM_train.indptr)\n",
    "\n",
    "        self.novelty = 0.0\n",
    "        self.n_evaluated_users = 0\n",
    "        self.n_items = len(self.item_popularity)\n",
    "        self.n_interactions = self.item_popularity.sum()\n",
    "\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "\n",
    "        self.n_evaluated_users += 1\n",
    "\n",
    "        if len(recommended_items_ids)>0:\n",
    "            recommended_items_popularity = self.item_popularity[recommended_items_ids]\n",
    "\n",
    "            probability = recommended_items_popularity/self.n_interactions\n",
    "            probability = probability[probability!=0]\n",
    "\n",
    "            self.novelty += np.sum(-np.log2(probability)/self.n_items)\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        if self.n_evaluated_users == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return self.novelty/self.n_evaluated_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Novelty, \"Novelty: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.novelty = self.novelty + other_metric_object.novelty\n",
    "        self.n_evaluated_users = self.n_evaluated_users + other_metric_object.n_evaluated_users\n",
    "\n",
    "class AveragePopularity(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Average popularity the recommended items have in the train data.\n",
    "    The popularity is normalized by setting as 1 the item with the highest popularity in the train data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, URM_train):\n",
    "        super(AveragePopularity, self).__init__()\n",
    "\n",
    "        URM_train = sps.csc_matrix(URM_train)\n",
    "        URM_train.eliminate_zeros()\n",
    "        item_popularity = np.ediff1d(URM_train.indptr)\n",
    "\n",
    "\n",
    "        self.cumulative_popularity = 0.0\n",
    "        self.n_evaluated_users = 0\n",
    "        self.n_items = URM_train.shape[0]\n",
    "        self.n_interactions = item_popularity.sum()\n",
    "\n",
    "        self.item_popularity_normalized = item_popularity/item_popularity.max()\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "\n",
    "        self.n_evaluated_users += 1\n",
    "\n",
    "        if len(recommended_items_ids)>0:\n",
    "            recommended_items_popularity = self.item_popularity_normalized[recommended_items_ids]\n",
    "\n",
    "            self.cumulative_popularity += np.sum(recommended_items_popularity)/len(recommended_items_ids)\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        if self.n_evaluated_users == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return self.cumulative_popularity/self.n_evaluated_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Novelty, \"AveragePopularity: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.cumulative_popularity = self.cumulative_popularity + other_metric_object.cumulative_popularity\n",
    "        self.n_evaluated_users = self.n_evaluated_users + other_metric_object.n_evaluated_users\n",
    "\n",
    "\n",
    "class Diversity_similarity(Metrics_Object):\n",
    "    \"\"\"\n",
    "    Intra list diversity computes the diversity of items appearing in the recommendations received by each single user, by using an item_diversity_matrix.\n",
    "    It can be used, for example, to compute the diversity in terms of features for a collaborative recommender.\n",
    "    A content-based recommender will have low IntraList diversity if that is computed on the same features the recommender uses.\n",
    "    A TopPopular recommender may exhibit high IntraList diversity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, item_diversity_matrix):\n",
    "        super(Diversity_similarity, self).__init__()\n",
    "\n",
    "        assert np.all(item_diversity_matrix >= 0.0) and np.all(item_diversity_matrix <= 1.0), \\\n",
    "            \"item_diversity_matrix contains value greated than 1.0 or lower than 0.0\"\n",
    "\n",
    "        self.item_diversity_matrix = item_diversity_matrix\n",
    "\n",
    "        self.n_evaluated_users = 0\n",
    "        self.diversity = 0.0\n",
    "\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "\n",
    "        current_recommended_items_diversity = 0.0\n",
    "\n",
    "        for item_index in range(len(recommended_items_ids)-1):\n",
    "\n",
    "            item_id = recommended_items_ids[item_index]\n",
    "\n",
    "            item_other_diversity = self.item_diversity_matrix[item_id, recommended_items_ids]\n",
    "            item_other_diversity[item_index] = 0.0\n",
    "\n",
    "            current_recommended_items_diversity += np.sum(item_other_diversity)\n",
    "\n",
    "\n",
    "        self.diversity += current_recommended_items_diversity/(len(recommended_items_ids)*(len(recommended_items_ids)-1))\n",
    "\n",
    "        self.n_evaluated_users += 1\n",
    "\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        if self.n_evaluated_users == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return self.diversity/self.n_evaluated_users\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "        assert other_metric_object is Diversity_similarity, \"Diversity: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        self.diversity = self.diversity + other_metric_object.diversity\n",
    "        self.n_evaluated_users = self.n_evaluated_users + other_metric_object.n_evaluated_users\n",
    "\n",
    "\n",
    "class Diversity_MeanInterList(Metrics_Object):\n",
    "    \"\"\"\n",
    "    MeanInterList diversity measures the uniqueness of different users' recommendation lists.\n",
    "    It can be used to measure how \"diversified\" are the recommendations different users receive.\n",
    "    While the original proposal called this metric \"Personalization\", we do not use this name since the highest MeanInterList diversity\n",
    "    is exhibited by a non personalized Random recommender.\n",
    "    It can be demonstrated that this metric does not require to compute the common items all possible couples of users have in common\n",
    "    but rather it is only sensitive to the total amount of time each item has been recommended.\n",
    "    MeanInterList diversity is a function of the square of the probability an item has been recommended to any user, hence\n",
    "    MeanInterList diversity is equivalent to the Herfindahl index as they measure the same quantity.\n",
    "    A TopPopular recommender that does not remove seen items will have 0.0 MeanInterList diversity.\n",
    "    pag. 3, http://www.pnas.org/content/pnas/107/10/4511.full.pdf\n",
    "    @article{zhou2010solving,\n",
    "      title={Solving the apparent diversity-accuracy dilemma of recommender systems},\n",
    "      author={Zhou, Tao and Kuscsik, Zolt{\\'a}n and Liu, Jian-Guo and Medo, Mat{\\'u}{\\v{s}} and Wakeling, Joseph Rushton and Zhang, Yi-Cheng},\n",
    "      journal={Proceedings of the National Academy of Sciences},\n",
    "      volume={107},\n",
    "      number={10},\n",
    "      pages={4511--4515},\n",
    "      year={2010},\n",
    "      publisher={National Acad Sciences}\n",
    "    }\n",
    "    # The formula is diversity_cumulative += 1 - common_recommendations(user1, user2)/cutoff\n",
    "    # for each couple of users, except the diagonal. It is VERY computationally expensive\n",
    "    # We can move the 1 and cutoff outside of the summation. Remember to exclude the diagonal\n",
    "    # co_counts = URM_predicted.dot(URM_predicted.T)\n",
    "    # co_counts[np.arange(0, n_user, dtype=np.int):np.arange(0, n_user, dtype=np.int)] = 0\n",
    "    # diversity = (n_user**2 - n_user) - co_counts.sum()/self.cutoff\n",
    "    # If we represent the summation of co_counts separating it for each item, we will have:\n",
    "    # co_counts.sum() = co_counts_item1.sum()  + co_counts_item2.sum() ...\n",
    "    # If we know how many times an item has been recommended, co_counts_item1.sum() can be computed as how many couples of\n",
    "    # users have item1 in common. If item1 has been recommended n times, the number of couples is n*(n-1)\n",
    "    # Therefore we can compute co_counts.sum() value as:\n",
    "    # np.sum(np.multiply(item-occurrence, item-occurrence-1))\n",
    "    # The naive implementation URM_predicted.dot(URM_predicted.T) might require an hour of computation\n",
    "    # The last implementation has a negligible computational time even for very big datasets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_items, cutoff):\n",
    "        super(Diversity_MeanInterList, self).__init__()\n",
    "\n",
    "        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n",
    "\n",
    "        self.n_evaluated_users = 0\n",
    "        self.n_items = n_items\n",
    "        self.diversity = 0.0\n",
    "        self.cutoff = cutoff\n",
    "\n",
    "\n",
    "    def add_recommendations(self, recommended_items_ids):\n",
    "\n",
    "        assert len(recommended_items_ids) <= self.cutoff, \"Diversity_MeanInterList: recommended list is contains more elements than cutoff\"\n",
    "\n",
    "        self.n_evaluated_users += 1\n",
    "\n",
    "        if len(recommended_items_ids) > 0:\n",
    "            self.recommended_counter[recommended_items_ids] += 1\n",
    "\n",
    "    def get_metric_value(self):\n",
    "\n",
    "        # Requires to compute the number of common elements for all couples of users\n",
    "        if self.n_evaluated_users == 0:\n",
    "            return 1.0\n",
    "\n",
    "        cooccurrences_cumulative = np.sum(self.recommended_counter**2) - self.n_evaluated_users*self.cutoff\n",
    "\n",
    "        # All user combinations except diagonal\n",
    "        all_user_couples_count = self.n_evaluated_users**2 - self.n_evaluated_users\n",
    "\n",
    "        diversity_cumulative = all_user_couples_count - cooccurrences_cumulative/self.cutoff\n",
    "\n",
    "        self.diversity = diversity_cumulative/all_user_couples_count\n",
    "\n",
    "        return self.diversity\n",
    "\n",
    "    def get_theoretical_max(self):\n",
    "\n",
    "        global_co_occurrence_count = (self.n_evaluated_users*self.cutoff)**2/self.n_items - self.n_evaluated_users*self.cutoff\n",
    "\n",
    "        mild = 1 - 1/(self.n_evaluated_users**2 - self.n_evaluated_users)*(global_co_occurrence_count/self.cutoff)\n",
    "\n",
    "        return mild\n",
    "\n",
    "    def merge_with_other(self, other_metric_object):\n",
    "\n",
    "        assert other_metric_object is Diversity_MeanInterList, \"Diversity_MeanInterList: attempting to merge with a metric object of different type\"\n",
    "\n",
    "        assert np.all(self.recommended_counter >= 0.0), \"Diversity_MeanInterList: self.recommended_counter contains negative counts\"\n",
    "        assert np.all(other_metric_object.recommended_counter >= 0.0), \"Diversity_MeanInterList: other.recommended_counter contains negative counts\"\n",
    "\n",
    "        self.recommended_counter += other_metric_object.recommended_counter\n",
    "        self.n_evaluated_users += other_metric_object.n_evaluated_users\n",
    "\n",
    "def roc_auc(is_relevant):\n",
    "\n",
    "    ranks = np.arange(len(is_relevant))\n",
    "    pos_ranks = ranks[is_relevant]\n",
    "    neg_ranks = ranks[~is_relevant]\n",
    "    auc_score = 0.0\n",
    "\n",
    "    if len(neg_ranks) == 0:\n",
    "        return 1.0\n",
    "\n",
    "    if len(pos_ranks) > 0:\n",
    "        for pos_pred in pos_ranks:\n",
    "            auc_score += np.sum(pos_pred < neg_ranks, dtype=np.float32)\n",
    "        auc_score /= (pos_ranks.shape[0] * neg_ranks.shape[0])\n",
    "\n",
    "    assert 0 <= auc_score <= 1, auc_score\n",
    "    return auc_score\n",
    "\n",
    "def arhr(is_relevant):\n",
    "    # average reciprocal hit-rank (ARHR) of all relevant items\n",
    "    # As opposed to MRR, ARHR takes into account all relevant items and not just the first\n",
    "    # pag 17\n",
    "    # http://glaros.dtc.umn.edu/gkhome/fetch/papers/itemrsTOIS04.pdf\n",
    "    # https://emunix.emich.edu/~sverdlik/COSC562/ItemBasedTopTen.pdf\n",
    "\n",
    "    p_reciprocal = 1/np.arange(1,len(is_relevant)+1, 1.0, dtype=np.float64)\n",
    "    arhr_score = is_relevant.dot(p_reciprocal)\n",
    "\n",
    "    #assert 0 <= arhr_score <= p_reciprocal.sum(), \"arhr_score {} should be between 0 and {}\".format(arhr_score, p_reciprocal.sum())\n",
    "    assert not np.isnan(arhr_score), \"ARHR is NaN\"\n",
    "    return arhr_score\n",
    "\n",
    "def precision(is_relevant):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        precision_score = 0.0\n",
    "    else:\n",
    "        precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "\n",
    "    assert 0 <= precision_score <= 1, precision_score\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "def precision_recall_min_denominator(is_relevant, n_test_items):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        precision_score = 0.0\n",
    "    else:\n",
    "        precision_score = np.sum(is_relevant, dtype=np.float32) / min(n_test_items, len(is_relevant))\n",
    "\n",
    "    assert 0 <= precision_score <= 1, precision_score\n",
    "    return precision_score\n",
    "\n",
    "def rmse(all_items_predicted_ratings, relevant_items, relevant_items_rating):\n",
    "\n",
    "    # Important, some items will have -np.inf score and are treated as if they did not exist\n",
    "\n",
    "    # RMSE with test items\n",
    "    relevant_items_error = (all_items_predicted_ratings[relevant_items]-relevant_items_rating)**2\n",
    "\n",
    "    finite_prediction_mask = np.isfinite(relevant_items_error)\n",
    "\n",
    "    if finite_prediction_mask.sum() == 0:\n",
    "        rmse = np.nan\n",
    "\n",
    "    else:\n",
    "        relevant_items_error = relevant_items_error[finite_prediction_mask]\n",
    "\n",
    "        squared_error = np.sum(relevant_items_error)\n",
    "\n",
    "        # # Second the RMSE against all non-test items assumed having true rating 0\n",
    "        # # In order to avoid the need of explicitly indexing all non-relevant items, use a difference\n",
    "        # squared_error += np.sum(all_items_predicted_ratings[np.isfinite(all_items_predicted_ratings)]**2) - \\\n",
    "        #                  np.sum(all_items_predicted_ratings[relevant_items][np.isfinite(all_items_predicted_ratings[relevant_items])]**2)\n",
    "\n",
    "        mean_squared_error = squared_error/finite_prediction_mask.sum()\n",
    "        rmse = np.sqrt(mean_squared_error)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def recall(is_relevant, pos_items):\n",
    "\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / pos_items.shape[0]\n",
    "\n",
    "    assert 0 <= recall_score <= 1, recall_score\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "def rr(is_relevant):\n",
    "    # reciprocal rank of the FIRST relevant item in the ranked list (0 if none)\n",
    "\n",
    "    ranks = np.arange(1, len(is_relevant) + 1)[is_relevant]\n",
    "\n",
    "    if len(ranks) > 0:\n",
    "        return 1. / ranks[0]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def average_precision(is_relevant, pos_items):\n",
    "\n",
    "    if len(is_relevant) == 0:\n",
    "        a_p = 0.0\n",
    "    else:\n",
    "        p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "        a_p = np.sum(p_at_k) / np.min([pos_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    assert 0 <= a_p <= 1, a_p\n",
    "    return a_p\n",
    "\n",
    "\n",
    "def ndcg(ranked_list, pos_items, relevance=None, at=None):\n",
    "\n",
    "    if relevance is None:\n",
    "        relevance = np.ones_like(pos_items)\n",
    "    assert len(relevance) == pos_items.shape[0]\n",
    "\n",
    "    # Create a dictionary associating item_id to its relevance\n",
    "    # it2rel[item] -> relevance[item]\n",
    "    it2rel = {it: r for it, r in zip(pos_items, relevance)}\n",
    "\n",
    "    # Creates array of length \"at\" with the relevance associated to the item in that position\n",
    "    rank_scores = np.asarray([it2rel.get(it, 0.0) for it in ranked_list[:at]], dtype=np.float32)\n",
    "\n",
    "    # IDCG has all relevances to 1, up to the number of items in the test set\n",
    "    ideal_dcg = dcg(np.sort(relevance)[::-1])\n",
    "\n",
    "    # DCG uses the relevance of the recommended items\n",
    "    rank_dcg = dcg(rank_scores)\n",
    "\n",
    "    if rank_dcg == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    ndcg_ = rank_dcg / ideal_dcg\n",
    "    # assert 0 <= ndcg_ <= 1, (rank_dcg, ideal_dcg, ndcg_)\n",
    "    return ndcg_\n",
    "\n",
    "\n",
    "def dcg(scores):\n",
    "    return np.sum(np.divide(np.power(2, scores) - 1, np.log(np.arange(scores.shape[0], dtype=np.float32) + 2)),\n",
    "                  dtype=np.float32)\n",
    "\n",
    "\n",
    "metrics = ['AUC', 'Precision' 'Recall', 'MAP', 'NDCG']\n",
    "\n",
    "\n",
    "def pp_metrics(metric_names, metric_values, metric_at):\n",
    "    \"\"\"\n",
    "    Pretty-prints metric values\n",
    "    :param metrics_arr:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(metric_names) == len(metric_values)\n",
    "    if isinstance(metric_at, int):\n",
    "        metric_at = [metric_at] * len(metric_values)\n",
    "    return ' '.join(['{}: {:.4f}'.format(mname, mvalue) if mcutoff is None or mcutoff == 0 else\n",
    "                     '{}@{}: {:.4f}'.format(mname, mcutoff, mvalue)\n",
    "                     for mname, mcutoff, mvalue in zip(metric_names, metric_at, metric_values)])\n",
    "\n",
    "\n",
    "class TestAUC(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4])\n",
    "        ranked_list = np.asarray([1, 2, 3, 4, 5])\n",
    "        self.assertTrue(np.allclose(roc_auc(ranked_list, pos_items),\n",
    "                                    (2. / 3 + 1. / 3) / 2))\n",
    "\n",
    "\n",
    "class TestRecall(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4, 5, 10])\n",
    "        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n",
    "        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n",
    "        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n",
    "        self.assertTrue(np.allclose(recall(ranked_list_1, pos_items), 3. / 4))\n",
    "        self.assertTrue(np.allclose(recall(ranked_list_2, pos_items), 1.0))\n",
    "        self.assertTrue(np.allclose(recall(ranked_list_3, pos_items), 0.0))\n",
    "\n",
    "        thresholds = [1, 2, 3, 4, 5]\n",
    "        values = [0.0, 1. / 4, 1. / 4, 2. / 4, 3. / 4]\n",
    "        for at, val in zip(thresholds, values):\n",
    "            self.assertTrue(np.allclose(np.asarray(recall(ranked_list_1, pos_items, at=at)), val))\n",
    "\n",
    "\n",
    "class TestPrecision(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4, 5, 10])\n",
    "        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n",
    "        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n",
    "        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n",
    "        self.assertTrue(np.allclose(precision(ranked_list_1, pos_items), 3. / 5))\n",
    "        self.assertTrue(np.allclose(precision(ranked_list_2, pos_items), 4. / 5))\n",
    "        self.assertTrue(np.allclose(precision(ranked_list_3, pos_items), 0.0))\n",
    "\n",
    "        thresholds = [1, 2, 3, 4, 5]\n",
    "        values = [0.0, 1. / 2, 1. / 3, 2. / 4, 3. / 5]\n",
    "        for at, val in zip(thresholds, values):\n",
    "            self.assertTrue(np.allclose(np.asarray(precision(ranked_list_1, pos_items, at=at)), val))\n",
    "\n",
    "\n",
    "class TestRR(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4, 5, 10])\n",
    "        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n",
    "        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n",
    "        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n",
    "        self.assertTrue(np.allclose(rr(ranked_list_1, pos_items), 1. / 2))\n",
    "        self.assertTrue(np.allclose(rr(ranked_list_2, pos_items), 1.))\n",
    "        self.assertTrue(np.allclose(rr(ranked_list_3, pos_items), 0.0))\n",
    "\n",
    "        thresholds = [1, 2, 3, 4, 5]\n",
    "        values = [0.0, 1. / 2, 1. / 2, 1. / 2, 1. / 2]\n",
    "        for at, val in zip(thresholds, values):\n",
    "            self.assertTrue(np.allclose(np.asarray(rr(ranked_list_1, pos_items, at=at)), val))\n",
    "\n",
    "\n",
    "class TestMAP(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4, 5, 10])\n",
    "        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n",
    "        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n",
    "        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n",
    "        ranked_list_4 = np.asarray([11, 12, 13, 14, 15, 16, 2, 4, 5, 10])\n",
    "        ranked_list_5 = np.asarray([2, 11, 12, 13, 14, 15, 4, 5, 10, 16])\n",
    "        self.assertTrue(np.allclose(map(ranked_list_1, pos_items), (1. / 2 + 2. / 4 + 3. / 5) / 4))\n",
    "        self.assertTrue(np.allclose(map(ranked_list_2, pos_items), 1.0))\n",
    "        self.assertTrue(np.allclose(map(ranked_list_3, pos_items), 0.0))\n",
    "        self.assertTrue(np.allclose(map(ranked_list_4, pos_items), (1. / 7 + 2. / 8 + 3. / 9 + 4. / 10) / 4))\n",
    "        self.assertTrue(np.allclose(map(ranked_list_5, pos_items), (1. + 2. / 7 + 3. / 8 + 4. / 9) / 4))\n",
    "\n",
    "        thresholds = [1, 2, 3, 4, 5]\n",
    "        values = [\n",
    "            0.0,\n",
    "            1. / 2 / 2,\n",
    "            1. / 2 / 3,\n",
    "            (1. / 2 + 2. / 4) / 4,\n",
    "            (1. / 2 + 2. / 4 + 3. / 5) / 4\n",
    "        ]\n",
    "        for at, val in zip(thresholds, values):\n",
    "            self.assertTrue(np.allclose(np.asarray(map(ranked_list_1, pos_items, at)), val))\n",
    "\n",
    "\n",
    "class TestNDCG(unittest.TestCase):\n",
    "    def runTest(self):\n",
    "        pos_items = np.asarray([2, 4, 5, 10])\n",
    "        pos_relevances = np.asarray([5, 4, 3, 2])\n",
    "        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])  # rel = 0, 5, 0, 4, 3\n",
    "        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])  # rel = 2, 3, 5, 4, 0\n",
    "        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])  # rel = 0, 0, 0, 0, 0\n",
    "        idcg = ((2 ** 5 - 1) / np.log(2) +\n",
    "                (2 ** 4 - 1) / np.log(3) +\n",
    "                (2 ** 3 - 1) / np.log(4) +\n",
    "                (2 ** 2 - 1) / np.log(5))\n",
    "        self.assertTrue(np.allclose(dcg(np.sort(pos_relevances)[::-1]), idcg))\n",
    "        self.assertTrue(np.allclose(ndcg(ranked_list_1, pos_items, pos_relevances),\n",
    "                                    ((2 ** 5 - 1) / np.log(3) +\n",
    "                                     (2 ** 4 - 1) / np.log(5) +\n",
    "                                     (2 ** 3 - 1) / np.log(6)) / idcg))\n",
    "        self.assertTrue(np.allclose(ndcg(ranked_list_2, pos_items, pos_relevances),\n",
    "                                    ((2 ** 2 - 1) / np.log(2) +\n",
    "                                     (2 ** 3 - 1) / np.log(3) +\n",
    "                                     (2 ** 5 - 1) / np.log(4) +\n",
    "                                     (2 ** 4 - 1) / np.log(5)) / idcg))\n",
    "        self.assertTrue(np.allclose(ndcg(ranked_list_3, pos_items, pos_relevances), 0.0))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     unittest.main()\n",
    "\n",
    "\n",
    "# seconds_to_biggest_unit.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 30/03/2019\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def seconds_to_biggest_unit(time_in_seconds, data_array = None):\n",
    "\n",
    "    conversion_factor = [\n",
    "        (\"sec\", 60),\n",
    "        (\"min\", 60),\n",
    "        (\"hour\", 24),\n",
    "        (\"day\", 365),\n",
    "    ]\n",
    "\n",
    "    terminate = False\n",
    "    unit_index = 0\n",
    "\n",
    "    new_time_value = time_in_seconds\n",
    "    new_time_unit = \"sec\"\n",
    "\n",
    "    while not terminate:\n",
    "\n",
    "        next_time = new_time_value/conversion_factor[unit_index][1]\n",
    "\n",
    "        if next_time >= 1.0:\n",
    "            new_time_value = next_time\n",
    "\n",
    "            if data_array is not None:\n",
    "                data_array /= conversion_factor[unit_index][1]\n",
    "\n",
    "            unit_index += 1\n",
    "            new_time_unit = conversion_factor[unit_index][0]\n",
    "\n",
    "        else:\n",
    "            terminate = True\n",
    "\n",
    "\n",
    "    if data_array is not None:\n",
    "        return new_time_value, new_time_unit, data_array\n",
    "\n",
    "    else:\n",
    "        return new_time_value, new_time_unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Hyperparameters tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# SearchAbstractClass.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 10/03/2018\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "import time, os, traceback\n",
    "# from utils.Evaluation.Incremental_Training_Early_Stopping import Incremental_Training_Early_Stopping\n",
    "import numpy as np\n",
    "# from utils.DataIO import DataIO\n",
    "\n",
    "class SearchInputRecommenderArgs(object):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                   # Dictionary of parameters needed by the constructor\n",
    "                   CONSTRUCTOR_POSITIONAL_ARGS = None,\n",
    "                   CONSTRUCTOR_KEYWORD_ARGS = None,\n",
    "\n",
    "                   # List containing all positional arguments needed by the fit function\n",
    "                   FIT_POSITIONAL_ARGS = None,\n",
    "                   FIT_KEYWORD_ARGS = None\n",
    "                   ):\n",
    "\n",
    "\n",
    "          super(SearchInputRecommenderArgs, self).__init__()\n",
    "\n",
    "          if CONSTRUCTOR_POSITIONAL_ARGS is None:\n",
    "              CONSTRUCTOR_POSITIONAL_ARGS = []\n",
    "\n",
    "          if CONSTRUCTOR_KEYWORD_ARGS is None:\n",
    "              CONSTRUCTOR_KEYWORD_ARGS = {}\n",
    "\n",
    "          if FIT_POSITIONAL_ARGS is None:\n",
    "              FIT_POSITIONAL_ARGS = []\n",
    "\n",
    "          if FIT_KEYWORD_ARGS is None:\n",
    "              FIT_KEYWORD_ARGS = {}\n",
    "\n",
    "\n",
    "          assert isinstance(CONSTRUCTOR_POSITIONAL_ARGS, list), \"CONSTRUCTOR_POSITIONAL_ARGS must be a list\"\n",
    "          assert isinstance(CONSTRUCTOR_KEYWORD_ARGS, dict), \"CONSTRUCTOR_KEYWORD_ARGS must be a dict\"\n",
    "\n",
    "          assert isinstance(FIT_POSITIONAL_ARGS, list), \"FIT_POSITIONAL_ARGS must be a list\"\n",
    "          assert isinstance(FIT_KEYWORD_ARGS, dict), \"FIT_KEYWORD_ARGS must be a dict\"\n",
    "\n",
    "\n",
    "          self.CONSTRUCTOR_POSITIONAL_ARGS = CONSTRUCTOR_POSITIONAL_ARGS\n",
    "          self.CONSTRUCTOR_KEYWORD_ARGS = CONSTRUCTOR_KEYWORD_ARGS\n",
    "\n",
    "          self.FIT_POSITIONAL_ARGS = FIT_POSITIONAL_ARGS\n",
    "          self.FIT_KEYWORD_ARGS = FIT_KEYWORD_ARGS\n",
    "\n",
    "\n",
    "    def copy(self):\n",
    "\n",
    "\n",
    "        clone_object = SearchInputRecommenderArgs(\n",
    "                            CONSTRUCTOR_POSITIONAL_ARGS = self.CONSTRUCTOR_POSITIONAL_ARGS.copy(),\n",
    "                            CONSTRUCTOR_KEYWORD_ARGS = self.CONSTRUCTOR_KEYWORD_ARGS.copy(),\n",
    "                            FIT_POSITIONAL_ARGS = self.FIT_POSITIONAL_ARGS.copy(),\n",
    "                            FIT_KEYWORD_ARGS = self.FIT_KEYWORD_ARGS.copy()\n",
    "                            )\n",
    "\n",
    "\n",
    "        return clone_object\n",
    "\n",
    "\n",
    "\n",
    "def _compute_avg_time_non_none_values(data_list):\n",
    "\n",
    "    non_none_values = sum([value is not None for value in data_list])\n",
    "    total_value = sum([value if value is not None else 0.0 for value in data_list])\n",
    "\n",
    "    return total_value, \\\n",
    "           total_value/non_none_values if non_none_values != 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def get_result_string_evaluate_on_validation(results_run_single_cutoff, n_decimals=7):\n",
    "\n",
    "    output_str = \"\"\n",
    "\n",
    "    for metric in results_run_single_cutoff.keys():\n",
    "        output_str += \"{}: {:.{n_decimals}f}, \".format(metric, results_run_single_cutoff[metric], n_decimals = n_decimals)\n",
    "\n",
    "    return output_str\n",
    "\n",
    "\n",
    "\n",
    "class SearchAbstractClass(object):\n",
    "\n",
    "    ALGORITHM_NAME = \"SearchAbstractClass\"\n",
    "\n",
    "    # Available values for the save_model attribute\n",
    "    _SAVE_MODEL_VALUES = [\"all\", \"best\", \"last\", \"no\"]\n",
    "\n",
    "\n",
    "    # Value to be assigned to invalid configuration or if an Exception is raised\n",
    "    INVALID_CONFIG_VALUE = np.finfo(np.float16).max\n",
    "\n",
    "    def __init__(self, recommender_class,\n",
    "                 evaluator_validation = None,\n",
    "                 evaluator_test = None,\n",
    "                 verbose = True):\n",
    "\n",
    "        super(SearchAbstractClass, self).__init__()\n",
    "\n",
    "        self.recommender_class = recommender_class\n",
    "        self.verbose = verbose\n",
    "        self.log_file = None\n",
    "\n",
    "        self.results_test_best = {}\n",
    "        self.parameter_dictionary_best = {}\n",
    "\n",
    "        self.evaluator_validation = evaluator_validation\n",
    "\n",
    "        if evaluator_test is None:\n",
    "            self.evaluator_test = None\n",
    "        else:\n",
    "            self.evaluator_test = evaluator_test\n",
    "\n",
    "\n",
    "    def search(self, recommender_input_args,\n",
    "               parameter_search_space,\n",
    "               metric_to_optimize = \"MAP\",\n",
    "               n_cases = None,\n",
    "               output_folder_path = None,\n",
    "               output_file_name_root = None,\n",
    "               parallelize = False,\n",
    "               save_model = \"best\",\n",
    "               evaluate_on_test_each_best_solution = True,\n",
    "               save_metadata = True,\n",
    "               ):\n",
    "\n",
    "        raise NotImplementedError(\"Function search not implemented for this class\")\n",
    "\n",
    "\n",
    "    def _set_search_attributes(self, recommender_input_args,\n",
    "                               recommender_input_args_last_test,\n",
    "                               metric_to_optimize,\n",
    "                               output_folder_path,\n",
    "                               output_file_name_root,\n",
    "                               resume_from_saved,\n",
    "                               save_metadata,\n",
    "                               save_model,\n",
    "                               evaluate_on_test_each_best_solution,\n",
    "                               n_cases):\n",
    "\n",
    "\n",
    "        if save_model not in self._SAVE_MODEL_VALUES:\n",
    "           raise ValueError(\"{}: parameter save_model must be in '{}', provided was '{}'.\".format(self.ALGORITHM_NAME, self._SAVE_MODEL_VALUES, save_model))\n",
    "\n",
    "        self.output_folder_path = output_folder_path\n",
    "        self.output_file_name_root = output_file_name_root\n",
    "\n",
    "        # If directory does not exist, create\n",
    "        if not os.path.exists(self.output_folder_path):\n",
    "            os.makedirs(self.output_folder_path)\n",
    "\n",
    "        self.log_file = open(self.output_folder_path + self.output_file_name_root + \"_{}.txt\".format(self.ALGORITHM_NAME), \"a\")\n",
    "\n",
    "        if save_model == \"last\" and recommender_input_args_last_test is None:\n",
    "            self._write_log(\"{}: parameter save_model is 'last' but no recommender_input_args_last_test provided, saving best model on train data alone.\".format(self.ALGORITHM_NAME))\n",
    "            save_model = \"best\"\n",
    "\n",
    "\n",
    "\n",
    "        self.recommender_input_args = recommender_input_args\n",
    "        self.recommender_input_args_last_test = recommender_input_args_last_test\n",
    "        self.metric_to_optimize = metric_to_optimize\n",
    "        self.save_model = save_model\n",
    "        self.resume_from_saved = resume_from_saved\n",
    "        self.save_metadata = save_metadata\n",
    "        self.evaluate_on_test_each_best_solution = evaluate_on_test_each_best_solution\n",
    "\n",
    "        self.model_counter = 0\n",
    "        self._init_metadata_dict(n_cases = n_cases)\n",
    "\n",
    "        if self.save_metadata:\n",
    "            self.dataIO = DataIO(folder_path = self.output_folder_path)\n",
    "\n",
    "\n",
    "\n",
    "    def _init_metadata_dict(self, n_cases):\n",
    "\n",
    "        self.metadata_dict = {\"algorithm_name_search\": self.ALGORITHM_NAME,\n",
    "                              \"algorithm_name_recommender\": self.recommender_class.RECOMMENDER_NAME,\n",
    "                              \"exception_list\": [None]*n_cases,\n",
    "\n",
    "                              \"hyperparameters_list\": [None]*n_cases,\n",
    "                              \"hyperparameters_best\": None,\n",
    "                              \"hyperparameters_best_index\": None,\n",
    "\n",
    "                              \"result_on_validation_list\": [None]*n_cases,\n",
    "                              \"result_on_validation_best\": None,\n",
    "                              \"result_on_test_list\": [None]*n_cases,\n",
    "                              \"result_on_test_best\": None,\n",
    "\n",
    "                              \"time_on_train_list\": [None]*n_cases,\n",
    "                              \"time_on_train_total\": 0.0,\n",
    "                              \"time_on_train_avg\": 0.0,\n",
    "\n",
    "                              \"time_on_validation_list\": [None]*n_cases,\n",
    "                              \"time_on_validation_total\": 0.0,\n",
    "                              \"time_on_validation_avg\": 0.0,\n",
    "\n",
    "                              \"time_on_test_list\": [None]*n_cases,\n",
    "                              \"time_on_test_total\": 0.0,\n",
    "                              \"time_on_test_avg\": 0.0,\n",
    "\n",
    "                              \"result_on_last\": None,\n",
    "                              \"time_on_last_train\": None,\n",
    "                              \"time_on_last_test\": None,\n",
    "                              }\n",
    "\n",
    "\n",
    "    def _print(self, string):\n",
    "\n",
    "        if self.verbose:\n",
    "            print(string)\n",
    "\n",
    "\n",
    "    def _write_log(self, string):\n",
    "\n",
    "        self._print(string)\n",
    "\n",
    "        if self.log_file is not None:\n",
    "            self.log_file.write(string)\n",
    "            self.log_file.flush()\n",
    "\n",
    "\n",
    "    def _fit_model(self, current_fit_parameters):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Construct a new recommender instance\n",
    "        recommender_instance = self.recommender_class(*self.recommender_input_args.CONSTRUCTOR_POSITIONAL_ARGS,\n",
    "                                                      **self.recommender_input_args.CONSTRUCTOR_KEYWORD_ARGS)\n",
    "\n",
    "\n",
    "        self._print(\"{}: Testing config: {}\".format(self.ALGORITHM_NAME, current_fit_parameters))\n",
    "\n",
    "\n",
    "        recommender_instance.fit(*self.recommender_input_args.FIT_POSITIONAL_ARGS,\n",
    "                                 **self.recommender_input_args.FIT_KEYWORD_ARGS,\n",
    "                                 **current_fit_parameters)\n",
    "\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        return recommender_instance, train_time\n",
    "\n",
    "\n",
    "\n",
    "    def _evaluate_on_validation(self, current_fit_parameters):\n",
    "\n",
    "        recommender_instance, train_time = self._fit_model(current_fit_parameters)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Evaluate recommender and get results for the first cutoff\n",
    "        result_dict, _ = self.evaluator_validation.evaluateRecommender(recommender_instance)\n",
    "        result_dict = result_dict[list(result_dict.keys())[0]]\n",
    "\n",
    "        evaluation_time = time.time() - start_time\n",
    "\n",
    "        result_string = get_result_string_evaluate_on_validation(result_dict, n_decimals=7)\n",
    "\n",
    "        return result_dict, result_string, recommender_instance, train_time, evaluation_time\n",
    "\n",
    "\n",
    "\n",
    "    def _evaluate_on_test(self, recommender_instance, current_fit_parameters_dict, print_log = True):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Evaluate recommender and get results for the first cutoff\n",
    "        result_dict, result_string = self.evaluator_test.evaluateRecommender(recommender_instance)\n",
    "\n",
    "        evaluation_test_time = time.time() - start_time\n",
    "\n",
    "        if print_log:\n",
    "            self._write_log(\"{}: Best config evaluated with evaluator_test. Config: {} - results:\\n{}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                                                         current_fit_parameters_dict,\n",
    "                                                                                                         result_string))\n",
    "\n",
    "        return result_dict, result_string, evaluation_test_time\n",
    "\n",
    "\n",
    "\n",
    "    def _evaluate_on_test_with_data_last(self):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Construct a new recommender instance\n",
    "        recommender_instance = self.recommender_class(*self.recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS,\n",
    "                                                      **self.recommender_input_args_last_test.CONSTRUCTOR_KEYWORD_ARGS)\n",
    "\n",
    "        # Check if last was already evaluated\n",
    "        if self.resume_from_saved:\n",
    "            result_on_last_saved_flag = self.metadata_dict[\"result_on_last\"] is not None and \\\n",
    "                                        self.metadata_dict[\"time_on_last_train\"] is not None and \\\n",
    "                                        self.metadata_dict[\"time_on_last_test\"] is not None\n",
    "\n",
    "            if result_on_last_saved_flag:\n",
    "                self._print(\"{}: Resuming '{}'... Result on last already available.\".format(self.ALGORITHM_NAME, self.output_file_name_root))\n",
    "                return\n",
    "\n",
    "\n",
    "\n",
    "        self._print(\"{}: Evaluation with constructor data for final test. Using best config: {}\".format(self.ALGORITHM_NAME, self.metadata_dict[\"hyperparameters_best\"]))\n",
    "\n",
    "\n",
    "        # Use the hyperparameters that have been saved\n",
    "        assert self.metadata_dict[\"hyperparameters_best\"] is not None, \"{}: Best hyperparameters not available, the search might have failed.\".format(self.ALGORITHM_NAME)\n",
    "        fit_keyword_args = self.metadata_dict[\"hyperparameters_best\"].copy()\n",
    "\n",
    "\n",
    "        recommender_instance.fit(*self.recommender_input_args_last_test.FIT_POSITIONAL_ARGS,\n",
    "                                 **fit_keyword_args)\n",
    "\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        result_dict_test, result_string, evaluation_test_time = self._evaluate_on_test(recommender_instance, fit_keyword_args, print_log = False)\n",
    "\n",
    "        self._write_log(\"{}: Best config evaluated with evaluator_test with constructor data for final test. Config: {} - results:\\n{}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                                                                                          self.metadata_dict[\"hyperparameters_best\"],\n",
    "                                                                                                                                          result_string))\n",
    "\n",
    "        self.metadata_dict[\"result_on_last\"] = result_dict_test\n",
    "        self.metadata_dict[\"time_on_last_train\"] = train_time\n",
    "        self.metadata_dict[\"time_on_last_test\"] = evaluation_test_time\n",
    "\n",
    "        if self.save_metadata:\n",
    "            self.dataIO.save_data(data_dict_to_save = self.metadata_dict.copy(),\n",
    "                                  file_name = self.output_file_name_root + \"_metadata\")\n",
    "\n",
    "        if self.save_model in [\"all\", \"best\", \"last\"]:\n",
    "            self._print(\"{}: Saving model in {}\\n\".format(self.ALGORITHM_NAME, self.output_folder_path + self.output_file_name_root))\n",
    "            recommender_instance.save_model(self.output_folder_path, file_name =self.output_file_name_root + \"_best_model_last\")\n",
    "\n",
    "\n",
    "    def _objective_function(self, current_fit_parameters_dict):\n",
    "\n",
    "        try:\n",
    "\n",
    "            self.metadata_dict[\"hyperparameters_list\"][self.model_counter] = current_fit_parameters_dict.copy()\n",
    "\n",
    "            result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\n",
    "\n",
    "            current_result = - result_dict[self.metric_to_optimize]\n",
    "\n",
    "            # If the recommender uses Earlystopping, get the selected number of epochs\n",
    "            if isinstance(recommender_instance, Incremental_Training_Early_Stopping):\n",
    "\n",
    "                n_epochs_early_stopping_dict = recommender_instance.get_early_stopping_final_epochs_dict()\n",
    "                current_fit_parameters_dict = current_fit_parameters_dict.copy()\n",
    "\n",
    "                for epoch_label in n_epochs_early_stopping_dict.keys():\n",
    "\n",
    "                    epoch_value = n_epochs_early_stopping_dict[epoch_label]\n",
    "                    current_fit_parameters_dict[epoch_label] = epoch_value\n",
    "\n",
    "\n",
    "\n",
    "            # Always save best model separately\n",
    "            if self.save_model in [\"all\"]:\n",
    "                self._print(\"{}: Saving model in {}\\n\".format(self.ALGORITHM_NAME, self.output_folder_path + self.output_file_name_root))\n",
    "                recommender_instance.save_model(self.output_folder_path, file_name = self.output_file_name_root + \"_model_{}\".format(self.model_counter))\n",
    "\n",
    "\n",
    "            if self.metadata_dict[\"result_on_validation_best\"] is None:\n",
    "                new_best_config_found = True\n",
    "            else:\n",
    "                best_solution_val = self.metadata_dict[\"result_on_validation_best\"][self.metric_to_optimize]\n",
    "                new_best_config_found = best_solution_val < result_dict[self.metric_to_optimize]\n",
    "\n",
    "\n",
    "            if new_best_config_found:\n",
    "\n",
    "                self._write_log(\"{}: New best config found. Config {}: {} - results: {}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                                           self.model_counter,\n",
    "                                                                                           current_fit_parameters_dict,\n",
    "                                                                                           result_string))\n",
    "\n",
    "                if self.save_model in [\"all\", \"best\"]:\n",
    "                    self._print(\"{}: Saving model in {}\\n\".format(self.ALGORITHM_NAME, self.output_folder_path + self.output_file_name_root))\n",
    "                    recommender_instance.save_model(self.output_folder_path, file_name =self.output_file_name_root + \"_best_model\")\n",
    "\n",
    "\n",
    "                if self.evaluator_test is not None and self.evaluate_on_test_each_best_solution:\n",
    "                    result_dict_test, _, evaluation_test_time = self._evaluate_on_test(recommender_instance, current_fit_parameters_dict, print_log = True)\n",
    "\n",
    "\n",
    "            else:\n",
    "                self._write_log(\"{}: Config {} is suboptimal. Config: {} - results: {}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                                          self.model_counter,\n",
    "                                                                                          current_fit_parameters_dict,\n",
    "                                                                                          result_string))\n",
    "\n",
    "\n",
    "\n",
    "            if current_result >= self.INVALID_CONFIG_VALUE:\n",
    "                self._write_log(\"{}: WARNING! Config {} returned a value equal or worse than the default value to be assigned to invalid configurations.\"\n",
    "                                \" If no better valid configuration is found, this parameter search may produce an invalid result.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "            self.metadata_dict[\"result_on_validation_list\"][self.model_counter] = result_dict.copy()\n",
    "\n",
    "            self.metadata_dict[\"time_on_train_list\"][self.model_counter] = train_time\n",
    "            self.metadata_dict[\"time_on_validation_list\"][self.model_counter] = evaluation_time\n",
    "\n",
    "            self.metadata_dict[\"time_on_train_total\"], self.metadata_dict[\"time_on_train_avg\"] = \\\n",
    "                _compute_avg_time_non_none_values(self.metadata_dict[\"time_on_train_list\"])\n",
    "            self.metadata_dict[\"time_on_validation_total\"], self.metadata_dict[\"time_on_validation_avg\"] = \\\n",
    "                _compute_avg_time_non_none_values(self.metadata_dict[\"time_on_validation_list\"])\n",
    "\n",
    "\n",
    "            if new_best_config_found:\n",
    "                self.metadata_dict[\"hyperparameters_best\"] = current_fit_parameters_dict.copy()\n",
    "                self.metadata_dict[\"hyperparameters_best_index\"] = self.model_counter\n",
    "                self.metadata_dict[\"result_on_validation_best\"] = result_dict.copy()\n",
    "\n",
    "                if self.evaluator_test is not None and self.evaluate_on_test_each_best_solution:\n",
    "                    self.metadata_dict[\"result_on_test_best\"] = result_dict_test.copy()\n",
    "                    self.metadata_dict[\"result_on_test_list\"][self.model_counter] = result_dict_test.copy()\n",
    "                    self.metadata_dict[\"time_on_test_list\"][self.model_counter] = evaluation_test_time\n",
    "\n",
    "                    self.metadata_dict[\"time_on_test_total\"], self.metadata_dict[\"time_on_test_avg\"] = \\\n",
    "                        _compute_avg_time_non_none_values(self.metadata_dict[\"time_on_test_list\"])\n",
    "\n",
    "\n",
    "        except (KeyboardInterrupt, SystemExit) as e:\n",
    "            # If getting a interrupt, terminate without saving the exception\n",
    "            raise e\n",
    "\n",
    "        except:\n",
    "            # Catch any error: Exception, Tensorflow errors etc...\n",
    "\n",
    "            traceback_string = traceback.format_exc()\n",
    "\n",
    "            self._write_log(\"{}: Config {} Exception. Config: {} - Exception: {}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                                  self.model_counter,\n",
    "                                                                                  current_fit_parameters_dict,\n",
    "                                                                                  traceback_string))\n",
    "\n",
    "            self.metadata_dict[\"exception_list\"][self.model_counter] = traceback_string\n",
    "\n",
    "\n",
    "            # Assign to this configuration the worst possible score\n",
    "            # Being a minimization problem, set it to the max value of a float\n",
    "            current_result = + self.INVALID_CONFIG_VALUE\n",
    "\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "        if self.save_metadata:\n",
    "            self.dataIO.save_data(data_dict_to_save = self.metadata_dict.copy(),\n",
    "                                  file_name = self.output_file_name_root + \"_metadata\")\n",
    "\n",
    "        self.model_counter += 1\n",
    "\n",
    "        return current_result\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# SearchBayesianSkopt.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 14/12/18\n",
    "@author: Emanuele Chioso, Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "# from utils.ParameterTuning.SearchAbstractClass import SearchAbstractClass\n",
    "import traceback\n",
    "\n",
    "\n",
    "class SearchBayesianSkopt(SearchAbstractClass):\n",
    "\n",
    "    ALGORITHM_NAME = \"SearchBayesianSkopt\"\n",
    "\n",
    "    def __init__(self, recommender_class, evaluator_validation = None, evaluator_test = None, verbose = True):\n",
    "\n",
    "        assert evaluator_validation is not None, \"{}: evaluator_validation must be provided\".format(self.ALGORITHM_NAME)\n",
    "\n",
    "        super(SearchBayesianSkopt, self).__init__(recommender_class,\n",
    "                                                  evaluator_validation = evaluator_validation,\n",
    "                                                  evaluator_test = evaluator_test,\n",
    "                                                  verbose = verbose)\n",
    "\n",
    "\n",
    "\n",
    "    def _set_skopt_params(self, n_calls = 70,\n",
    "                          n_random_starts = 20,\n",
    "                          n_points = 10000,\n",
    "                          n_jobs = 1,\n",
    "                          # noise = 'gaussian',\n",
    "                          noise = 1e-5,\n",
    "                          acq_func = 'gp_hedge',\n",
    "                          acq_optimizer = 'auto',\n",
    "                          random_state = None,\n",
    "                          verbose = True,\n",
    "                          n_restarts_optimizer = 10,\n",
    "                          xi = 0.01,\n",
    "                          kappa = 1.96,\n",
    "                          x0 = None,\n",
    "                          y0 = None):\n",
    "        \"\"\"\n",
    "        wrapper to change the params of the bayesian optimizator.\n",
    "        for further details:\n",
    "        https://scikit-optimize.github.io/#skopt.gp_minimize\n",
    "        \"\"\"\n",
    "        self.n_point = n_points\n",
    "        self.n_calls = n_calls\n",
    "        self.n_random_starts = n_random_starts\n",
    "        self.n_jobs = n_jobs\n",
    "        self.acq_func = acq_func\n",
    "        self.acq_optimizer = acq_optimizer\n",
    "        self.random_state = random_state\n",
    "        self.n_restarts_optimizer = n_restarts_optimizer\n",
    "        self.verbose = verbose\n",
    "        self.xi = xi\n",
    "        self.kappa = kappa\n",
    "        self.noise = noise\n",
    "        self.x0 = x0\n",
    "        self.y0 = y0\n",
    "\n",
    "\n",
    "\n",
    "    def _resume_from_saved(self):\n",
    "\n",
    "        try:\n",
    "            self.metadata_dict = self.dataIO.load_data(file_name = self.output_file_name_root + \"_metadata\")\n",
    "\n",
    "        except (KeyboardInterrupt, SystemExit) as e:\n",
    "            # If getting a interrupt, terminate without saving the exception\n",
    "            raise e\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            self._write_log(\"{}: Resuming '{}' Failed, no such file exists.\\n\".format(self.ALGORITHM_NAME, self.output_file_name_root))\n",
    "            self.resume_from_saved = False\n",
    "            return None, None\n",
    "\n",
    "        except Exception as e:\n",
    "            self._write_log(\"{}: Resuming '{}' Failed, generic exception: {}.\\n\".format(self.ALGORITHM_NAME, self.output_file_name_root, str(e)))\n",
    "            traceback.print_exc()\n",
    "            self.resume_from_saved = False\n",
    "            return None, None\n",
    "\n",
    "        # Get hyperparameter list and corresponding result\n",
    "        # Make sure that the hyperparameters only contain those given as input and not others like the number of epochs\n",
    "        # selected by earlystopping\n",
    "        hyperparameters_list_saved = self.metadata_dict['hyperparameters_list']\n",
    "        result_on_validation_list_saved = self.metadata_dict['result_on_validation_list']\n",
    "\n",
    "        hyperparameters_list_input = []\n",
    "        result_on_validation_list_input = []\n",
    "\n",
    "        # The hyperparameters are saved for all cases even if they throw an exception\n",
    "        while self.model_counter<len(hyperparameters_list_saved) and hyperparameters_list_saved[self.model_counter] is not None:\n",
    "\n",
    "            hyperparameters_config_saved = hyperparameters_list_saved[self.model_counter]\n",
    "\n",
    "            hyperparameters_config_input = []\n",
    "\n",
    "            # Add only those having a search space, in the correct ordering\n",
    "            for index in range(len(self.hyperparams_names)):\n",
    "                key = self.hyperparams_names[index]\n",
    "                value_saved = hyperparameters_config_saved[key]\n",
    "\n",
    "                # Check if single value categorical. It is aimed at intercepting\n",
    "                # Hyperparameters that are chosen via early stopping and set them as the\n",
    "                # maximum value as per hyperparameter search space. If not, the gp_minimize will return an error\n",
    "                # as some values will be outside (lower) than the search space\n",
    "\n",
    "                if isinstance(self.hyperparams_values[index], Categorical) and self.hyperparams_values[index].transformed_size == 1:\n",
    "                    value_input = self.hyperparams_values[index].bounds[0]\n",
    "                else:\n",
    "                    value_input = value_saved\n",
    "\n",
    "                hyperparameters_config_input.append(value_input)\n",
    "\n",
    "\n",
    "            hyperparameters_list_input.append(hyperparameters_config_input)\n",
    "\n",
    "            # Check if the hyperparameters have a valid result or an exception\n",
    "            validation_result = result_on_validation_list_saved[self.model_counter]\n",
    "\n",
    "            if validation_result is None:\n",
    "                # Exception detected\n",
    "                result_on_validation_list_input.append(+ self.INVALID_CONFIG_VALUE)\n",
    "\n",
    "                assert self.metadata_dict[\"exception_list\"][self.model_counter] is not None, \\\n",
    "                    \"{}: Resuming '{}' Failed due to inconsistent data. Invalid validation result found in position {} but no corresponding exception detected.\".format(self.ALGORITHM_NAME, self.output_file_name_root, self.model_counter)\n",
    "            else:\n",
    "                result_on_validation_list_input.append(- validation_result[self.metric_to_optimize])\n",
    "\n",
    "\n",
    "\n",
    "            self.model_counter += 1\n",
    "\n",
    "\n",
    "        self._print(\"{}: Resuming '{}'... Loaded {} configurations.\".format(self.ALGORITHM_NAME, self.output_file_name_root, self.model_counter))\n",
    "\n",
    "\n",
    "        # If the data structure exists but is empty, return None\n",
    "        if len(hyperparameters_list_input) == 0:\n",
    "            self.resume_from_saved = False\n",
    "            return None, None\n",
    "\n",
    "        # If loaded less configurations than desired ones\n",
    "        if self.model_counter < self.n_calls:\n",
    "            self.resume_from_saved = False\n",
    "\n",
    "\n",
    "        return hyperparameters_list_input, result_on_validation_list_input\n",
    "\n",
    "\n",
    "    def search(self, recommender_input_args,\n",
    "               parameter_search_space,\n",
    "               metric_to_optimize = \"MAP\",\n",
    "               n_cases = 20,\n",
    "               n_random_starts = 5,\n",
    "               output_folder_path = None,\n",
    "               output_file_name_root = None,\n",
    "               save_model = \"best\",\n",
    "               save_metadata = True,\n",
    "               resume_from_saved = False,\n",
    "               recommender_input_args_last_test = None,\n",
    "               evaluate_on_test_each_best_solution = True,\n",
    "               ):\n",
    "        \"\"\"\n",
    "        :param recommender_input_args:\n",
    "        :param parameter_search_space:\n",
    "        :param metric_to_optimize:\n",
    "        :param n_cases:\n",
    "        :param n_random_starts:\n",
    "        :param output_folder_path:\n",
    "        :param output_file_name_root:\n",
    "        :param save_model:          \"no\"    don't save anything\n",
    "                                    \"all\"   save every model\n",
    "                                    \"best\"  save the best model trained on train data alone and on last, if present\n",
    "                                    \"last\"  save only last, if present\n",
    "        :param save_metadata:\n",
    "        :param recommender_input_args_last_test:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self._set_skopt_params()    ### default parameters are set here\n",
    "\n",
    "        self._set_search_attributes(recommender_input_args,\n",
    "                                    recommender_input_args_last_test,\n",
    "                                    metric_to_optimize,\n",
    "                                    output_folder_path,\n",
    "                                    output_file_name_root,\n",
    "                                    resume_from_saved,\n",
    "                                    save_metadata,\n",
    "                                    save_model,\n",
    "                                    evaluate_on_test_each_best_solution,\n",
    "                                    n_cases)\n",
    "\n",
    "\n",
    "        self.parameter_search_space = parameter_search_space\n",
    "        self.n_random_starts = n_random_starts\n",
    "        self.n_calls = n_cases\n",
    "        self.n_jobs = 1\n",
    "        self.n_loaded_counter = 0\n",
    "\n",
    "\n",
    "        self.hyperparams = dict()\n",
    "        self.hyperparams_names = list()\n",
    "        self.hyperparams_values = list()\n",
    "\n",
    "        skopt_types = [Real, Integer, Categorical]\n",
    "\n",
    "        for name, hyperparam in self.parameter_search_space.items():\n",
    "\n",
    "            if any(isinstance(hyperparam, sko_type) for sko_type in skopt_types):\n",
    "                self.hyperparams_names.append(name)\n",
    "                self.hyperparams_values.append(hyperparam)\n",
    "                self.hyperparams[name] = hyperparam\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"{}: Unexpected parameter type: {} - {}\".format(self.ALGORITHM_NAME, str(name), str(hyperparam)))\n",
    "\n",
    "\n",
    "        if self.resume_from_saved:\n",
    "            hyperparameters_list_input, result_on_validation_list_saved = self._resume_from_saved()\n",
    "            self.x0 = hyperparameters_list_input\n",
    "            self.y0 = result_on_validation_list_saved\n",
    "\n",
    "            self.n_random_starts = max(0, self.n_random_starts - self.model_counter)\n",
    "            self.n_calls = max(0, self.n_calls - self.model_counter)\n",
    "            self.n_loaded_counter = self.model_counter\n",
    "\n",
    "\n",
    "\n",
    "        self.result = gp_minimize(self._objective_function_list_input,\n",
    "                                  self.hyperparams_values,\n",
    "                                  base_estimator=None,\n",
    "                                  n_calls=self.n_calls,\n",
    "                                  n_random_starts=self.n_random_starts,\n",
    "                                  acq_func=self.acq_func,\n",
    "                                  acq_optimizer=self.acq_optimizer,\n",
    "                                  x0=self.x0,\n",
    "                                  y0=self.y0,\n",
    "                                  random_state=self.random_state,\n",
    "                                  verbose=self.verbose,\n",
    "                                  callback=None,\n",
    "                                  n_points=self.n_point,\n",
    "                                  n_restarts_optimizer=self.n_restarts_optimizer,\n",
    "                                  xi=self.xi,\n",
    "                                  kappa=self.kappa,\n",
    "                                  noise=self.noise,\n",
    "                                  n_jobs=self.n_jobs)\n",
    "\n",
    "\n",
    "        if self.n_loaded_counter < self.model_counter:\n",
    "            self._write_log(\"{}: Search complete. Best config is {}: {}\\n\".format(self.ALGORITHM_NAME,\n",
    "                                                                           self.metadata_dict[\"hyperparameters_best_index\"],\n",
    "                                                                           self.metadata_dict[\"hyperparameters_best\"]))\n",
    "\n",
    "\n",
    "        if self.recommender_input_args_last_test is not None:\n",
    "            self._evaluate_on_test_with_data_last()\n",
    "\n",
    "\n",
    "    def _objective_function_list_input(self, current_fit_parameters_list_of_values):\n",
    "\n",
    "        current_fit_parameters_dict = dict(zip(self.hyperparams_names, current_fit_parameters_list_of_values))\n",
    "\n",
    "        return self._objective_function(current_fit_parameters_dict)\n",
    "\n",
    "\n",
    "\n",
    "# hyperparameter_search.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 22/11/17\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "# Automate hyperparameter tuning\n",
    "# using bayesian optimization with scikit-optimize\n",
    "# -------------------------------------------------\n",
    "\n",
    "import os, multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "######################################################################\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import traceback\n",
    "# from Utils.PoolWithSubprocess import PoolWithSubprocess\n",
    "\n",
    "\n",
    "def run_KNNRecommender_on_similarity_type(similarity_type, parameterSearch,\n",
    "                                          parameter_search_space,\n",
    "                                          recommender_input_args,\n",
    "                                          n_cases,\n",
    "                                          n_random_starts,\n",
    "                                          resume_from_saved,\n",
    "                                          save_model,\n",
    "                                          output_folder_path,\n",
    "                                          output_file_name_root,\n",
    "                                          metric_to_optimize,\n",
    "                                          allow_weighting=False,\n",
    "                                          recommender_input_args_last_test=None):\n",
    "\n",
    "    original_parameter_search_space = parameter_search_space\n",
    "\n",
    "    hyperparameters_range_dictionary = {}\n",
    "    hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n",
    "    hyperparameters_range_dictionary[\"shrink\"] = Integer(0, 1000)\n",
    "    hyperparameters_range_dictionary[\"similarity\"] = Categorical([similarity_type])\n",
    "    hyperparameters_range_dictionary[\"normalize\"] = Categorical([True, False])\n",
    "\n",
    "    is_set_similarity = similarity_type in [\"tversky\", \"dice\", \"jaccard\", \"tanimoto\"]\n",
    "\n",
    "    if similarity_type == \"asymmetric\":\n",
    "        hyperparameters_range_dictionary[\"asymmetric_alpha\"] = Real(low=0, high=2, prior='uniform')\n",
    "        hyperparameters_range_dictionary[\"normalize\"] = Categorical([True])\n",
    "\n",
    "    elif similarity_type == \"tversky\":\n",
    "        hyperparameters_range_dictionary[\"tversky_alpha\"] = Real(low=0, high=2, prior='uniform')\n",
    "        hyperparameters_range_dictionary[\"tversky_beta\"] = Real(low=0, high=2, prior='uniform')\n",
    "        hyperparameters_range_dictionary[\"normalize\"] = Categorical([True])\n",
    "\n",
    "    elif similarity_type == \"euclidean\":\n",
    "        hyperparameters_range_dictionary[\"normalize\"] = Categorical([True, False])\n",
    "        hyperparameters_range_dictionary[\"normalize_avg_row\"] = Categorical([True, False])\n",
    "        hyperparameters_range_dictionary[\"similarity_from_distance_mode\"] = Categorical([\"lin\", \"log\", \"exp\"])\n",
    "\n",
    "    if not is_set_similarity:\n",
    "\n",
    "        if allow_weighting:\n",
    "            hyperparameters_range_dictionary[\"feature_weighting\"] = Categorical([\"none\", \"BM25\", \"TF-IDF\"])\n",
    "\n",
    "    local_parameter_search_space = {**hyperparameters_range_dictionary, **original_parameter_search_space}\n",
    "\n",
    "    parameterSearch.search(recommender_input_args,\n",
    "                           parameter_search_space=local_parameter_search_space,\n",
    "                           n_cases=n_cases,\n",
    "                           n_random_starts=n_random_starts,\n",
    "                           resume_from_saved=resume_from_saved,\n",
    "                           save_model=save_model,\n",
    "                           output_folder_path=output_folder_path,\n",
    "                           output_file_name_root=output_file_name_root + \"_\" + similarity_type,\n",
    "                           metric_to_optimize=metric_to_optimize,\n",
    "                           recommender_input_args_last_test=recommender_input_args_last_test)\n",
    "\n",
    "\n",
    "def runParameterSearch_Content(recommender_class, URM_train, ICM_object, ICM_name, URM_train_last_test=None,\n",
    "                               n_cases=30, n_random_starts=5, resume_from_saved=False, save_model=\"best\",\n",
    "                               evaluator_validation=None, evaluator_test=None, metric_to_optimize=\"PRECISION\",\n",
    "                               output_folder_path=\"result_experiments/\", parallelizeKNN=False, allow_weighting=True,\n",
    "                               similarity_type_list=None):\n",
    "    # If directory does not exist, create\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    URM_train = URM_train.copy()\n",
    "    ICM_object = ICM_object.copy()\n",
    "\n",
    "    if URM_train_last_test is not None:\n",
    "        URM_train_last_test = URM_train_last_test.copy()\n",
    "\n",
    "    ##########################################################################################################\n",
    "\n",
    "    output_file_name_root = recommender_class.RECOMMENDER_NAME + \"_{}\".format(ICM_name)\n",
    "\n",
    "    parameterSearch = SearchBayesianSkopt(recommender_class,\n",
    "                                          evaluator_validation=evaluator_validation,\n",
    "                                          evaluator_test=evaluator_test)\n",
    "\n",
    "    if similarity_type_list is None:\n",
    "        similarity_type_list = ['cosine', 'jaccard', \"asymmetric\", \"dice\", \"tversky\"]\n",
    "\n",
    "    recommender_input_args = SearchInputRecommenderArgs(\n",
    "        CONSTRUCTOR_POSITIONAL_ARGS=[URM_train, ICM_object],\n",
    "        CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        FIT_POSITIONAL_ARGS=[],\n",
    "        FIT_KEYWORD_ARGS={}\n",
    "    )\n",
    "\n",
    "    if URM_train_last_test is not None:\n",
    "        recommender_input_args_last_test = recommender_input_args.copy()\n",
    "        recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n",
    "    else:\n",
    "        recommender_input_args_last_test = None\n",
    "\n",
    "    run_KNNCBFRecommender_on_similarity_type_partial = partial(run_KNNRecommender_on_similarity_type,\n",
    "                                                               recommender_input_args=recommender_input_args,\n",
    "                                                               parameter_search_space={},\n",
    "                                                               parameterSearch=parameterSearch,\n",
    "                                                               n_cases=n_cases,\n",
    "                                                               n_random_starts=n_random_starts,\n",
    "                                                               resume_from_saved=resume_from_saved,\n",
    "                                                               save_model=save_model,\n",
    "                                                               output_folder_path=output_folder_path,\n",
    "                                                               output_file_name_root=output_file_name_root,\n",
    "                                                               metric_to_optimize=metric_to_optimize,\n",
    "                                                               allow_weighting=allow_weighting,\n",
    "                                                               recommender_input_args_last_test=recommender_input_args_last_test)\n",
    "\n",
    "    # if parallelizeKNN:\n",
    "    #     pool = multiprocessing.Pool(processes=int(multiprocessing.cpu_count()), maxtasksperchild=1)\n",
    "    #     pool.map(run_KNNCBFRecommender_on_similarity_type_partial, similarity_type_list)\n",
    "    #\n",
    "    #     pool.close()\n",
    "    #     pool.join()\n",
    "    #\n",
    "    # else:\n",
    "    #\n",
    "    for similarity_type in similarity_type_list:\n",
    "        run_KNNCBFRecommender_on_similarity_type_partial(similarity_type)\n",
    "\n",
    "\n",
    "def runParameterSearch_Collaborative(recommender_class, URM_train, URM_train_last_test=None,\n",
    "                                     metric_to_optimize=\"PRECISION\",\n",
    "                                     evaluator_validation=None, evaluator_test=None,\n",
    "                                     evaluator_validation_earlystopping=None,\n",
    "                                     output_folder_path=\"result_experiments/\", parallelizeKNN=True,\n",
    "                                     n_cases=35, n_random_starts=5, resume_from_saved=False, save_model=\"best\",\n",
    "                                     allow_weighting=True,\n",
    "                                     similarity_type_list=None):\n",
    "\n",
    "    # If directory does not exist, create\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    earlystopping_keywargs = {\"validation_every_n\": 5,\n",
    "                              \"stop_on_validation\": True,\n",
    "                              \"evaluator_object\": evaluator_validation_earlystopping,\n",
    "                              \"lower_validations_allowed\": 5,\n",
    "                              \"validation_metric\": metric_to_optimize,\n",
    "                              }\n",
    "\n",
    "    URM_train = URM_train.copy()\n",
    "\n",
    "    if URM_train_last_test is not None:\n",
    "        URM_train_last_test = URM_train_last_test.copy()\n",
    "\n",
    "    try:\n",
    "\n",
    "        output_file_name_root = recommender_class.RECOMMENDER_NAME\n",
    "\n",
    "        parameterSearch = SearchBayesianSkopt(recommender_class,\n",
    "                                              evaluator_validation=evaluator_validation,\n",
    "                                              evaluator_test=evaluator_test)\n",
    "\n",
    "        # if recommender_class in [TopPop, GlobalEffects, Random]:\n",
    "        #     \"\"\"\n",
    "        #     TopPop, GlobalEffects and Random have no parameters therefore only one evaluation is needed\n",
    "        #     \"\"\"\n",
    "        #\n",
    "        #     parameterSearch = SearchSingleCase(recommender_class,\n",
    "        #                                        evaluator_validation=evaluator_validation,\n",
    "        #                                        evaluator_test=evaluator_test)\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={}\n",
    "        #     )\n",
    "        #\n",
    "        #     if URM_train_last_test is not None:\n",
    "        #         recommender_input_args_last_test = recommender_input_args.copy()\n",
    "        #         recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n",
    "        #     else:\n",
    "        #         recommender_input_args_last_test = None\n",
    "        #\n",
    "        #     parameterSearch.search(recommender_input_args,\n",
    "        #                            recommender_input_args_last_test=recommender_input_args_last_test,\n",
    "        #                            fit_hyperparameters_values={},\n",
    "        #                            output_folder_path=output_folder_path,\n",
    "        #                            output_file_name_root=output_file_name_root,\n",
    "        #                            resume_from_saved=resume_from_saved,\n",
    "        #                            save_model=save_model,\n",
    "        #                            )\n",
    "        #\n",
    "        #     return\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "#         if recommender_class in [ItemKNNCFRecommender, UserKNNCFRecommender]:\n",
    "\n",
    "#             if similarity_type_list is None:\n",
    "#                 similarity_type_list = ['cosine', 'jaccard', \"asymmetric\", \"dice\", \"tversky\"]\n",
    "\n",
    "#             recommender_input_args = SearchInputRecommenderArgs(\n",
    "#                 CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "#                 CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "#                 FIT_POSITIONAL_ARGS=[],\n",
    "#                 FIT_KEYWORD_ARGS={}\n",
    "#             )\n",
    "\n",
    "#             if URM_train_last_test is not None:\n",
    "#                 recommender_input_args_last_test = recommender_input_args.copy()\n",
    "#                 recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n",
    "#             else:\n",
    "#                 recommender_input_args_last_test = None\n",
    "\n",
    "#             run_KNNCFRecommender_on_similarity_type_partial = partial(run_KNNRecommender_on_similarity_type,\n",
    "#                                                                       recommender_input_args=recommender_input_args,\n",
    "#                                                                       parameter_search_space={},\n",
    "#                                                                       parameterSearch=parameterSearch,\n",
    "#                                                                       n_cases=n_cases,\n",
    "#                                                                       n_random_starts=n_random_starts,\n",
    "#                                                                       resume_from_saved=resume_from_saved,\n",
    "#                                                                       save_model=save_model,\n",
    "#                                                                       output_folder_path=output_folder_path,\n",
    "#                                                                       output_file_name_root=output_file_name_root,\n",
    "#                                                                       metric_to_optimize=metric_to_optimize,\n",
    "#                                                                       allow_weighting=allow_weighting,\n",
    "#                                                                       recommender_input_args_last_test=recommender_input_args_last_test)\n",
    "\n",
    "            # if parallelizeKNN:\n",
    "            #     pool = multiprocessing.Pool(processes=multiprocessing.cpu_count(), maxtasksperchild=1)\n",
    "            #     pool.map(run_KNNCFRecommender_on_similarity_type_partial, similarity_type_list)\n",
    "            #\n",
    "            #     pool.close()\n",
    "            #     pool.join()\n",
    "            #\n",
    "            # else:\n",
    "\n",
    "#             for similarity_type in similarity_type_list:\n",
    "#                 run_KNNCFRecommender_on_similarity_type_partial(similarity_type)\n",
    "\n",
    "#             return\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        # if recommender_class is P3alphaRecommender:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n",
    "        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=0, high=2, prior='uniform')\n",
    "        #     hyperparameters_range_dictionary[\"normalize_similarity\"] = Categorical([True, False])\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={}\n",
    "        #     )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        # if recommender_class is RP3betaRecommender:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n",
    "        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=0, high=2, prior='uniform')\n",
    "        #     hyperparameters_range_dictionary[\"beta\"] = Real(low=0, high=2, prior='uniform')\n",
    "        #     hyperparameters_range_dictionary[\"normalize_similarity\"] = Categorical([True, False])\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={}\n",
    "        #     )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        if recommender_class is MatrixFactorization_FunkSVD_Cython:\n",
    "            hyperparameters_range_dictionary = {}\n",
    "            hyperparameters_range_dictionary[\"sgd_mode\"] = Categorical([\"sgd\", \"adagrad\", \"adam\"])\n",
    "            hyperparameters_range_dictionary[\"epochs\"] = Categorical([500])\n",
    "            hyperparameters_range_dictionary[\"use_bias\"] = Categorical([True, False])\n",
    "            hyperparameters_range_dictionary[\"batch_size\"] = Categorical([1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024])\n",
    "            hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n",
    "            hyperparameters_range_dictionary[\"item_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "            hyperparameters_range_dictionary[\"user_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "            hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n",
    "            hyperparameters_range_dictionary[\"negative_interactions_quota\"] = Real(low=0.0, high=0.5, prior='uniform')\n",
    "        \n",
    "            recommender_input_args = SearchInputRecommenderArgs(\n",
    "                CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "                CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "                FIT_POSITIONAL_ARGS=[],\n",
    "                FIT_KEYWORD_ARGS=earlystopping_keywargs\n",
    "            )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        # if recommender_class is MatrixFactorization_AsySVD_Cython:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"sgd_mode\"] = Categoricl([\"sgd\", \"adagrad\", \"adam\"])\n",
    "        #     hyperparameters_range_dictionary[\"epochs\"] = Categorical([500])\n",
    "        #     hyperparameters_range_dictionary[\"use_bias\"] = Categorical([True, False])\n",
    "        #     hyperparameters_range_dictionary[\"batch_size\"] = Categorical([1])\n",
    "        #     hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n",
    "        #     hyperparameters_range_dictionary[\"item_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"user_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"negative_interactions_quota\"] = Real(low=0.0, high=0.5, prior='uniform')\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS=earlystopping_keywargs\n",
    "        #     )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        if recommender_class is MatrixFactorization_BPR_Cython:\n",
    "            hyperparameters_range_dictionary = {}\n",
    "            hyperparameters_range_dictionary[\"sgd_mode\"] = Categorical([\"sgd\", \"adagrad\", \"adam\"])\n",
    "            hyperparameters_range_dictionary[\"epochs\"] = Categorical([1000])  # 1500\n",
    "            hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n",
    "            hyperparameters_range_dictionary[\"batch_size\"] = Categorical([1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024])\n",
    "            hyperparameters_range_dictionary[\"positive_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "            hyperparameters_range_dictionary[\"negative_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "            hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n",
    "        \n",
    "            recommender_input_args = SearchInputRecommenderArgs(\n",
    "                CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "                CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "                FIT_POSITIONAL_ARGS=[],\n",
    "                FIT_KEYWORD_ARGS={**earlystopping_keywargs,\n",
    "                                  \"positive_threshold_BPR\": None}\n",
    "            )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        # if recommender_class is IALSRecommender:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n",
    "        #     hyperparameters_range_dictionary[\"confidence_scaling\"] = Categorical([\"linear\", \"log\"])\n",
    "        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=1e-3, high=50.0, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"epsilon\"] = Real(low=1e-3, high=10.0, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS=earlystopping_keywargs\n",
    "        #     )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "#         if recommender_class is PureSVDRecommender:\n",
    "#             hyperparameters_range_dictionary = {}\n",
    "#             hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 350)\n",
    "\n",
    "#             recommender_input_args = SearchInputRecommenderArgs(\n",
    "#                 CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "#                 CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "#                 FIT_POSITIONAL_ARGS=[],\n",
    "#                 FIT_KEYWORD_ARGS={}\n",
    "#             )\n",
    "\n",
    "        ##########################################################################################################\n",
    "\n",
    "        # if recommender_class is NMFRecommender:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 350)\n",
    "        #     hyperparameters_range_dictionary[\"solver\"] = Categorical([\"coordinate_descent\", \"multiplicative_update\"])\n",
    "        #     hyperparameters_range_dictionary[\"init_type\"] = Categorical([\"random\", \"nndsvda\"])\n",
    "        #     hyperparameters_range_dictionary[\"beta_loss\"] = Categorical([\"frobenius\", \"kullback-leibler\"])\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={}\n",
    "        #     )\n",
    "\n",
    "        #########################################################################################################\n",
    "\n",
    "        # if recommender_class is SLIM_BPR_Cython:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n",
    "        #     hyperparameters_range_dictionary[\"epochs\"] = Categorical([1500])\n",
    "        #     hyperparameters_range_dictionary[\"symmetric\"] = Categorical([True, False])\n",
    "        #     hyperparameters_range_dictionary[\"sgd_mode\"] = Categorical([\"sgd\", \"adagrad\", \"adam\"])\n",
    "        #     hyperparameters_range_dictionary[\"lambda_i\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"lambda_j\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={**earlystopping_keywargs,\n",
    "        #                           \"positive_threshold_BPR\": None,\n",
    "        #                           'train_with_sparse_weights': None}\n",
    "        #     )\n",
    "\n",
    "        ##########################################################################################################\n",
    "        #\n",
    "        # if recommender_class is SLIMElasticNetRecommender:\n",
    "        #     hyperparameters_range_dictionary = {}\n",
    "        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n",
    "        #     hyperparameters_range_dictionary[\"l1_ratio\"] = Real(low=1e-5, high=1.0, prior='log-uniform')\n",
    "        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=1e-3, high=1.0, prior='uniform')\n",
    "        #\n",
    "        #     recommender_input_args = SearchInputRecommenderArgs(\n",
    "        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n",
    "        #         CONSTRUCTOR_KEYWORD_ARGS={},\n",
    "        #         FIT_POSITIONAL_ARGS=[],\n",
    "        #         FIT_KEYWORD_ARGS={}\n",
    "        #     )\n",
    "\n",
    "        #########################################################################################################\n",
    "\n",
    "        if URM_train_last_test is not None:\n",
    "            recommender_input_args_last_test = recommender_input_args.copy()\n",
    "            recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n",
    "        else:\n",
    "            recommender_input_args_last_test = None\n",
    "\n",
    "        ## Final step, after the hyperparameter range has been defined for each type of algorithm\n",
    "        parameterSearch.search(recommender_input_args,\n",
    "                               parameter_search_space=hyperparameters_range_dictionary,\n",
    "                               n_cases=n_cases,\n",
    "                               n_random_starts=n_random_starts,\n",
    "                               resume_from_saved=resume_from_saved,\n",
    "                               save_model=save_model,\n",
    "                               output_folder_path=output_folder_path,\n",
    "                               output_file_name_root=output_file_name_root,\n",
    "                               metric_to_optimize=metric_to_optimize,\n",
    "                               recommender_input_args_last_test=recommender_input_args_last_test)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(\"On recommender {} Exception {}\".format(recommender_class, str(e)))\n",
    "        traceback.print_exc()\n",
    "\n",
    "        error_file = open(output_folder_path + \"ErrorLog.txt\", \"a\")\n",
    "        error_file.write(\"On recommender {} Exception {}\\n\".format(recommender_class, str(e)))\n",
    "        error_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Matrix Factorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes care of the compilation step\n",
    "# %load_ext Cython\n",
    "\n",
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "# BaseRecommender.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from utils.compute_similarity import check_matrix\n",
    "\n",
    "class BaseRecommender(object):\n",
    "    \"\"\"Abstract BaseRecommender\"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"Recommender_Base_Class\"\n",
    "\n",
    "    def __init__(self, URM_train, verbose=True):\n",
    "\n",
    "        super(BaseRecommender, self).__init__()\n",
    "\n",
    "        self.URM_train = check_matrix(URM_train.copy(), 'csr', dtype=np.float32)\n",
    "        self.URM_train.eliminate_zeros()\n",
    "\n",
    "        self.n_users, self.n_items = self.URM_train.shape\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.filterTopPop = False\n",
    "        self.filterTopPop_ItemsID = np.array([], dtype=np.int)\n",
    "\n",
    "        self.items_to_ignore_flag = False\n",
    "        self.items_to_ignore_ID = np.array([], dtype=np.int)\n",
    "\n",
    "        self._cold_user_mask = np.ediff1d(self.URM_train.indptr) == 0\n",
    "\n",
    "        if self._cold_user_mask.any():\n",
    "            self._print(\"URM Detected {} ({:.2f} %) cold users.\".format(\n",
    "                self._cold_user_mask.sum(), self._cold_user_mask.sum()/self.n_users*100))\n",
    "\n",
    "\n",
    "        self._cold_item_mask = np.ediff1d(self.URM_train.tocsc().indptr) == 0\n",
    "\n",
    "        if self._cold_item_mask.any():\n",
    "            self._print(\"URM Detected {} ({:.2f} %) cold items.\".format(\n",
    "                self._cold_item_mask.sum(), self._cold_item_mask.sum()/self.n_items*100))\n",
    "\n",
    "\n",
    "    def _get_cold_user_mask(self):\n",
    "        return self._cold_user_mask\n",
    "\n",
    "    def _get_cold_item_mask(self):\n",
    "        return self._cold_item_mask\n",
    "\n",
    "\n",
    "    def _print(self, string):\n",
    "        if self.verbose:\n",
    "            print(\"{}: {}\".format(self.RECOMMENDER_NAME, string))\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def get_URM_train(self):\n",
    "        return self.URM_train.copy()\n",
    "\n",
    "    def set_items_to_ignore(self, items_to_ignore):\n",
    "        self.items_to_ignore_flag = True\n",
    "        self.items_to_ignore_ID = np.array(items_to_ignore, dtype=np.int)\n",
    "\n",
    "    def reset_items_to_ignore(self):\n",
    "        self.items_to_ignore_flag = False\n",
    "        self.items_to_ignore_ID = np.array([], dtype=np.int)\n",
    "\n",
    "\n",
    "    #########################################################################################################\n",
    "    ##########                                                                                     ##########\n",
    "    ##########                     COMPUTE AND FILTER RECOMMENDATION LIST                          ##########\n",
    "    ##########                                                                                     ##########\n",
    "    #########################################################################################################\n",
    "\n",
    "\n",
    "    def _remove_TopPop_on_scores(self, scores_batch):\n",
    "        scores_batch[:, self.filterTopPop_ItemsID] = -np.inf\n",
    "        return scores_batch\n",
    "\n",
    "\n",
    "    def _remove_custom_items_on_scores(self, scores_batch):\n",
    "        scores_batch[:, self.items_to_ignore_ID] = -np.inf\n",
    "        return scores_batch\n",
    "\n",
    "\n",
    "    def _remove_seen_on_scores(self, user_id, scores):\n",
    "\n",
    "        assert self.URM_train.getformat() == \"csr\", \"Recommender_Base_Class: URM_train is not CSR, this will cause errors in filtering seen items\"\n",
    "\n",
    "        seen = self.URM_train.indices[self.URM_train.indptr[user_id]:self.URM_train.indptr[user_id + 1]]\n",
    "\n",
    "        scores[seen] = -np.inf\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def _compute_item_score(self, user_id_array, items_to_compute = None):\n",
    "        \"\"\"\n",
    "        :param user_id_array:       array containing the user indices whose recommendations need to be computed\n",
    "        :param items_to_compute:    array containing the items whose scores are to be computed.\n",
    "                                        If None, all items are computed, otherwise discarded items will have as score -np.inf\n",
    "        :return:                    array (len(user_id_array), n_items) with the score.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"BaseRecommender: compute_item_score not assigned for current recommender, unable to compute prediction scores\")\n",
    "\n",
    "\n",
    "    def recommend(self, user_id_array, cutoff = None, remove_seen_flag=True, items_to_compute = None,\n",
    "                  remove_top_pop_flag = False, remove_custom_items_flag = False, return_scores = False):\n",
    "\n",
    "        # If is a scalar transform it in a 1-cell array\n",
    "        if np.isscalar(user_id_array):\n",
    "            user_id_array = np.atleast_1d(user_id_array)\n",
    "            single_user = True\n",
    "        else:\n",
    "            single_user = False\n",
    "\n",
    "        if cutoff is None:\n",
    "            cutoff = self.URM_train.shape[1] - 1\n",
    "\n",
    "        # Compute the scores using the model-specific function\n",
    "        # Vectorize over all users in user_id_array\n",
    "        scores_batch = self._compute_item_score(user_id_array, items_to_compute=items_to_compute)\n",
    "\n",
    "\n",
    "        for user_index in range(len(user_id_array)):\n",
    "\n",
    "            user_id = user_id_array[user_index]\n",
    "\n",
    "            if remove_seen_flag:\n",
    "                scores_batch[user_index,:] = self._remove_seen_on_scores(user_id, scores_batch[user_index, :])\n",
    "\n",
    "            # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "            # - Partition the data to extract the set of relevant items\n",
    "            # - Sort only the relevant items\n",
    "            # - Get the original item index\n",
    "            # relevant_items_partition = (-scores_user).argpartition(cutoff)[0:cutoff]\n",
    "            # relevant_items_partition_sorting = np.argsort(-scores_user[relevant_items_partition])\n",
    "            # ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "            #\n",
    "            # ranking_list.append(ranking)\n",
    "\n",
    "\n",
    "        if remove_top_pop_flag:\n",
    "            scores_batch = self._remove_TopPop_on_scores(scores_batch)\n",
    "\n",
    "        if remove_custom_items_flag:\n",
    "            scores_batch = self._remove_custom_items_on_scores(scores_batch)\n",
    "\n",
    "        # relevant_items_partition is block_size x cutoff\n",
    "        relevant_items_partition = (-scores_batch).argpartition(cutoff, axis=1)[:,0:cutoff]\n",
    "\n",
    "        # Get original value and sort it\n",
    "        # [:, None] adds 1 dimension to the array, from (block_size,) to (block_size,1)\n",
    "        # This is done to correctly get scores_batch value as [row, relevant_items_partition[row,:]]\n",
    "        relevant_items_partition_original_value = scores_batch[np.arange(scores_batch.shape[0])[:, None], relevant_items_partition]\n",
    "        relevant_items_partition_sorting = np.argsort(-relevant_items_partition_original_value, axis=1)\n",
    "        ranking = relevant_items_partition[np.arange(relevant_items_partition.shape[0])[:, None], relevant_items_partition_sorting]\n",
    "\n",
    "        ranking_list = [None] * ranking.shape[0]\n",
    "\n",
    "        # Remove from the recommendation list any item that has a -inf score\n",
    "        # Since -inf is a flag to indicate an item to remove\n",
    "        for user_index in range(len(user_id_array)):\n",
    "            user_recommendation_list = ranking[user_index]\n",
    "            user_item_scores = scores_batch[user_index, user_recommendation_list]\n",
    "\n",
    "            not_inf_scores_mask = np.logical_not(np.isinf(user_item_scores))\n",
    "\n",
    "            user_recommendation_list = user_recommendation_list[not_inf_scores_mask]\n",
    "            ranking_list[user_index] = user_recommendation_list.tolist()\n",
    "\n",
    "        # Return single list for one user, instead of list of lists\n",
    "        if single_user:\n",
    "            ranking_list = ranking_list[0]\n",
    "\n",
    "\n",
    "        if return_scores:\n",
    "            return ranking_list, scores_batch\n",
    "\n",
    "        else:\n",
    "            return ranking_list\n",
    "\n",
    "\n",
    "\n",
    "# BaseMatrixFactorizationRecommender.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 16/09/2017\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "# from recommenders.BaseRecommender import BaseRecommender\n",
    "# from KNN.ItemKNNCustomSimilarityRecommender import ItemKNNCustomSimilarityRecommender\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BaseMatrixFactorizationRecommender(BaseRecommender):\n",
    "    \"\"\"\n",
    "    This class refers to a BaseRecommender KNN which uses matrix factorization,\n",
    "    it provides functions to compute item's score as well as a function to save the W_matrix\n",
    "    The prediction for cold users will always be -inf for ALL items\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, URM_train, verbose=True):\n",
    "        super(BaseMatrixFactorizationRecommender, self).__init__(URM_train, verbose=verbose)\n",
    "\n",
    "        self.use_bias = False\n",
    "\n",
    "    #########################################################################################################\n",
    "    ##########                                                                                     ##########\n",
    "    ##########                               COMPUTE ITEM SCORES                                   ##########\n",
    "    ##########                                                                                     ##########\n",
    "    #########################################################################################################\n",
    "\n",
    "\n",
    "    def _compute_item_score(self, user_id_array, items_to_compute = None):\n",
    "        \"\"\"\n",
    "        USER_factors is n_users x n_factors\n",
    "        ITEM_factors is n_items x n_factors\n",
    "        The prediction for cold users will always be -inf for ALL items\n",
    "        :param user_id_array:\n",
    "        :param items_to_compute:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.USER_factors.shape[1] == self.ITEM_factors.shape[1], \\\n",
    "            \"{}: User and Item factors have inconsistent shape\".format(self.RECOMMENDER_NAME)\n",
    "\n",
    "        assert self.USER_factors.shape[0] > np.max(user_id_array),\\\n",
    "                \"{}: Cold users not allowed. Users in trained model are {}, requested prediction for users up to {}\".format(\n",
    "                self.RECOMMENDER_NAME, self.USER_factors.shape[0], np.max(user_id_array))\n",
    "\n",
    "        if items_to_compute is not None:\n",
    "            item_scores = - np.ones((len(user_id_array), self.ITEM_factors.shape[0]), dtype=np.float32)*np.inf\n",
    "            item_scores[:, items_to_compute] = np.dot(self.USER_factors[user_id_array], self.ITEM_factors[items_to_compute,:].T)\n",
    "\n",
    "        else:\n",
    "            item_scores = np.dot(self.USER_factors[user_id_array], self.ITEM_factors.T)\n",
    "\n",
    "\n",
    "        # No need to select only the specific negative items or warm users because the -inf score will not change\n",
    "        if self.use_bias:\n",
    "            item_scores += self.ITEM_bias + self.GLOBAL_bias\n",
    "            item_scores = (item_scores.T + self.USER_bias[user_id_array]).T\n",
    "\n",
    "        return item_scores\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# MatrixFactorization_Cython_Epoch.py\n",
    "# ------------------------------------------------------------------\n",
    "\"\"\"\n",
    "Created on 07/09/17\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "#cython: language_level=3\n",
    "#cython: boundscheck=False\n",
    "#cython: wraparound=False\n",
    "#cython: initializedcheck=False\n",
    "#cython: nonecheck=False\n",
    "#cython: cdivision=True\n",
    "#cython: unpack_method_calls=True\n",
    "#cython: overflowcheck=False\n",
    "\n",
    "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n",
    "\n",
    "# from Base.Recommender_utils import check_matrix\n",
    "\n",
    "import cython\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import numpy as np\n",
    "# cimport numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from libc.math cimport exp, sqrt\n",
    "from libc.stdlib cimport rand, srand, RAND_MAX\n",
    "\n",
    "cdef struct BPR_sample:\n",
    "    long user\n",
    "    long pos_item\n",
    "    long neg_item\n",
    "\n",
    "\n",
    "cdef struct MSE_sample:\n",
    "    long user\n",
    "    long item\n",
    "    double rating\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.initializedcheck(False)\n",
    "@cython.nonecheck(False)\n",
    "@cython.cdivision(True)\n",
    "@cython.overflowcheck(False)\n",
    "cdef class MatrixFactorization_Cython_Epoch:\n",
    "\n",
    "    cdef int n_users, n_items, n_factors\n",
    "    cdef algorithm_name\n",
    "\n",
    "    cdef double learning_rate, user_reg, item_reg, positive_reg, negative_reg, bias_reg\n",
    "    cdef double init_mean, init_std_dev, MSE_negative_interactions_quota, MSE_sample_negative_interactions_flag\n",
    "\n",
    "    cdef int batch_size\n",
    "\n",
    "    cdef int algorithm_is_funk_svd, algorithm_is_asy_svd, algorithm_is_BPR\n",
    "\n",
    "    cdef int[:] URM_train_indices, URM_train_indptr, profile_length\n",
    "    cdef double[:] URM_train_data\n",
    "\n",
    "    cdef double[:,:] USER_factors, ITEM_factors\n",
    "    cdef double[:] USER_bias, ITEM_bias, GLOBAL_bias\n",
    "\n",
    "\n",
    "    # Mini-batch sample data\n",
    "    cdef double[:,:] USER_factors_minibatch_accumulator, ITEM_factors_minibatch_accumulator\n",
    "    cdef double[:] USER_bias_minibatch_accumulator, ITEM_bias_minibatch_accumulator, GLOBAL_bias_minibatch_accumulator\n",
    "\n",
    "    cdef long[:] mini_batch_sampled_items, mini_batch_sampled_users\n",
    "    cdef long[:] mini_batch_sampled_items_flag, mini_batch_sampled_users_flag\n",
    "    cdef long mini_batch_sampled_items_counter, mini_batch_sampled_users_counter\n",
    "\n",
    "    # Adaptive gradient\n",
    "    cdef int useAdaGrad, useRmsprop, useAdam, verbose, use_bias\n",
    "\n",
    "    cdef double [:,:] sgd_cache_I, sgd_cache_U, sgd_cache_bias_I, sgd_cache_bias_U, sgd_cache_bias_GLOBAL\n",
    "    cdef double gamma\n",
    "\n",
    "    cdef double [:,:] sgd_cache_I_momentum_1, sgd_cache_I_momentum_2\n",
    "    cdef double [:,:] sgd_cache_U_momentum_1, sgd_cache_U_momentum_2\n",
    "    cdef double [:,:] sgd_cache_bias_I_momentum_1, sgd_cache_bias_I_momentum_2\n",
    "    cdef double [:,:] sgd_cache_bias_U_momentum_1, sgd_cache_bias_U_momentum_2\n",
    "    cdef double [:,:] sgd_cache_bias_GLOBAL_momentum_1, sgd_cache_bias_GLOBAL_momentum_2\n",
    "    cdef double beta_1, beta_2, beta_1_power_t, beta_2_power_t\n",
    "    cdef double momentum_1, momentum_2\n",
    "\n",
    "    SGD_MODE_VALUES = [\"sgd\", \"adam\", \"adagrad\", \"rmsprop\"]\n",
    "    ALGORITHM_NAME_VALUES = [\"FUNK_SVD\", \"ASY_SVD\", \"MF_BPR\"]\n",
    "\n",
    "\n",
    "    def __init__(self, URM_train, n_factors = 1, algorithm_name = None,\n",
    "                 batch_size = 1,\n",
    "                 negative_interactions_quota = 0.5,\n",
    "                 learning_rate = 1e-3, use_bias = False,\n",
    "                 user_reg = 0.0, item_reg = 0.0, bias_reg = 0.0, positive_reg = 0.0, negative_reg = 0.0,\n",
    "                 verbose = False, random_seed = None,\n",
    "                 init_mean = 0.0, init_std_dev = 0.1,\n",
    "                 sgd_mode='sgd', gamma=0.995, beta_1=0.9, beta_2=0.999):\n",
    "\n",
    "        super(MatrixFactorization_Cython_Epoch, self).__init__()\n",
    "\n",
    "\n",
    "        if sgd_mode not in self.SGD_MODE_VALUES:\n",
    "           raise ValueError(\"Value for 'sgd_mode' not recognized. Acceptable values are {}, provided was '{}'\".format(self.SGD_MODE_VALUES, sgd_mode))\n",
    "\n",
    "        if algorithm_name not in self.ALGORITHM_NAME_VALUES:\n",
    "           raise ValueError(\"Value for 'algorithm_name' not recognized. Acceptable values are {}, provided was '{}'\".format(self.ALGORITHM_NAME_VALUES, algorithm_name))\n",
    "\n",
    "        # Create copy of URM_train in csr format\n",
    "        # make sure indices are sorted\n",
    "#         URM_train = check_matrix(URM_train, 'csr')\n",
    "        URM_train = URM_train.sorted_indices()\n",
    "\n",
    "        self.profile_length = np.ediff1d(URM_train.indptr)\n",
    "        self.n_users, self.n_items = URM_train.shape\n",
    "\n",
    "\n",
    "        self.n_factors = n_factors\n",
    "        self.verbose = verbose\n",
    "        self.algorithm_name = algorithm_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.user_reg = user_reg\n",
    "        self.item_reg = item_reg\n",
    "        self.positive_reg = positive_reg\n",
    "        self.negative_reg = negative_reg\n",
    "        self.bias_reg = bias_reg\n",
    "        self.use_bias = use_bias\n",
    "        self.batch_size = batch_size\n",
    "        self.init_mean = init_mean\n",
    "        self.init_std_dev = init_std_dev\n",
    "        self.MSE_negative_interactions_quota = negative_interactions_quota\n",
    "        self.MSE_sample_negative_interactions_flag = self.MSE_negative_interactions_quota != 0.0\n",
    "\n",
    "        self.URM_train_indices = URM_train.indices\n",
    "        self.URM_train_data = np.array(URM_train.data, dtype=np.float64)\n",
    "        self.URM_train_indptr = URM_train.indptr\n",
    "\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(seed=random_seed)\n",
    "            srand(<unsigned int> int(random_seed))\n",
    "\n",
    "        self._init_latent_factors()\n",
    "        self._init_minibatch_data_structures()\n",
    "        self._init_adaptive_gradient_cache(sgd_mode, gamma, beta_1, beta_2)\n",
    "\n",
    "    def _init_latent_factors(self):\n",
    "\n",
    "        self.algorithm_is_funk_svd = False\n",
    "        self.algorithm_is_asy_svd = False\n",
    "        self.algorithm_is_BPR = False\n",
    "\n",
    "        n_user_factors = self.n_users\n",
    "        n_item_factors = self.n_items\n",
    "\n",
    "        if self.algorithm_name == \"FUNK_SVD\":\n",
    "            self.algorithm_is_funk_svd = True\n",
    "\n",
    "        elif self.algorithm_name == \"ASY_SVD\":\n",
    "            self.algorithm_is_asy_svd = True\n",
    "            n_user_factors = self.n_items\n",
    "            n_item_factors = self.n_items\n",
    "\n",
    "        elif self.algorithm_name == \"MF_BPR\":\n",
    "            self.algorithm_is_BPR = True\n",
    "\n",
    "        # W and H cannot be initialized as zero, otherwise the gradient will always be zero\n",
    "        self.USER_factors = np.random.normal(self.init_mean, self.init_std_dev, (n_user_factors, self.n_factors)).astype(np.float64)\n",
    "        self.ITEM_factors = np.random.normal(self.init_mean, self.init_std_dev, (n_item_factors, self.n_factors)).astype(np.float64)\n",
    "\n",
    "        self.USER_factors_minibatch_accumulator = np.zeros((n_user_factors, self.n_factors), dtype=np.float64)\n",
    "        self.ITEM_factors_minibatch_accumulator = np.zeros((n_item_factors, self.n_factors), dtype=np.float64)\n",
    "\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.USER_bias = np.zeros(self.n_users, dtype=np.float64)\n",
    "            self.ITEM_bias = np.zeros(self.n_items, dtype=np.float64)\n",
    "            self.GLOBAL_bias = np.zeros(1, dtype=np.float64)\n",
    "\n",
    "            self.USER_bias_minibatch_accumulator = np.zeros(self.n_users, dtype=np.float64)\n",
    "            self.ITEM_bias_minibatch_accumulator = np.zeros(self.n_items, dtype=np.float64)\n",
    "            self.GLOBAL_bias_minibatch_accumulator = np.zeros(1, dtype=np.float64)\n",
    "\n",
    "    def _init_adaptive_gradient_cache(self, sgd_mode, gamma, beta_1, beta_2):\n",
    "\n",
    "        self.useAdaGrad = False\n",
    "        self.useRmsprop = False\n",
    "        self.useAdam = False\n",
    "\n",
    "        if sgd_mode=='adagrad':\n",
    "            self.useAdaGrad = True\n",
    "\n",
    "        elif sgd_mode=='rmsprop':\n",
    "            self.useRmsprop = True\n",
    "\n",
    "            # Gamma default value suggested by Hinton\n",
    "            # self.gamma = 0.9\n",
    "            self.gamma = gamma\n",
    "\n",
    "        elif sgd_mode=='adam':\n",
    "            self.useAdam = True\n",
    "\n",
    "            # Default value suggested by the original paper\n",
    "            # beta_1=0.9, beta_2=0.999\n",
    "            self.beta_1 = beta_1\n",
    "            self.beta_2 = beta_2\n",
    "            self.beta_1_power_t = beta_1\n",
    "            self.beta_2_power_t = beta_2\n",
    "\n",
    "\n",
    "        if sgd_mode=='sgd':\n",
    "            self.sgd_cache_I = None\n",
    "            self.sgd_cache_U = None\n",
    "\n",
    "            self.sgd_cache_bias_I = None\n",
    "            self.sgd_cache_bias_U = None\n",
    "            self.sgd_cache_bias_GLOBAL = None\n",
    "\n",
    "            self.sgd_cache_I_momentum_1 = None\n",
    "            self.sgd_cache_I_momentum_2 = None\n",
    "\n",
    "            self.sgd_cache_U_momentum_1 = None\n",
    "            self.sgd_cache_U_momentum_2 = None\n",
    "\n",
    "            self.sgd_cache_bias_I_momentum_1 = None\n",
    "            self.sgd_cache_bias_I_momentum_2 = None\n",
    "\n",
    "            self.sgd_cache_bias_U_momentum_1 = None\n",
    "            self.sgd_cache_bias_U_momentum_2 = None\n",
    "\n",
    "            self.sgd_cache_bias_GLOBAL_momentum_1 = None\n",
    "            self.sgd_cache_bias_GLOBAL_momentum_2 = None\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Adagrad and RMSProp\n",
    "            self.sgd_cache_I = np.zeros((self.ITEM_factors.shape[0], self.n_factors), dtype=np.float64)\n",
    "            self.sgd_cache_U = np.zeros((self.USER_factors.shape[0], self.n_factors), dtype=np.float64)\n",
    "\n",
    "            self.sgd_cache_bias_I = np.zeros((self.n_items, 1), dtype=np.float64)\n",
    "            self.sgd_cache_bias_U = np.zeros((self.n_users, 1), dtype=np.float64)\n",
    "            self.sgd_cache_bias_GLOBAL = np.zeros((1, 1), dtype=np.float64)\n",
    "\n",
    "            # Adam\n",
    "            self.sgd_cache_I_momentum_1 = np.zeros((self.ITEM_factors.shape[0], self.n_factors), dtype=np.float64)\n",
    "            self.sgd_cache_I_momentum_2 = np.zeros((self.ITEM_factors.shape[0], self.n_factors), dtype=np.float64)\n",
    "\n",
    "            self.sgd_cache_U_momentum_1 = np.zeros((self.USER_factors.shape[0], self.n_factors), dtype=np.float64)\n",
    "            self.sgd_cache_U_momentum_2 = np.zeros((self.USER_factors.shape[0], self.n_factors), dtype=np.float64)\n",
    "\n",
    "            self.sgd_cache_bias_I_momentum_1 = np.zeros((self.n_items, 1), dtype=np.float64)\n",
    "            self.sgd_cache_bias_I_momentum_2 = np.zeros((self.n_items, 1), dtype=np.float64)\n",
    "\n",
    "            self.sgd_cache_bias_U_momentum_1 = np.zeros((self.n_users, 1), dtype=np.float64)\n",
    "            self.sgd_cache_bias_U_momentum_2 = np.zeros((self.n_users, 1), dtype=np.float64)\n",
    "\n",
    "            self.sgd_cache_bias_GLOBAL_momentum_1 = np.zeros((1, 1), dtype=np.float64)\n",
    "            self.sgd_cache_bias_GLOBAL_momentum_2 = np.zeros((1, 1), dtype=np.float64)\n",
    "\n",
    "    def epochIteration_Cython(self):\n",
    "\n",
    "        if self.algorithm_is_funk_svd:\n",
    "            self.epochIteration_Cython_FUNK_SVD_SGD()\n",
    "\n",
    "        elif self.algorithm_is_asy_svd:\n",
    "            self.epochIteration_Cython_ASY_SVD_SGD()\n",
    "\n",
    "        elif self.algorithm_is_BPR:\n",
    "            self.epochIteration_Cython_BPR_SGD()\n",
    "\n",
    "    def epochIteration_Cython_FUNK_SVD_SGD(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        cdef long num_total_batch = int(len(self.URM_train_data) / self.batch_size) + 1\n",
    "\n",
    "        cdef MSE_sample sample\n",
    "        cdef long factor_index, num_current_batch, num_sample_in_batch, processed_samples_last_print, print_block_size = 500\n",
    "        cdef double prediction, prediction_error\n",
    "        cdef double local_gradient_item, local_gradient_user, local_gradient_bias_item, local_gradient_bias_user, local_gradient_bias_global\n",
    "\n",
    "        cdef double H_i, W_u, cumulative_loss = 0.0\n",
    "\n",
    "        cdef long start_time_epoch = time.time()\n",
    "        cdef long last_print_time = start_time_epoch\n",
    "\n",
    "        for num_current_batch in range(num_total_batch):\n",
    "\n",
    "            self._clear_minibatch_data_structures()\n",
    "\n",
    "            # Iterate over samples in batch\n",
    "            for num_sample_in_batch in range(self.batch_size):\n",
    "\n",
    "                # Uniform user sampling with replacement\n",
    "                sample = self.sampleMSE_Cython()\n",
    "\n",
    "                self._add_MSE_sample_in_minibatch(sample)\n",
    "\n",
    "                # Compute prediction\n",
    "                if self.use_bias:\n",
    "                    prediction = self.GLOBAL_bias[0] + self.USER_bias[sample.user] + self.ITEM_bias[sample.item]\n",
    "                else:\n",
    "                    prediction = 0.0\n",
    "\n",
    "                for factor_index in range(self.n_factors):\n",
    "                    prediction += self.USER_factors[sample.user, factor_index] * self.ITEM_factors[sample.item, factor_index]\n",
    "\n",
    "\n",
    "                # Compute gradients\n",
    "                prediction_error = sample.rating - prediction\n",
    "                cumulative_loss += prediction_error**2\n",
    "\n",
    "\n",
    "                if self.use_bias:\n",
    "                    local_gradient_bias_global = prediction_error - self.bias_reg * self.GLOBAL_bias[0]\n",
    "                    local_gradient_bias_item = prediction_error - self.bias_reg * self.ITEM_bias[sample.item]\n",
    "                    local_gradient_bias_user = prediction_error - self.bias_reg * self.USER_bias[sample.user]\n",
    "\n",
    "                    self.GLOBAL_bias_minibatch_accumulator[0] += local_gradient_bias_global\n",
    "                    self.ITEM_bias_minibatch_accumulator[sample.item] += local_gradient_bias_item\n",
    "                    self.USER_bias_minibatch_accumulator[sample.user] += local_gradient_bias_user\n",
    "\n",
    "\n",
    "                for factor_index in range(self.n_factors):\n",
    "\n",
    "                    # Copy original value to avoid messing up the updates\n",
    "                    H_i = self.ITEM_factors[sample.item, factor_index]\n",
    "                    W_u = self.USER_factors[sample.user, factor_index]\n",
    "\n",
    "                    # Compute gradients\n",
    "                    local_gradient_item = prediction_error * W_u - self.positive_reg * H_i\n",
    "                    local_gradient_user = prediction_error * H_i - self.user_reg * W_u\n",
    "\n",
    "                    # Store the gradient in the temporary accumulator\n",
    "                    self.ITEM_factors_minibatch_accumulator[sample.item, factor_index] += local_gradient_item\n",
    "                    self.USER_factors_minibatch_accumulator[sample.user, factor_index] += local_gradient_user\n",
    "\n",
    "\n",
    "            self._apply_minibatch_updates_to_latent_factors()\n",
    "\n",
    "\n",
    "            # Exponentiation of beta at the end of each mini batch\n",
    "            if self.useAdam:\n",
    "\n",
    "                self.beta_1_power_t *= self.beta_1\n",
    "                self.beta_2_power_t *= self.beta_2\n",
    "\n",
    "\n",
    "            if self.verbose and (processed_samples_last_print >= print_block_size or num_current_batch == num_total_batch-1):\n",
    "\n",
    "                current_time = time.time()\n",
    "\n",
    "                # Set block size to the number of items necessary in order to print every 30 seconds\n",
    "                samples_per_sec = num_current_batch/(time.time() - start_time_epoch)\n",
    "\n",
    "                print_block_size = int(samples_per_sec*30)\n",
    "\n",
    "                if current_time - last_print_time > 30 or num_current_batch == num_total_batch-1:\n",
    "\n",
    "                    print(\"{}: Processed {} ( {:.2f}% ) in {:.2f} seconds. MSE loss {:.2E}. Sample per second: {:.0f}\".format(\n",
    "                        self.algorithm_name,\n",
    "                        num_current_batch*self.batch_size,\n",
    "                        100.0* num_current_batch/num_total_batch,\n",
    "                        time.time() - last_print_time,\n",
    "                        cumulative_loss/(num_current_batch*self.batch_size + 1),\n",
    "                        float(num_current_batch*self.batch_size + 1) / (time.time() - start_time_epoch)))\n",
    "\n",
    "                    last_print_time = current_time\n",
    "                    processed_samples_last_print = 0\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "                    sys.stderr.flush()\n",
    "\n",
    "\n",
    "    def epochIteration_Cython_ASY_SVD_SGD(self):\n",
    "\n",
    "        assert self.batch_size == 1, \"Batch size other than 1 not supported for ASY_SVD\"\n",
    "\n",
    "        # Get number of available interactions\n",
    "        cdef long num_total_batch = int(len(self.URM_train_data) / self.batch_size) + 1\n",
    "\n",
    "        cdef MSE_sample sample\n",
    "        cdef long num_current_batch, num_sample_in_batch, processed_samples_last_print, print_block_size = 500\n",
    "        cdef double prediction, prediction_error\n",
    "        cdef double local_gradient_item, local_gradient_user, local_gradient_bias_item, local_gradient_bias_user, local_gradient_bias_global\n",
    "\n",
    "        cdef double[:] user_factors_accumulated = np.zeros(self.n_factors, dtype=np.float64)\n",
    "        cdef long start_pos_seen_items, end_pos_seen_items, item_id, factor_index, item_index, user_index\n",
    "\n",
    "        cdef double H_i, W_u, cumulative_loss = 0.0, denominator\n",
    "\n",
    "\n",
    "        cdef long start_time_epoch = time.time()\n",
    "        cdef long last_print_time = start_time_epoch\n",
    "\n",
    "\n",
    "        for num_current_batch in range(num_total_batch):\n",
    "\n",
    "\n",
    "            prediction_error = 0.0\n",
    "\n",
    "            # Iterate over samples in batch\n",
    "            for num_sample_in_batch in range(self.batch_size):\n",
    "\n",
    "                # Uniform user sampling with replacement\n",
    "                sample = self.sampleMSE_Cython()\n",
    "\n",
    "                self.mini_batch_sampled_items[num_sample_in_batch] = sample.item\n",
    "                self.mini_batch_sampled_users[num_sample_in_batch] = sample.user\n",
    "\n",
    "\n",
    "                for factor_index in range(self.n_factors):\n",
    "                    user_factors_accumulated[factor_index] = 0.0\n",
    "\n",
    "\n",
    "                # Accumulate latent factors of rated items\n",
    "                start_pos_seen_items = self.URM_train_indptr[sample.user]\n",
    "                end_pos_seen_items = self.URM_train_indptr[sample.user+1]\n",
    "\n",
    "                for item_index in range(start_pos_seen_items, end_pos_seen_items):\n",
    "                    item_id = self.URM_train_indices[item_index]\n",
    "\n",
    "                    for factor_index in range(self.n_factors):\n",
    "                        user_factors_accumulated[factor_index] += self.USER_factors[item_id, factor_index]\n",
    "\n",
    "\n",
    "                denominator = sqrt(self.profile_length[sample.user])\n",
    "\n",
    "\n",
    "                for factor_index in range(self.n_factors):\n",
    "                    user_factors_accumulated[factor_index] /= denominator\n",
    "\n",
    "                # Compute prediction\n",
    "                if self.use_bias:\n",
    "                    prediction = self.GLOBAL_bias[0] + self.USER_bias[sample.user] + self.ITEM_bias[sample.item]\n",
    "                else:\n",
    "                    prediction = 0.0\n",
    "\n",
    "                for factor_index in range(self.n_factors):\n",
    "                    prediction += user_factors_accumulated[factor_index] * self.ITEM_factors[sample.item, factor_index]\n",
    "\n",
    "                prediction_error += sample.rating - prediction\n",
    "\n",
    "            prediction_error /= self.batch_size\n",
    "            cumulative_loss += prediction_error**2\n",
    "\n",
    "            if self.use_bias:\n",
    "\n",
    "                # Compute gradients\n",
    "                local_gradient_bias_global = prediction_error - self.bias_reg * self.GLOBAL_bias[0]\n",
    "\n",
    "                # Compute adaptive gradients\n",
    "                local_gradient_bias_global = self.adaptive_gradient(local_gradient_bias_global, 0, 0, self.sgd_cache_bias_GLOBAL, self.sgd_cache_bias_GLOBAL_momentum_1, self.sgd_cache_bias_GLOBAL_momentum_2)\n",
    "\n",
    "                # Apply updates to bias and latent factors\n",
    "                self.GLOBAL_bias[0] += self.learning_rate * local_gradient_bias_global\n",
    "\n",
    "\n",
    "\n",
    "            # Iterate over samples in batch\n",
    "            for num_sample_in_batch in range(self.batch_size):\n",
    "\n",
    "                sample.item = self.mini_batch_sampled_items[num_sample_in_batch]\n",
    "                sample.user = self.mini_batch_sampled_users[num_sample_in_batch]\n",
    "\n",
    "                if self.use_bias:\n",
    "\n",
    "                    # Compute gradients\n",
    "                    local_gradient_bias_item = prediction_error - self.bias_reg * self.ITEM_bias[sample.item]\n",
    "                    local_gradient_bias_user = prediction_error - self.bias_reg * self.USER_bias[sample.user]\n",
    "\n",
    "                    # Compute adaptive gradients\n",
    "                    local_gradient_bias_item = self.adaptive_gradient(local_gradient_bias_item, sample.item, 0, self.sgd_cache_bias_I, self.sgd_cache_bias_I_momentum_1, self.sgd_cache_bias_I_momentum_2)\n",
    "                    local_gradient_bias_user = self.adaptive_gradient(local_gradient_bias_user, sample.user, 0, self.sgd_cache_bias_U, self.sgd_cache_bias_U_momentum_1, self.sgd_cache_bias_U_momentum_2)\n",
    "\n",
    "                    # Apply updates to bias\n",
    "                    self.ITEM_bias[sample.item] += self.learning_rate * local_gradient_bias_item\n",
    "                    self.USER_bias[sample.user] += self.learning_rate * local_gradient_bias_user\n",
    "\n",
    "\n",
    "                # Update USER factors, therefore all item factors for seen items\n",
    "                for item_index in range(start_pos_seen_items, end_pos_seen_items):\n",
    "                    item_id = self.URM_train_indices[item_index]\n",
    "\n",
    "                    for factor_index in range(self.n_factors):\n",
    "\n",
    "                        H_i = self.ITEM_factors[sample.item, factor_index]\n",
    "                        W_u = self.USER_factors[item_id, factor_index]\n",
    "\n",
    "                        # Compute gradients USER\n",
    "                        # Both matrices will have the size |I|x|F|\n",
    "                        local_gradient_user = prediction_error * H_i - self.user_reg * W_u\n",
    "\n",
    "                        # Compute adaptive gradients USER\n",
    "                        # I need to update NOT sample.item but item_id\n",
    "                        local_gradient_user = self.adaptive_gradient(local_gradient_user, item_id, factor_index, self.sgd_cache_U, self.sgd_cache_U_momentum_1, self.sgd_cache_U_momentum_2)\n",
    "\n",
    "                        # Apply update to latent factors\n",
    "                        self.USER_factors[item_id, factor_index] += self.learning_rate * local_gradient_user\n",
    "\n",
    "\n",
    "                # Update ITEM factors\n",
    "                for factor_index in range(self.n_factors):\n",
    "\n",
    "                    # Copy original value to avoid messing up the updates\n",
    "                    H_i = self.ITEM_factors[sample.item, factor_index]\n",
    "                    W_u = user_factors_accumulated[factor_index]\n",
    "\n",
    "                    # Compute gradients ITEM\n",
    "                    # Both matrices will have the size |I|x|F|\n",
    "                    local_gradient_item = prediction_error * W_u - self.item_reg * H_i\n",
    "\n",
    "                    # Compute adaptive gradients ITEM\n",
    "                    local_gradient_item = self.adaptive_gradient(local_gradient_item, sample.item, factor_index, self.sgd_cache_I, self.sgd_cache_I_momentum_1, self.sgd_cache_I_momentum_2)\n",
    "\n",
    "                    # Apply update to latent factors\n",
    "                    self.ITEM_factors[sample.item, factor_index] += self.learning_rate * local_gradient_item\n",
    "\n",
    "            # Exponentiation of beta at the end of each sample\n",
    "            if self.useAdam:\n",
    "\n",
    "                self.beta_1_power_t *= self.beta_1\n",
    "                self.beta_2_power_t *= self.beta_2\n",
    "\n",
    "\n",
    "            if self.verbose and (processed_samples_last_print >= print_block_size or num_current_batch == num_total_batch-1):\n",
    "\n",
    "                current_time = time.time()\n",
    "\n",
    "                # Set block size to the number of items necessary in order to print every 30 seconds\n",
    "                samples_per_sec = num_current_batch/(time.time() - start_time_epoch)\n",
    "\n",
    "                print_block_size = int(samples_per_sec*30)\n",
    "\n",
    "                if current_time - last_print_time > 30 or num_current_batch == num_total_batch-1:\n",
    "\n",
    "                    print(\"{}: Processed {} ( {:.2f}% ) in {:.2f} seconds. MSE loss {:.2E}. Sample per second: {:.0f}\".format(\n",
    "                        self.algorithm_name,\n",
    "                        num_current_batch*self.batch_size,\n",
    "                        100.0* num_current_batch/num_total_batch,\n",
    "                        time.time() - last_print_time,\n",
    "                        cumulative_loss/(num_current_batch*self.batch_size + 1),\n",
    "                        float(num_current_batch*self.batch_size + 1) / (time.time() - start_time_epoch)))\n",
    "\n",
    "                    last_print_time = current_time\n",
    "                    processed_samples_last_print = 0\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "                    sys.stderr.flush()\n",
    "\n",
    "    def epochIteration_Cython_BPR_SGD(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        cdef long num_total_batch = int(self.n_users / self.batch_size) + 1\n",
    "\n",
    "\n",
    "        cdef BPR_sample sample\n",
    "        cdef long u, i, j\n",
    "        cdef long factor_index, num_current_batch, num_sample_in_batch, processed_samples_last_print, print_block_size = 500\n",
    "        cdef double x_uij, sigmoid_user, sigmoid_item, local_gradient_i, local_gradient_j, local_gradient_u\n",
    "\n",
    "        cdef double H_i, H_j, W_u, cumulative_loss = 0.0\n",
    "\n",
    "\n",
    "        cdef long start_time_epoch = time.time()\n",
    "        cdef long last_print_time = start_time_epoch\n",
    "\n",
    "        for num_current_batch in range(num_total_batch):\n",
    "\n",
    "            self._clear_minibatch_data_structures()\n",
    "\n",
    "            # Iterate over samples in batch\n",
    "            for num_sample_in_batch in range(self.batch_size):\n",
    "\n",
    "                # Uniform user sampling with replacement\n",
    "                sample = self.sampleBPR_Cython()\n",
    "\n",
    "                self._add_BPR_sample_in_minibatch(sample)\n",
    "\n",
    "                u = sample.user\n",
    "                i = sample.pos_item\n",
    "                j = sample.neg_item\n",
    "\n",
    "                x_uij = 0.0\n",
    "\n",
    "                for factor_index in range(self.n_factors):\n",
    "                    x_uij += self.USER_factors[u,factor_index] * (self.ITEM_factors[i,factor_index] - self.ITEM_factors[j,factor_index])\n",
    "\n",
    "                # Use gradient of log(sigm(-x_uij))\n",
    "                sigmoid_item = 1 / (1 + exp(x_uij))\n",
    "                sigmoid_user = sigmoid_item\n",
    "\n",
    "                cumulative_loss += x_uij**2\n",
    "\n",
    "\n",
    "                for factor_index in range(self.n_factors):\n",
    "\n",
    "                    # Copy original value to avoid messing up the updates\n",
    "                    H_i = self.ITEM_factors[i, factor_index]\n",
    "                    H_j = self.ITEM_factors[j, factor_index]\n",
    "                    W_u = self.USER_factors[u, factor_index]\n",
    "\n",
    "                    # Compute gradients\n",
    "                    local_gradient_i = sigmoid_item * ( W_u ) - self.positive_reg * H_i\n",
    "                    local_gradient_j = sigmoid_item * (-W_u ) - self.negative_reg * H_j\n",
    "                    local_gradient_u = sigmoid_user * ( H_i - H_j ) - self.user_reg * W_u\n",
    "\n",
    "                    self.USER_factors_minibatch_accumulator[u, factor_index] += local_gradient_u\n",
    "                    self.ITEM_factors_minibatch_accumulator[i, factor_index] += local_gradient_i\n",
    "                    self.ITEM_factors_minibatch_accumulator[j, factor_index] += local_gradient_j\n",
    "\n",
    "\n",
    "            self._apply_minibatch_updates_to_latent_factors()\n",
    "\n",
    "\n",
    "            # Exponentiation of beta at the end of each sample\n",
    "            if self.useAdam:\n",
    "\n",
    "                self.beta_1_power_t *= self.beta_1\n",
    "                self.beta_2_power_t *= self.beta_2\n",
    "\n",
    "\n",
    "            if self.verbose and (processed_samples_last_print >= print_block_size or num_current_batch == num_total_batch-1):\n",
    "\n",
    "                current_time = time.time()\n",
    "\n",
    "                # Set block size to the number of items necessary in order to print every 30 seconds\n",
    "                samples_per_sec = num_current_batch/(time.time() - start_time_epoch)\n",
    "\n",
    "                print_block_size = int(samples_per_sec*30)\n",
    "\n",
    "                if current_time - last_print_time > 30 or num_current_batch == num_total_batch-1:\n",
    "\n",
    "                    print(\"{}: Processed {} ( {:.2f}% ) in {:.2f} seconds. BPR loss {:.2E}. Sample per second: {:.0f}\".format(\n",
    "                        self.algorithm_name,\n",
    "                        num_current_batch*self.batch_size,\n",
    "                        100.0* num_current_batch/num_total_batch,\n",
    "                        time.time() - last_print_time,\n",
    "                        cumulative_loss/(num_current_batch*self.batch_size + 1),\n",
    "                        float(num_current_batch*self.batch_size + 1) / (time.time() - start_time_epoch)))\n",
    "\n",
    "                    last_print_time = current_time\n",
    "                    processed_samples_last_print = 0\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "                    sys.stderr.flush()\n",
    "\n",
    "    def get_USER_factors(self):\n",
    "        return np.array(self.USER_factors)\n",
    "\n",
    "\n",
    "    def get_ITEM_factors(self):\n",
    "        return np.array(self.ITEM_factors)\n",
    "\n",
    "\n",
    "    def get_USER_bias(self):\n",
    "        return np.array(self.USER_bias)\n",
    "\n",
    "\n",
    "    def get_ITEM_bias(self):\n",
    "        return np.array(self.ITEM_bias)\n",
    "\n",
    "\n",
    "    def get_GLOBAL_bias(self):\n",
    "        return np.array(self.GLOBAL_bias[0])\n",
    "\n",
    "\n",
    "    def _init_minibatch_data_structures(self):\n",
    "\n",
    "        # The shape depends on the batch size. 1 for FunkSVD 2 for BPR as it samples two items\n",
    "        self.mini_batch_sampled_items = np.zeros(self.batch_size*2, dtype=np.int)\n",
    "        self.mini_batch_sampled_users = np.zeros(self.batch_size, dtype=np.int)\n",
    "\n",
    "        self.mini_batch_sampled_items_flag = np.zeros(self.n_items, dtype=np.int)\n",
    "        self.mini_batch_sampled_users_flag = np.zeros(self.n_users, dtype=np.int)\n",
    "\n",
    "        self.mini_batch_sampled_items_counter = 0\n",
    "        self.mini_batch_sampled_users_counter = 0\n",
    "\n",
    "    cdef void _clear_minibatch_data_structures(self):\n",
    "\n",
    "        cdef long array_index, item_index\n",
    "\n",
    "        for array_index in range(self.mini_batch_sampled_items_counter):\n",
    "            item_index = self.mini_batch_sampled_items[array_index]\n",
    "            self.mini_batch_sampled_items_flag[item_index] = False\n",
    "\n",
    "        for array_index in range(self.mini_batch_sampled_users_counter):\n",
    "            item_index = self.mini_batch_sampled_users[array_index]\n",
    "            self.mini_batch_sampled_users_flag[item_index] = False\n",
    "\n",
    "        self.mini_batch_sampled_items_counter = 0\n",
    "        self.mini_batch_sampled_users_counter = 0\n",
    "\n",
    "    cdef void _add_MSE_sample_in_minibatch(self, MSE_sample sample):\n",
    "\n",
    "        if not self.mini_batch_sampled_items_flag[sample.item]:\n",
    "            self.mini_batch_sampled_items_flag[sample.item] = True\n",
    "            self.mini_batch_sampled_items[self.mini_batch_sampled_items_counter] = sample.item\n",
    "            self.mini_batch_sampled_items_counter += 1\n",
    "\n",
    "        if not self.mini_batch_sampled_users_flag[sample.user]:\n",
    "            self.mini_batch_sampled_users_flag[sample.user] = True\n",
    "            self.mini_batch_sampled_users[self.mini_batch_sampled_users_counter] = sample.user\n",
    "            self.mini_batch_sampled_users_counter += 1\n",
    "\n",
    "    cdef void _add_BPR_sample_in_minibatch(self, BPR_sample sample):\n",
    "\n",
    "        if not self.mini_batch_sampled_items_flag[sample.pos_item]:\n",
    "            self.mini_batch_sampled_items_flag[sample.pos_item] = True\n",
    "            self.mini_batch_sampled_items[self.mini_batch_sampled_items_counter] = sample.pos_item\n",
    "            self.mini_batch_sampled_items_counter += 1\n",
    "\n",
    "        if not self.mini_batch_sampled_items_flag[sample.neg_item]:\n",
    "            self.mini_batch_sampled_items_flag[sample.neg_item] = True\n",
    "            self.mini_batch_sampled_items[self.mini_batch_sampled_items_counter] = sample.neg_item\n",
    "            self.mini_batch_sampled_items_counter += 1\n",
    "\n",
    "        if not self.mini_batch_sampled_users_flag[sample.user]:\n",
    "            self.mini_batch_sampled_users_flag[sample.user] = True\n",
    "            self.mini_batch_sampled_users[self.mini_batch_sampled_users_counter] = sample.user\n",
    "            self.mini_batch_sampled_users_counter += 1\n",
    "\n",
    "    cdef void _apply_minibatch_updates_to_latent_factors(self):\n",
    "\n",
    "        cdef double local_gradient_item, local_gradient_user, local_gradient_bias_item, local_gradient_bias_user, local_gradient_bias_global\n",
    "        cdef long sampled_user, sampled_item, num_sample_in_batch\n",
    "\n",
    "        if self.use_bias:\n",
    "\n",
    "            # Compute adaptive gradients\n",
    "            local_gradient_bias_global = self.GLOBAL_bias_minibatch_accumulator[0] / self.batch_size\n",
    "            local_gradient_bias_global = self.adaptive_gradient(local_gradient_bias_global, 0, 0, self.sgd_cache_bias_GLOBAL, self.sgd_cache_bias_GLOBAL_momentum_1, self.sgd_cache_bias_GLOBAL_momentum_2)\n",
    "\n",
    "            # Apply updates to bias\n",
    "            self.GLOBAL_bias[0] += self.learning_rate * local_gradient_bias_global\n",
    "            self.GLOBAL_bias_minibatch_accumulator[0] = 0.0\n",
    "\n",
    "        for num_sample_in_batch in range(self.mini_batch_sampled_items_counter):\n",
    "\n",
    "            sampled_item = self.mini_batch_sampled_items[num_sample_in_batch]\n",
    "\n",
    "            if self.use_bias:\n",
    "                local_gradient_bias_item = self.ITEM_bias_minibatch_accumulator[sampled_item] / self.batch_size\n",
    "                local_gradient_bias_item = self.adaptive_gradient(local_gradient_bias_item, sampled_item, 0, self.sgd_cache_bias_I, self.sgd_cache_bias_I_momentum_1, self.sgd_cache_bias_I_momentum_2)\n",
    "\n",
    "                self.ITEM_bias[sampled_item] += self.learning_rate * local_gradient_bias_item\n",
    "                self.ITEM_bias_minibatch_accumulator[sampled_item] = 0.0\n",
    "\n",
    "\n",
    "            for factor_index in range(self.n_factors):\n",
    "                local_gradient_item = self.ITEM_factors_minibatch_accumulator[sampled_item, factor_index] / self.batch_size\n",
    "                local_gradient_item = self.adaptive_gradient(local_gradient_item, sampled_item, factor_index, self.sgd_cache_I, self.sgd_cache_I_momentum_1, self.sgd_cache_I_momentum_2)\n",
    "\n",
    "                self.ITEM_factors[sampled_item, factor_index] += self.learning_rate * local_gradient_item\n",
    "                self.ITEM_factors_minibatch_accumulator[sampled_item, factor_index] = 0.0\n",
    "\n",
    "        for num_sample_in_batch in range(self.mini_batch_sampled_users_counter):\n",
    "\n",
    "            sampled_user = self.mini_batch_sampled_users[num_sample_in_batch]\n",
    "\n",
    "            if self.use_bias:\n",
    "                local_gradient_bias_user = self.USER_bias_minibatch_accumulator[sampled_user] / self.batch_size\n",
    "                local_gradient_bias_user = self.adaptive_gradient(local_gradient_bias_user, sampled_user, 0, self.sgd_cache_bias_U, self.sgd_cache_bias_U_momentum_1, self.sgd_cache_bias_U_momentum_2)\n",
    "\n",
    "                self.USER_bias[sampled_user] += self.learning_rate * local_gradient_bias_user\n",
    "                self.USER_bias_minibatch_accumulator[sampled_user] = 0.0\n",
    "\n",
    "\n",
    "            for factor_index in range(self.n_factors):\n",
    "                local_gradient_user = self.USER_factors_minibatch_accumulator[sampled_user, factor_index] / self.batch_size\n",
    "                local_gradient_user = self.adaptive_gradient(local_gradient_user, sampled_user, factor_index, self.sgd_cache_U, self.sgd_cache_U_momentum_1, self.sgd_cache_U_momentum_2)\n",
    "\n",
    "                self.USER_factors[sampled_user, factor_index] += self.learning_rate * local_gradient_user\n",
    "                self.USER_factors_minibatch_accumulator[sampled_user, factor_index] = 0.0\n",
    "\n",
    "    cdef double adaptive_gradient(self, double gradient, long user_or_item_id, long factor_id, double[:,:] sgd_cache, double[:,:] sgd_cache_momentum_1, double[:,:] sgd_cache_momentum_2):\n",
    "\n",
    "        cdef double gradient_update\n",
    "\n",
    "        if self.useAdaGrad:\n",
    "            sgd_cache[user_or_item_id, factor_id] += gradient ** 2\n",
    "\n",
    "            gradient_update = gradient / (sqrt(sgd_cache[user_or_item_id, factor_id]) + 1e-8)\n",
    "\n",
    "        elif self.useRmsprop:\n",
    "            sgd_cache[user_or_item_id, factor_id] = sgd_cache[user_or_item_id, factor_id] * self.gamma + (1 - self.gamma) * gradient ** 2\n",
    "\n",
    "            gradient_update = gradient / (sqrt(sgd_cache[user_or_item_id, factor_id]) + 1e-8)\n",
    "\n",
    "        elif self.useAdam:\n",
    "\n",
    "            sgd_cache_momentum_1[user_or_item_id, factor_id] = \\\n",
    "                sgd_cache_momentum_1[user_or_item_id, factor_id] * self.beta_1 + (1 - self.beta_1) * gradient\n",
    "\n",
    "            sgd_cache_momentum_2[user_or_item_id, factor_id] = \\\n",
    "                sgd_cache_momentum_2[user_or_item_id, factor_id] * self.beta_2 + (1 - self.beta_2) * gradient**2\n",
    "\n",
    "            self.momentum_1 = sgd_cache_momentum_1[user_or_item_id, factor_id]/ (1 - self.beta_1_power_t)\n",
    "            self.momentum_2 = sgd_cache_momentum_2[user_or_item_id, factor_id]/ (1 - self.beta_2_power_t)\n",
    "\n",
    "            gradient_update = self.momentum_1/ (sqrt(self.momentum_2) + 1e-8)\n",
    "\n",
    "        else:\n",
    "\n",
    "            gradient_update = gradient\n",
    "\n",
    "        return gradient_update\n",
    "\n",
    "    cdef MSE_sample sampleMSE_Cython(self):\n",
    "\n",
    "        cdef MSE_sample sample = MSE_sample(-1,-1,-1.0)\n",
    "        cdef long index, start_pos_seen_items, end_pos_seen_items\n",
    "\n",
    "        cdef int neg_item_selected, sample_positive, n_seen_items = 0\n",
    "\n",
    "        # Skip users with no interactions or with no negative items\n",
    "        while n_seen_items == 0 or n_seen_items == self.n_items:\n",
    "\n",
    "            sample.user = rand() % self.n_users\n",
    "\n",
    "            start_pos_seen_items = self.URM_train_indptr[sample.user]\n",
    "            end_pos_seen_items = self.URM_train_indptr[sample.user+1]\n",
    "\n",
    "            n_seen_items = end_pos_seen_items - start_pos_seen_items\n",
    "\n",
    "\n",
    "        # Decide to sample positive or negative\n",
    "        if self.MSE_sample_negative_interactions_flag:\n",
    "            sample_positive = rand() <= self.MSE_negative_interactions_quota * RAND_MAX\n",
    "        else:\n",
    "            sample_positive = True\n",
    "\n",
    "        if sample_positive:\n",
    "\n",
    "            # Sample positive\n",
    "            index = rand() % n_seen_items\n",
    "\n",
    "            sample.item = self.URM_train_indices[start_pos_seen_items + index]\n",
    "            sample.rating = self.URM_train_data[start_pos_seen_items + index]\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Sample negative\n",
    "            neg_item_selected = False\n",
    "\n",
    "            # It's faster to just try again then to build a mapping of the non-seen items\n",
    "            # for every user\n",
    "            while not neg_item_selected:\n",
    "\n",
    "                sample.item = rand() % self.n_items\n",
    "                sample.rating = 0.0\n",
    "\n",
    "                index = 0\n",
    "                # Indices data is sorted, so I don't need to go to the end of the current row\n",
    "                while index < n_seen_items and self.URM_train_indices[start_pos_seen_items + index] < sample.item:\n",
    "                    index+=1\n",
    "\n",
    "                # If the positive item in position 'index' is == sample.item, negative not selected\n",
    "                # If the positive item in position 'index' is > sample.item or index == n_seen_items, negative selected\n",
    "                if index == n_seen_items or self.URM_train_indices[start_pos_seen_items + index] > sample.item:\n",
    "                    neg_item_selected = True\n",
    "        return sample\n",
    "\n",
    "    cdef BPR_sample sampleBPR_Cython(self):\n",
    "\n",
    "        cdef BPR_sample sample = BPR_sample(-1,-1,-1)\n",
    "        cdef long index, start_pos_seen_items, end_pos_seen_items\n",
    "\n",
    "        cdef int neg_item_selected, n_seen_items = 0\n",
    "\n",
    "\n",
    "        # Skip users with no interactions or with no negative items\n",
    "        # Skip users with no interactions or with no negative items\n",
    "        while n_seen_items == 0 or n_seen_items == self.n_items:\n",
    "\n",
    "            sample.user = rand() % self.n_users\n",
    "\n",
    "            start_pos_seen_items = self.URM_train_indptr[sample.user]\n",
    "            end_pos_seen_items = self.URM_train_indptr[sample.user+1]\n",
    "\n",
    "            n_seen_items = end_pos_seen_items - start_pos_seen_items\n",
    "            \n",
    "\n",
    "        index = rand() % n_seen_items\n",
    "\n",
    "        sample.pos_item = self.URM_train_indices[start_pos_seen_items + index]\n",
    "\n",
    "        neg_item_selected = False\n",
    "\n",
    "        # It's faster to just try again then to build a mapping of the non-seen items\n",
    "        # for every user\n",
    "        while not neg_item_selected:\n",
    "\n",
    "            sample.neg_item = rand() % self.n_items\n",
    "\n",
    "            index = 0\n",
    "            # Indices data is sorted, so I don't need to go to the end of the current row\n",
    "            while index < n_seen_items and self.URM_train_indices[start_pos_seen_items + index] < sample.neg_item:\n",
    "                index+=1\n",
    "\n",
    "            # If the positive item in position 'index' is == sample.neg_item, negative not selected\n",
    "            # If the positive item in position 'index' is > sample.neg_item or index == n_seen_items, negative selected\n",
    "            if index == n_seen_items or self.URM_train_indices[start_pos_seen_items + index] > sample.neg_item:\n",
    "                neg_item_selected = True\n",
    "\n",
    "        return sample\n",
    "    \n",
    "\n",
    "    \n",
    "# seconds_to_biggest_unit.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 30/03/2019\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "def seconds_to_biggest_unit(time_in_seconds, data_array = None):\n",
    "\n",
    "    conversion_factor = [\n",
    "        (\"sec\", 60),\n",
    "        (\"min\", 60),\n",
    "        (\"hour\", 24),\n",
    "        (\"day\", 365),\n",
    "    ]\n",
    "\n",
    "    terminate = False\n",
    "    unit_index = 0\n",
    "\n",
    "    new_time_value = time_in_seconds\n",
    "    new_time_unit = \"sec\"\n",
    "\n",
    "    while not terminate:\n",
    "\n",
    "        next_time = new_time_value/conversion_factor[unit_index][1]\n",
    "\n",
    "        if next_time >= 1.0:\n",
    "            new_time_value = next_time\n",
    "\n",
    "            if data_array is not None:\n",
    "                data_array /= conversion_factor[unit_index][1]\n",
    "\n",
    "            unit_index += 1\n",
    "            new_time_unit = conversion_factor[unit_index][0]\n",
    "\n",
    "        else:\n",
    "            terminate = True\n",
    "\n",
    "\n",
    "    if data_array is not None:\n",
    "        return new_time_value, new_time_unit, data_array\n",
    "\n",
    "    else:\n",
    "        return new_time_value, new_time_unit\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Incremental_Training_Early_Stopping.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 06/07/2018\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "import time, sys\n",
    "import numpy as np\n",
    "# from Base.BaseTempFolder import BaseTempFolder\n",
    "# from utils.Evaluation.Utils.seconds_to_biggest_unit import seconds_to_biggest_unit\n",
    "\n",
    "\n",
    "class Incremental_Training_Early_Stopping(object):\n",
    "    \"\"\"\n",
    "    This class provides a function which trains a model applying early stopping\n",
    "    The term \"incremental\" refers to the model that is updated at every epoch\n",
    "    The term \"best\" refers to the incremental model which corresponded to the best validation score\n",
    "    The object must implement the following methods:\n",
    "    _run_epoch(self, num_epoch)                 : trains the model for one epoch (e.g. calling another object implementing the training cython, pyTorch...)\n",
    "    _prepare_model_for_validation(self)         : ensures the recommender being trained can compute the predictions needed for the validation step\n",
    "    _update_best_model(self)                    : updates the best model with the current incremental one\n",
    "    _train_with_early_stopping(.)               : Function that executes the training, validation and early stopping by using the previously implemented functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Incremental_Training_Early_Stopping, self).__init__()\n",
    "\n",
    "\n",
    "    def get_early_stopping_final_epochs_dict(self):\n",
    "        \"\"\"\n",
    "        This function returns a dictionary to be used as optimal parameters in the .fit() function\n",
    "        It provides the flexibility to deal with multiple early-stopping in a single algorithm\n",
    "        e.g. in NeuMF there are three model components each with its own optimal number of epochs\n",
    "        the return dict would be {\"epochs\": epochs_best_neumf, \"epochs_gmf\": epochs_best_gmf, \"epochs_mlp\": epochs_best_mlp}\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        return {\"epochs\": self.epochs_best}\n",
    "\n",
    "\n",
    "    def _run_epoch(self, num_epoch):\n",
    "        \"\"\"\n",
    "        This function should run a single epoch on the object you train. This may either involve calling a function to do an epoch\n",
    "        on a Cython object or a loop on the data points directly in python\n",
    "        :param num_epoch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _prepare_model_for_validation(self):\n",
    "        \"\"\"\n",
    "        This function is executed before the evaluation of the current model\n",
    "        It should ensure the current object \"self\" can be passed to the evaluator object\n",
    "        E.G. if the epoch is done via Cython or PyTorch, this function should get the new parameter values from\n",
    "        the cython or pytorch objects into the self. pyhon object\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _update_best_model(self):\n",
    "        \"\"\"\n",
    "        This function is called when the incremental model is found to have better validation score than the current best one\n",
    "        So the current best model should be replaced by the current incremental one.\n",
    "        Important, remember to clone the objects and NOT to create a pointer-reference, otherwise the best solution will be altered\n",
    "        by the next epoch\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "    def _train_with_early_stopping(self, epochs_max, epochs_min = 0,\n",
    "                                   validation_every_n = None, stop_on_validation = False,\n",
    "                                   validation_metric = None, lower_validations_allowed = None, evaluator_object = None,\n",
    "                                   algorithm_name = \"Incremental_Training_Early_Stopping\"):\n",
    "        \"\"\"\n",
    "        :param epochs_max:                  max number of epochs the training will last\n",
    "        :param epochs_min:                  min number of epochs the training will last\n",
    "        :param validation_every_n:          number of epochs after which the model will be evaluated and a best_model selected\n",
    "        :param stop_on_validation:          [True/False] whether to stop the training before the max number of epochs\n",
    "        :param validation_metric:           which metric to use when selecting the best model, higher values are better\n",
    "        :param lower_validations_allowed:    number of contiguous validation steps required for the tranining to early-stop\n",
    "        :param evaluator_object:            evaluator instance used to compute the validation metrics.\n",
    "                                                If multiple cutoffs are available, the first one is used\n",
    "        :param algorithm_name:              name of the algorithm to be displayed in the output updates\n",
    "        :return: -\n",
    "        Supported uses:\n",
    "        - Train for max number of epochs with no validation nor early stopping:\n",
    "            _train_with_early_stopping(epochs_max = 100,\n",
    "                                        evaluator_object = None\n",
    "                                        epochs_min,                 not used\n",
    "                                        validation_every_n,         not used\n",
    "                                        stop_on_validation,         not used\n",
    "                                        validation_metric,          not used\n",
    "                                        lower_validations_allowed,   not used\n",
    "                                        )\n",
    "        - Train for max number of epochs with validation but NOT early stopping:\n",
    "            _train_with_early_stopping(epochs_max = 100,\n",
    "                                        evaluator_object = evaluator\n",
    "                                        stop_on_validation = False\n",
    "                                        validation_every_n = int value\n",
    "                                        validation_metric = metric name string\n",
    "                                        epochs_min,                 not used\n",
    "                                        lower_validations_allowed,   not used\n",
    "                                        )\n",
    "        - Train for max number of epochs with validation AND early stopping:\n",
    "            _train_with_early_stopping(epochs_max = 100,\n",
    "                                        epochs_min = int value\n",
    "                                        evaluator_object = evaluator\n",
    "                                        stop_on_validation = True\n",
    "                                        validation_every_n = int value\n",
    "                                        validation_metric = metric name string\n",
    "                                        lower_validations_allowed = int value\n",
    "                                        )\n",
    "        \"\"\"\n",
    "\n",
    "        assert epochs_max >= 0, \"{}: Number of epochs_max must be >= 0, passed was {}\".format(algorithm_name, epochs_max)\n",
    "        assert epochs_min >= 0, \"{}: Number of epochs_min must be >= 0, passed was {}\".format(algorithm_name, epochs_min)\n",
    "        assert epochs_min <= epochs_max, \"{}: epochs_min must be <= epochs_max, passed are epochs_min {}, epochs_max {}\".format(algorithm_name, epochs_min, epochs_max)\n",
    "\n",
    "        # Train for max number of epochs with no validation nor early stopping\n",
    "        # OR Train for max number of epochs with validation but NOT early stopping\n",
    "        # OR Train for max number of epochs with validation AND early stopping\n",
    "        assert evaluator_object is None or\\\n",
    "               (evaluator_object is not None and not stop_on_validation and validation_every_n is not None and validation_metric is not None) or\\\n",
    "               (evaluator_object is not None and stop_on_validation and validation_every_n is not None and validation_metric is not None and lower_validations_allowed is not None),\\\n",
    "            \"{}: Inconsistent parameters passed, please check the supported uses\".format(algorithm_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.best_validation_metric = None\n",
    "        lower_validatons_count = 0\n",
    "        convergence = False\n",
    "\n",
    "        self.epochs_best = 0\n",
    "\n",
    "        epochs_current = 0\n",
    "\n",
    "        while epochs_current < epochs_max and not convergence:\n",
    "\n",
    "            self._run_epoch(epochs_current)\n",
    "\n",
    "            # If no validation required, always keep the latest\n",
    "            if evaluator_object is None:\n",
    "\n",
    "                self.epochs_best = epochs_current\n",
    "\n",
    "            # Determine whether a validaton step is required\n",
    "            elif (epochs_current + 1) % validation_every_n == 0:\n",
    "\n",
    "                print(\"{}: Validation begins...\".format(algorithm_name))\n",
    "\n",
    "                self._prepare_model_for_validation()\n",
    "\n",
    "                # If the evaluator validation has multiple cutoffs, choose the first one\n",
    "                results_run, results_run_string = evaluator_object.evaluateRecommender(self)\n",
    "                results_run = results_run[list(results_run.keys())[0]]\n",
    "\n",
    "                print(\"{}: {}\".format(algorithm_name, results_run_string))\n",
    "\n",
    "                # Update optimal model\n",
    "                current_metric_value = results_run[validation_metric]\n",
    "                #\n",
    "                # if not np.isfinite(current_metric_value):\n",
    "                #     if isinstance(self, BaseTempFolder):\n",
    "                #         # If the recommender uses BaseTempFolder, clean the temp folder\n",
    "                #         self._clean_temp_folder(temp_file_folder=self.temp_file_folder)\n",
    "                #\n",
    "                #     assert False, \"{}: metric value is not a finite number, terminating!\".format(self.RECOMMENDER_NAME)\n",
    "                #\n",
    "\n",
    "                if self.best_validation_metric is None or self.best_validation_metric < current_metric_value:\n",
    "\n",
    "                    print(\"{}: New best model found! Updating.\".format(algorithm_name))\n",
    "\n",
    "                    self.best_validation_metric = current_metric_value\n",
    "\n",
    "                    self._update_best_model()\n",
    "\n",
    "                    self.epochs_best = epochs_current +1\n",
    "                    lower_validatons_count = 0\n",
    "\n",
    "                else:\n",
    "                    lower_validatons_count += 1\n",
    "\n",
    "\n",
    "                if stop_on_validation and lower_validatons_count >= lower_validations_allowed and epochs_current >= epochs_min:\n",
    "                    convergence = True\n",
    "\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n",
    "\n",
    "                    print(\"{}: Convergence reached! Terminating at epoch {}. Best value for '{}' at epoch {} is {:.4f}. Elapsed time {:.2f} {}\".format(\n",
    "                        algorithm_name, epochs_current+1, validation_metric, self.epochs_best, self.best_validation_metric, new_time_value, new_time_unit))\n",
    "\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n",
    "\n",
    "            print(\"{}: Epoch {} of {}. Elapsed time {:.2f} {}\".format(\n",
    "                algorithm_name, epochs_current+1, epochs_max, new_time_value, new_time_unit))\n",
    "\n",
    "            epochs_current += 1\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            sys.stderr.flush()\n",
    "\n",
    "        # If no validation required, keep the latest\n",
    "        if evaluator_object is None:\n",
    "\n",
    "            self._prepare_model_for_validation()\n",
    "            self._update_best_model()\n",
    "\n",
    "\n",
    "        # Stop when max epochs reached and not early-stopping\n",
    "        if not convergence:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n",
    "\n",
    "            if evaluator_object is not None and self.best_validation_metric is not None:\n",
    "                print(\"{}: Terminating at epoch {}. Best value for '{}' at epoch {} is {:.4f}. Elapsed time {:.2f} {}\".format(\n",
    "                    algorithm_name, epochs_current, validation_metric, self.epochs_best, self.best_validation_metric, new_time_value, new_time_unit))\n",
    "            else:\n",
    "                print(\"{}: Terminating at epoch {}. Elapsed time {:.2f} {}\".format(\n",
    "                    algorithm_name, epochs_current, new_time_value, new_time_unit))\n",
    "\n",
    "\n",
    "    \n",
    "     \n",
    "# _MatrixFactorization_Cython\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 07/09/17\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "# from Base.BaseMatrixFactorizationRecommender import BaseMatrixFactorizationRecommender\n",
    "# from Base.Incremental_Training_Early_Stopping import Incremental_Training_Early_Stopping\n",
    "# from Base.Recommender_utils import check_matrix\n",
    "\n",
    "# from CythonCompiler.run_compile_subprocess import run_compile_subprocess\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class _MatrixFactorization_Cython(BaseMatrixFactorizationRecommender, Incremental_Training_Early_Stopping):\n",
    "\n",
    "    RECOMMENDER_NAME = \"MatrixFactorization_Cython_Recommender\"\n",
    "\n",
    "\n",
    "    def __init__(self, URM_train, verbose = True, recompile_cython = False, algorithm_name = \"MF_BPR\"):\n",
    "        super(_MatrixFactorization_Cython, self).__init__(URM_train, verbose = verbose)\n",
    "\n",
    "        self.n_users, self.n_items = self.URM_train.shape\n",
    "        self.normalize = False\n",
    "        self.algorithm_name = algorithm_name\n",
    "\n",
    "        if recompile_cython:\n",
    "            print(\"Compiling in Cython\")\n",
    "            self.runCompilationScript()\n",
    "            print(\"Compilation Complete\")\n",
    "\n",
    "\n",
    "    def fit(self, epochs=300, batch_size = 1000,\n",
    "            num_factors=10, positive_threshold_BPR = None,\n",
    "            learning_rate = 0.001, use_bias = True,\n",
    "            sgd_mode='sgd',\n",
    "            negative_interactions_quota = 0.0,\n",
    "            init_mean = 0.0, init_std_dev = 0.1,\n",
    "            user_reg = 0.0, item_reg = 0.0, bias_reg = 0.0, positive_reg = 0.0, negative_reg = 0.0,\n",
    "            random_seed = None,\n",
    "            **earlystopping_kwargs):\n",
    "\n",
    "\n",
    "        self.num_factors = num_factors\n",
    "        self.use_bias = use_bias\n",
    "        self.sgd_mode = sgd_mode\n",
    "        self.positive_threshold_BPR = positive_threshold_BPR\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        assert negative_interactions_quota >= 0.0 and negative_interactions_quota < 1.0, \"{}: negative_interactions_quota must be a float value >=0 and < 1.0, provided was '{}'\".format(self.RECOMMENDER_NAME, negative_interactions_quota)\n",
    "        self.negative_interactions_quota = negative_interactions_quota\n",
    "\n",
    "        # Import compiled module\n",
    "#         from MatrixFactorization.Cython.MatrixFactorization_Cython_Epoch import MatrixFactorization_Cython_Epoch\n",
    "\n",
    "\n",
    "        if self.algorithm_name in [\"FUNK_SVD\", \"ASY_SVD\"]:\n",
    "\n",
    "            self.cythonEpoch = MatrixFactorization_Cython_Epoch(self.URM_train,\n",
    "                                                                algorithm_name = self.algorithm_name,\n",
    "                                                                n_factors = self.num_factors,\n",
    "                                                                learning_rate = learning_rate,\n",
    "                                                                sgd_mode = sgd_mode,\n",
    "                                                                user_reg = user_reg,\n",
    "                                                                item_reg = item_reg,\n",
    "                                                                bias_reg = bias_reg,\n",
    "                                                                batch_size = batch_size,\n",
    "                                                                use_bias = use_bias,\n",
    "                                                                init_mean = init_mean,\n",
    "                                                                negative_interactions_quota = negative_interactions_quota,\n",
    "                                                                init_std_dev = init_std_dev,\n",
    "                                                                verbose = self.verbose,\n",
    "                                                                random_seed = random_seed)\n",
    "\n",
    "        elif self.algorithm_name == \"MF_BPR\":\n",
    "\n",
    "            # Select only positive interactions\n",
    "            URM_train_positive = self.URM_train.copy()\n",
    "\n",
    "            if self.positive_threshold_BPR is not None:\n",
    "                URM_train_positive.data = URM_train_positive.data >= self.positive_threshold_BPR\n",
    "                URM_train_positive.eliminate_zeros()\n",
    "\n",
    "                assert URM_train_positive.nnz > 0, \"MatrixFactorization_Cython: URM_train_positive is empty, positive threshold is too high\"\n",
    "\n",
    "            self.cythonEpoch = MatrixFactorization_Cython_Epoch(URM_train_positive,\n",
    "                                                                algorithm_name = self.algorithm_name,\n",
    "                                                                n_factors = self.num_factors,\n",
    "                                                                learning_rate=learning_rate,\n",
    "                                                                sgd_mode = sgd_mode,\n",
    "                                                                user_reg = user_reg,\n",
    "                                                                positive_reg = positive_reg,\n",
    "                                                                negative_reg = negative_reg,\n",
    "                                                                batch_size = batch_size,\n",
    "                                                                use_bias = use_bias,\n",
    "                                                                init_mean = init_mean,\n",
    "                                                                init_std_dev = init_std_dev,\n",
    "                                                                verbose = self.verbose,\n",
    "                                                                random_seed = random_seed)\n",
    "        self._prepare_model_for_validation()\n",
    "        self._update_best_model()\n",
    "\n",
    "        self._train_with_early_stopping(epochs,\n",
    "                                        algorithm_name = self.algorithm_name,\n",
    "                                        **earlystopping_kwargs)\n",
    "\n",
    "\n",
    "        self.USER_factors = self.USER_factors_best\n",
    "        self.ITEM_factors = self.ITEM_factors_best\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.USER_bias = self.USER_bias_best\n",
    "            self.ITEM_bias = self.ITEM_bias_best\n",
    "            self.GLOBAL_bias = self.GLOBAL_bias_best\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def _prepare_model_for_validation(self):\n",
    "        self.USER_factors = self.cythonEpoch.get_USER_factors()\n",
    "        self.ITEM_factors = self.cythonEpoch.get_ITEM_factors()\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.USER_bias = self.cythonEpoch.get_USER_bias()\n",
    "            self.ITEM_bias = self.cythonEpoch.get_ITEM_bias()\n",
    "            self.GLOBAL_bias = self.cythonEpoch.get_GLOBAL_bias()\n",
    "\n",
    "    def _update_best_model(self):\n",
    "        self.USER_factors_best = self.USER_factors.copy()\n",
    "        self.ITEM_factors_best = self.ITEM_factors.copy()\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.USER_bias_best = self.USER_bias.copy()\n",
    "            self.ITEM_bias_best = self.ITEM_bias.copy()\n",
    "            self.GLOBAL_bias_best = self.GLOBAL_bias\n",
    "\n",
    "\n",
    "    def _run_epoch(self, num_epoch):\n",
    "       self.cythonEpoch.epochIteration_Cython()\n",
    "\n",
    "\n",
    "    def runCompilationScript(self):\n",
    "\n",
    "        # Run compile script setting the working directory to ensure the compiled file are contained in the\n",
    "        # appropriate subfolder and not the project root\n",
    "\n",
    "        file_subfolder = \"/MatrixFactorization/Cython\"\n",
    "        file_to_compile_list = ['MatrixFactorization_Cython_Epoch.pyx']\n",
    "\n",
    "        run_compile_subprocess(file_subfolder, file_to_compile_list)\n",
    "\n",
    "        print(\"{}: Compiled module {} in subfolder: {}\".format(self.RECOMMENDER_NAME, file_to_compile_list, file_subfolder))\n",
    "\n",
    "        # Command to run compilation script\n",
    "        # python compile_script.py MatrixFactorization_Cython_Epoch.pyx build_ext --inplace\n",
    "\n",
    "        # Command to generate html report\n",
    "        # cython -a MatrixFactorization_Cython_Epoch.pyx\n",
    "\n",
    "\n",
    "\n",
    "class MatrixFactorization_BPR_Cython(_MatrixFactorization_Cython):\n",
    "    \"\"\"\n",
    "    Subclas allowing only for MF BPR\n",
    "    \"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"MatrixFactorization_BPR_Cython_Recommender\"\n",
    "\n",
    "    def __init__(self, *pos_args, **key_args):\n",
    "        super(MatrixFactorization_BPR_Cython, self).__init__(*pos_args, algorithm_name=\"MF_BPR\", **key_args)\n",
    "\n",
    "    def fit(self, **key_args):\n",
    "\n",
    "        key_args[\"use_bias\"] = False\n",
    "        key_args[\"negative_interactions_quota\"] = 0.0\n",
    "\n",
    "        super(MatrixFactorization_BPR_Cython, self).fit(**key_args)\n",
    "\n",
    "\n",
    "\n",
    "class MatrixFactorization_FunkSVD_Cython(_MatrixFactorization_Cython):\n",
    "    \"\"\"\n",
    "    Subclas allowing only for FunkSVD model\n",
    "    Reference: http://sifter.org/~simon/journal/20061211.html\n",
    "    Factorizes the rating matrix R into the dot product of two matrices U and V of latent factors.\n",
    "    U represent the user latent factors, V the item latent factors.\n",
    "    The model is learned by solving the following regularized Least-squares objective function with Stochastic Gradient Descent\n",
    "    \\operatornamewithlimits{argmin} \\limits_{U,V}\\frac{1}{2}||R - UV^T||^2_2 + \\frac{\\lambda}{2}(||U||^2_F + ||V||^2_F)\n",
    "    Latent factors are initialized from a Normal distribution with given mean and std.\n",
    "    \"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"MatrixFactorization_FunkSVD_Cython_Recommender\"\n",
    "\n",
    "    def __init__(self, *pos_args, **key_args):\n",
    "        super(MatrixFactorization_FunkSVD_Cython, self).__init__(*pos_args, algorithm_name=\"FUNK_SVD\", **key_args)\n",
    "\n",
    "\n",
    "    def fit(self, **key_args):\n",
    "\n",
    "        super(MatrixFactorization_FunkSVD_Cython, self).fit(**key_args)\n",
    "\n",
    "\n",
    "class MatrixFactorization_AsySVD_Cython(_MatrixFactorization_Cython):\n",
    "    \"\"\"\n",
    "    Subclas allowing only for AsymmetricSVD model\n",
    "    Reference: Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model (Koren, 2008)\n",
    "    Factorizes the rating matrix R into two matrices X and Y of latent factors, which both represent item latent features.\n",
    "    Users are represented by aggregating the latent features in Y of items they have already rated.\n",
    "    Rating prediction is performed by computing the dot product of this accumulated user profile with the target item's\n",
    "    latent factor in X.\n",
    "    The model is learned by solving the following regularized Least-squares objective function with Stochastic Gradient Descent\n",
    "    \\operatornamewithlimits{argmin}\\limits_{x*,y*}\\frac{1}{2}\\sum_{i,j \\in R}(r_{ij} - x_j^T \\sum_{l \\in R(i)} r_{il}y_l)^2 + \\frac{\\lambda}{2}(\\sum_{i}{||x_i||^2} + \\sum_{j}{||y_j||^2})\n",
    "    \"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"MatrixFactorization_AsySVD_Cython_Recommender\"\n",
    "\n",
    "    def __init__(self, *pos_args, **key_args):\n",
    "        super(MatrixFactorization_AsySVD_Cython, self).__init__(*pos_args, algorithm_name=\"ASY_SVD\", **key_args)\n",
    "\n",
    "\n",
    "    def fit(self, **key_args):\n",
    "\n",
    "        if \"batch_size\" in key_args and key_args[\"batch_size\"] > 1:\n",
    "            print(\"{}: batch_size not supported for this recommender, setting to default value 1.\".format(self.RECOMMENDER_NAME))\n",
    "\n",
    "        key_args[\"batch_size\"] = 1\n",
    "\n",
    "        super(MatrixFactorization_AsySVD_Cython, self).fit(**key_args)\n",
    "\n",
    "\n",
    "\n",
    "    def _prepare_model_for_validation(self):\n",
    "        \"\"\"\n",
    "        AsymmetricSVD Computes two |n_items| x |n_features| matrices of latent factors\n",
    "        ITEM_factors_Y must be used to estimate user's latent factors via the items they interacted with\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.ITEM_factors_Y = self.cythonEpoch.get_USER_factors()\n",
    "        self.USER_factors = self._estimate_user_factors(self.ITEM_factors_Y)\n",
    "\n",
    "        self.ITEM_factors = self.cythonEpoch.get_ITEM_factors()\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.USER_bias = self.cythonEpoch.get_USER_bias()\n",
    "            self.ITEM_bias = self.cythonEpoch.get_ITEM_bias()\n",
    "            self.GLOBAL_bias = self.cythonEpoch.get_GLOBAL_bias()\n",
    "\n",
    "\n",
    "    def _update_best_model(self):\n",
    "        self.USER_factors_best = self.USER_factors.copy()\n",
    "        self.ITEM_factors_best = self.ITEM_factors.copy()\n",
    "        self.ITEM_factors_Y_best = self.ITEM_factors_Y.copy()\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.USER_bias_best = self.USER_bias.copy()\n",
    "            self.ITEM_bias_best = self.ITEM_bias.copy()\n",
    "            self.GLOBAL_bias_best = self.GLOBAL_bias\n",
    "\n",
    "\n",
    "    def _estimate_user_factors(self, ITEM_factors_Y):\n",
    "\n",
    "        profile_length = np.ediff1d(self.URM_train.indptr)\n",
    "        profile_length_sqrt = np.sqrt(profile_length)\n",
    "\n",
    "        # Estimating the USER_factors using ITEM_factors_Y\n",
    "        if self.verbose:\n",
    "            print(\"{}: Estimating user factors... \".format(self.algorithm_name))\n",
    "\n",
    "        USER_factors = self.URM_train.dot(ITEM_factors_Y)\n",
    "\n",
    "        #Divide every row for the sqrt of the profile length\n",
    "        for user_index in range(self.n_users):\n",
    "\n",
    "            if profile_length_sqrt[user_index] > 0:\n",
    "\n",
    "                USER_factors[user_index, :] /= profile_length_sqrt[user_index]\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"{}: Estimating user factors... done!\".format(self.algorithm_name))\n",
    "\n",
    "        return USER_factors\n",
    "    \n",
    "\n",
    "\n",
    "# check_matrix\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Function hiding some conversion checks\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)\n",
    "\n",
    "# run_compile_subprocess.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 06/01/2018\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "import subprocess, os, sys, shutil\n",
    "\n",
    "def run_compile_subprocess(file_subfolder, file_to_compile_list):\n",
    "\n",
    "    # Run compile script setting the working directory to ensure the compiled file are contained in the\n",
    "    # appropriate subfolder and not the project root\n",
    "\n",
    "    current_python_path = sys.executable\n",
    "\n",
    "    compile_script_absolute_path = os.getcwd() + '/CythonCompiler/compile_script.py'\n",
    "    file_subfolder_absolute_path = os.getcwd() + \"/\" + file_subfolder\n",
    "\n",
    "    for file_to_compile in file_to_compile_list:\n",
    "\n",
    "        try:\n",
    "            command = [current_python_path,\n",
    "                       compile_script_absolute_path,\n",
    "                       file_to_compile,\n",
    "                       'build_ext',\n",
    "                       '--inplace'\n",
    "                       ]\n",
    "\n",
    "            output = subprocess.check_output(' '.join(command),\n",
    "                                             shell=True,\n",
    "                                             cwd=file_subfolder_absolute_path)\n",
    "\n",
    "            try:\n",
    "\n",
    "                command = ['cython',\n",
    "                           file_to_compile,\n",
    "                           '-a'\n",
    "                           ]\n",
    "                output = subprocess.check_output(' '.join(command),\n",
    "                                                 shell=True,\n",
    "                                                 cwd=file_subfolder_absolute_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        except Exception as exc:\n",
    "            raise exc\n",
    "\n",
    "        finally:\n",
    "            # Removing temporary \"build\" subfolder\n",
    "            shutil.rmtree(file_subfolder_absolute_path + \"/build\", ignore_errors=True)\n",
    "\n",
    "\n",
    "    # Command to run compilation script\n",
    "    # python CythonCompiler/compile_script.py filename.pyx build_ext --inplace\n",
    "\n",
    "    # Command to generate html report\n",
    "    # cython -a filename.pyx\n",
    "    \n",
    "    \n",
    "\n",
    "# DataIO.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 27/04/2019\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "import os, json, zipfile, shutil, platform\n",
    "\n",
    "import scipy.sparse as sps\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def json_not_serializable_handler(o):\n",
    "    \"\"\"\n",
    "    Json cannot serialize automatically some data types, for example numpy integers (int32).\n",
    "    This may be a limitation of numpy-json interfaces for Python 3.6 and may not occur in Python 3.7\n",
    "    :param o:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(o, np.integer):\n",
    "        return int(o)\n",
    "\n",
    "    raise TypeError(\"json_not_serializable_handler: object '{}' is not serializable.\".format(type(o)))\n",
    "\n",
    "\n",
    "class DataIO(object):\n",
    "    \"\"\" DataIO\"\"\"\n",
    "\n",
    "    _DEFAULT_TEMP_FOLDER = \".temp_DataIO_\"\n",
    "\n",
    "    # _MAX_PATH_LENGTH_LINUX = 4096\n",
    "    _MAX_PATH_LENGTH_WINDOWS = 255\n",
    "\n",
    "    def __init__(self, folder_path):\n",
    "        super(DataIO, self).__init__()\n",
    "\n",
    "        self._is_windows = platform.system() == \"Windows\"\n",
    "\n",
    "        self.folder_path = folder_path\n",
    "        self._key_string_alert_done = False\n",
    "\n",
    "        # if self._is_windows:\n",
    "        #     self.folder_path = \"\\\\\\\\?\\\\\" + self.folder_path\n",
    "\n",
    "\n",
    "    def _print(self, message):\n",
    "        print(\"{}: {}\".format(\"DataIO\", message))\n",
    "\n",
    "\n",
    "    def _get_temp_folder(self, file_name):\n",
    "        \"\"\"\n",
    "        Creates a temporary folder to be used during the data saving\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Ignore the .zip extension\n",
    "        file_name = file_name[:-4]\n",
    "\n",
    "        current_temp_folder = \"{}{}_{}/\".format(self.folder_path, self._DEFAULT_TEMP_FOLDER, file_name)\n",
    "\n",
    "        if os.path.exists(current_temp_folder):\n",
    "            self._print(\"Folder {} already exists, could be the result of a previous failed save attempt or multiple saver are active in parallel. \" \\\n",
    "            \"Folder will be removed.\".format(current_temp_folder))\n",
    "\n",
    "            shutil.rmtree(current_temp_folder, ignore_errors=True)\n",
    "\n",
    "        os.makedirs(current_temp_folder)\n",
    "\n",
    "        return current_temp_folder\n",
    "\n",
    "\n",
    "    def _check_dict_key_type(self, dict_to_save):\n",
    "        \"\"\"\n",
    "        Check whether the keys of the dictionary are string. If not, transforms them into strings\n",
    "        :param dict_to_save:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        all_keys_are_str = all(isinstance(key, str) for key in dict_to_save.keys())\n",
    "\n",
    "        if all_keys_are_str:\n",
    "            return dict_to_save\n",
    "\n",
    "        if not self._key_string_alert_done:\n",
    "            self._print(\"Json dumps supports only 'str' as dictionary keys. Transforming keys to string, note that this will alter the mapper content.\")\n",
    "            self._key_string_alert_done = True\n",
    "\n",
    "        dict_to_save_key_str = {str(key):val for (key,val) in dict_to_save.items()}\n",
    "\n",
    "        assert all(dict_to_save_key_str[str(key)] == val for (key,val) in dict_to_save.items()), \\\n",
    "            \"DataIO: Transforming dictionary keys into strings altered its content. Duplicate keys may have been produced.\"\n",
    "\n",
    "        return dict_to_save_key_str\n",
    "\n",
    "\n",
    "    def save_data(self, file_name, data_dict_to_save):\n",
    "\n",
    "        # If directory does not exist, create with .temp_model_folder\n",
    "        if not os.path.exists(self.folder_path):\n",
    "            os.makedirs(self.folder_path)\n",
    "\n",
    "        if file_name[-4:] != \".zip\":\n",
    "            file_name += \".zip\"\n",
    "\n",
    "\n",
    "        current_temp_folder = self._get_temp_folder(file_name)\n",
    "\n",
    "        attribute_to_file_name = {}\n",
    "        attribute_to_json_file = {}\n",
    "\n",
    "        for attrib_name, attrib_data in data_dict_to_save.items():\n",
    "\n",
    "            current_file_path = current_temp_folder + attrib_name\n",
    "\n",
    "            if isinstance(attrib_data, DataFrame):\n",
    "                attrib_data.to_csv(current_file_path, index=False)\n",
    "                attribute_to_file_name[attrib_name] = attrib_name + \".csv\"\n",
    "\n",
    "            elif isinstance(attrib_data, sps.spmatrix):\n",
    "                sps.save_npz(current_file_path, attrib_data)\n",
    "                attribute_to_file_name[attrib_name] = attrib_name + \".npz\"\n",
    "\n",
    "            elif isinstance(attrib_data, np.ndarray):\n",
    "                # allow_pickle is FALSE to prevent using pickle and ensure portability\n",
    "                np.save(current_file_path, attrib_data, allow_pickle=False)\n",
    "                attribute_to_file_name[attrib_name] = attrib_name + \".npy\"\n",
    "\n",
    "            else:\n",
    "                attribute_to_json_file[attrib_name] = attrib_data\n",
    "                attribute_to_file_name[attrib_name] = attrib_name + \".json\"\n",
    "\n",
    "\n",
    "        # Save list objects\n",
    "        attribute_to_json_file[\".DataIO_attribute_to_file_name\"] = attribute_to_file_name.copy()\n",
    "\n",
    "        for attrib_name, attrib_data in attribute_to_json_file.items():\n",
    "\n",
    "            current_file_path = current_temp_folder + attrib_name\n",
    "            attribute_to_file_name[attrib_name] = attrib_name + \".json\"\n",
    "\n",
    "            # if self._is_windows and len(current_file_path + \".json\") >= self._MAX_PATH_LENGTH_WINDOWS:\n",
    "            #     current_file_path = \"\\\\\\\\?\\\\\" + current_file_path\n",
    "\n",
    "            absolute_path = current_file_path + \".json\" if current_file_path.startswith(os.getcwd()) else os.getcwd() + current_file_path + \".json\"\n",
    "\n",
    "            assert not self._is_windows or (self._is_windows and len(absolute_path) <= self._MAX_PATH_LENGTH_WINDOWS), \\\n",
    "                \"DataIO: Path of file exceeds {} characters, which is the maximum allowed under standard paths for Windows.\".format(self._MAX_PATH_LENGTH_WINDOWS)\n",
    "\n",
    "\n",
    "            with open(current_file_path + \".json\", 'w') as outfile:\n",
    "\n",
    "                if isinstance(attrib_data, dict):\n",
    "                    attrib_data = self._check_dict_key_type(attrib_data)\n",
    "\n",
    "                json.dump(attrib_data, outfile, default=json_not_serializable_handler)\n",
    "\n",
    "\n",
    "\n",
    "        with zipfile.ZipFile(self.folder_path + file_name, 'w', compression=zipfile.ZIP_DEFLATED) as myzip:\n",
    "\n",
    "            for file_name in attribute_to_file_name.values():\n",
    "                myzip.write(current_temp_folder + file_name, arcname = file_name)\n",
    "\n",
    "\n",
    "\n",
    "        shutil.rmtree(current_temp_folder, ignore_errors=True)\n",
    "\n",
    "\n",
    "    def load_data(self, file_name):\n",
    "\n",
    "        if file_name[-4:] != \".zip\":\n",
    "            file_name += \".zip\"\n",
    "\n",
    "        dataFile = zipfile.ZipFile(self.folder_path + file_name)\n",
    "\n",
    "        dataFile.testzip()\n",
    "\n",
    "        current_temp_folder = self._get_temp_folder(file_name)\n",
    "\n",
    "        try:\n",
    "\n",
    "            try:\n",
    "                attribute_to_file_name_path = dataFile.extract(\".DataIO_attribute_to_file_name.json\", path = current_temp_folder)\n",
    "            except KeyError:\n",
    "                attribute_to_file_name_path = dataFile.extract(\"__DataIO_attribute_to_file_name.json\", path = current_temp_folder)\n",
    "\n",
    "\n",
    "            with open(attribute_to_file_name_path, \"r\") as json_file:\n",
    "                attribute_to_file_name = json.load(json_file)\n",
    "\n",
    "            data_dict_loaded = {}\n",
    "\n",
    "            for attrib_name, file_name in attribute_to_file_name.items():\n",
    "\n",
    "                attrib_file_path = dataFile.extract(file_name, path = current_temp_folder)\n",
    "                attrib_data_type = file_name.split(\".\")[-1]\n",
    "\n",
    "                if attrib_data_type == \"csv\":\n",
    "                    attrib_data = pd.read_csv(attrib_file_path, index_col=False)\n",
    "\n",
    "                elif attrib_data_type == \"npz\":\n",
    "                    attrib_data = sps.load_npz(attrib_file_path)\n",
    "\n",
    "                elif attrib_data_type == \"npy\":\n",
    "                    # allow_pickle is FALSE to prevent using pickle and ensure portability\n",
    "                    attrib_data = np.load(attrib_file_path, allow_pickle=False)\n",
    "\n",
    "                elif attrib_data_type == \"json\":\n",
    "                    with open(attrib_file_path, \"r\") as json_file:\n",
    "                        attrib_data = json.load(json_file)\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"Attribute type not recognized for: '{}' of class: '{}'\".format(attrib_file_path, attrib_data_type))\n",
    "\n",
    "                data_dict_loaded[attrib_name] = attrib_data\n",
    "\n",
    "\n",
    "        except Exception as exec:\n",
    "\n",
    "            shutil.rmtree(current_temp_folder, ignore_errors=True)\n",
    "            raise exec\n",
    "\n",
    "        shutil.rmtree(current_temp_folder, ignore_errors=True)\n",
    "\n",
    "\n",
    "        return data_dict_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URM built!\n",
      "ICM built!\n",
      "\n",
      "\n",
      "Recommender Systems: \n",
      "1. MatrixFactorization_BPR_Cython_Recommender\n",
      "2. MatrixFactorization_FunkSVD_Cython_Recommender\n"
     ]
    },
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b68eac00e327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mselected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nSelect a recommender system: '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mrecommender_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollaborative_algorithm_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n ... {} ... '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecommender_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRECOMMENDER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_stdin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             raise StdinNotImplementedError(\n\u001b[0;32m--> 855\u001b[0;31m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m             )\n\u001b[1;32m    857\u001b[0m         return self._input_request(str(prompt),\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import os, traceback\n",
    "import numpy as np\n",
    "\n",
    "URM_all = build_URM()\n",
    "ICM_all = build_ICM()\n",
    "# data_manager.get_statistics_URM(URM)\n",
    "\n",
    "######################################################################\n",
    "##########                                                  ##########\n",
    "##########      TRAINING, EVALUATION AND PREDICTIONS        ##########\n",
    "##########                                                  ##########\n",
    "######################################################################\n",
    "\n",
    "\n",
    "# URM train/validation/test splitting\n",
    "# -----------------------------------\n",
    "\n",
    "# from Data_manager.Movielens1M.Movielens1MReader import Movielens1MReader\n",
    "# from Data_manager.DataSplitter_k_fold_stratified import DataSplitter_Warm_k_fold\n",
    "\n",
    "# dataset_object = Movielens1MReader()\n",
    "# dataSplitter = DataSplitter_Warm_k_fold(dataset_object)\n",
    "# dataSplitter.load_data()\n",
    "# URM_train, URM_validation, URM_test = dataSplitter.get_holdout_split()\n",
    "\n",
    "URM_train, URM_test = split_train_validation_random_holdout(URM_all, train_split=0.8)\n",
    "URM_train, URM_validation = split_train_validation_random_holdout(URM_train, train_split=0.9)\n",
    "\n",
    "\n",
    "# Tuning parameters\n",
    "# -----------------\n",
    "\n",
    "metric_to_optimize = \"MAP\"\n",
    "\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10, 15])\n",
    "evaluator_validation_earlystopping = EvaluatorHoldout(URM_train, cutoff_list=[10], exclude_seen = False) # None \n",
    "output_folder_path = \"result_experiments/\"\n",
    "\n",
    "n_cases = 8 # 2\n",
    "n_random_starts = 5  # 1\n",
    "\n",
    "save_model = \"no\"\n",
    "allow_weighting = False\n",
    "similarity_type_list = [\"cosine\"]\n",
    "\n",
    "ICM_name = \"ICM_all\"\n",
    "\n",
    "# Collaborative recommenders\n",
    "collaborative_algorithm_list = [\n",
    "    MatrixFactorization_BPR_Cython,\n",
    "    MatrixFactorization_FunkSVD_Cython,\n",
    "    # SLIM_BPR_Cython,\n",
    "    # SLIMElasticNetRecommender\n",
    "]\n",
    "\n",
    "# from Utils.PoolWithSubprocess import PoolWithSubprocess\n",
    "# import multiprocessing\n",
    "#\n",
    "# pool = PoolWithSubprocess(processes=int(multiprocessing.cpu_count()), maxtasksperchild=1)\n",
    "# resultList = pool.map(runParameterSearch_Collaborative_partial, collaborative_algorithm_list)\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "\n",
    "print('\\nRecommender Systems: ')\n",
    "for i, recomm_type in enumerate(collaborative_algorithm_list, start=1):\n",
    "    print('{}. {}'.format(i, recomm_type.RECOMMENDER_NAME))\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        selected = int(input('\\nSelect a recommender system: '.format(i)))\n",
    "        recommender_class = collaborative_algorithm_list[selected - 1]\n",
    "        print('\\n ... {} ... '.format(recommender_class.RECOMMENDER_NAME))\n",
    "\n",
    "        output_file_name_root = \"{}_metadata.zip\".format(recommender_class.RECOMMENDER_NAME)\n",
    "\n",
    "        # Hyperparameters tuning\n",
    "        apply_hyperparams_tuning = False\n",
    "        \n",
    "        if apply_hyperparams_tuning:\n",
    "            if recommender_class in collaborative_algorithm_list:\n",
    "\n",
    "                try:\n",
    "                    runParameterSearch_Collaborative(recommender_class=recommender_class,\n",
    "                                                     URM_train=URM_train,\n",
    "                                                     metric_to_optimize=metric_to_optimize,\n",
    "                                                     evaluator_validation=evaluator_validation,\n",
    "                                                     evaluator_test=evaluator_test,\n",
    "                                                     evaluator_validation_earlystopping=evaluator_validation_earlystopping,\n",
    "                                                     output_folder_path=output_folder_path,\n",
    "                                                     n_cases=n_cases,\n",
    "                                                     n_random_starts=n_random_starts,\n",
    "                                                     save_model=save_model,\n",
    "                                                     allow_weighting=allow_weighting,\n",
    "                                                     similarity_type_list=similarity_type_list)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"On recommender {} Exception {}\".format(recommender_class, str(e)))\n",
    "                    traceback.print_exc()\n",
    "\n",
    "                # Load best_parameters for training\n",
    "                data_loader = DataIO(folder_path=output_folder_path)\n",
    "                search_metadata = data_loader.load_data(output_file_name_root)\n",
    "                best_parameters = search_metadata[\"hyperparameters_best\"]  # dictionary with all the fit parameters\n",
    "                print(\"best_parameters {}\".format(best_parameters))\n",
    "\n",
    "        else:\n",
    "            best_parameters =  {'sgd_mode': 'adagrad', 'epochs': 200, 'num_factors': 177, 'batch_size': 4, \n",
    "                                'positive_reg': 2.3859950782265896e-05, 'negative_reg': 7.572911338047984e-05, 'learning_rate': 0.0005586331284886803}\n",
    "        \n",
    "        # Fit the recommender with the parameters we just learned\n",
    "        recommender = recommender_class(URM_train)\n",
    "        recommender.fit(**best_parameters)\n",
    "            \n",
    "        # Evaluate model\n",
    "        evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])\n",
    "        result_dict, _ = evaluator_test.evaluateRecommender(recommender)\n",
    "        print(\"{} result_dict MAP {}\".format(recommender_class.RECOMMENDER_NAME, result_dict[10][\"MAP\"]))\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = input('\\nCompute and save top10 predictions?: y - Yes  n - No\\n')\n",
    "\n",
    "        if predictions == 'y':\n",
    "            # Train the model on the whole dataset using tuned params\n",
    "            if recommender_class in content_algorithm_list:\n",
    "                # todo: ICM_all or ICM_train?\n",
    "                recommender = recommender_class(URM_all, ICM_all)\n",
    "            else:\n",
    "                recommender = recommender_class(URM_all)\n",
    "\n",
    "            recommender.fit(**best_parameters)\n",
    "\n",
    "            top_10_items = {}\n",
    "            target_user_id_list = get_target_users()\n",
    "\n",
    "            for user_id in target_user_id_list:\n",
    "                item_list = ''\n",
    "                for item in range(10):  # recommended_items\n",
    "                    item_list = recommender.recommend(user_id, cutoff=10)\n",
    "                    item_list = np.array(item_list)  # list to np.array\n",
    "                    top_10_items[user_id] = item_list  # .strip() # remove trailing space\n",
    "\n",
    "            # save predictions on csv file\n",
    "            create_csv(top_10_items, recommender_class.RECOMMENDER_NAME)\n",
    "\n",
    "        break\n",
    "\n",
    "    except (ValueError, IndexError):\n",
    "        print('Error. Please enter number between 1 and {}'.format(i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
