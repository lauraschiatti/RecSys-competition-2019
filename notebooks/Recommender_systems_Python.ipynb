{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport scipy.sparse as sps\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndataset_dir = \"../input/dataset/\"\n\n# Interactions files (URM)\ndata_train = dataset_dir + \"data_train.csv\"\ndata_target_users = dataset_dir + \"data_target_users_test.csv\"\n\n# Item content files (ICM)\ndata_ICM_asset = dataset_dir + \"/data_ICM_asset.csv\"  # description of the item (id)\ndata_ICM_price = dataset_dir + \"/data_ICM_price.csv\"  # price of each item (already normalized)\ndata_ICM_sub_class = dataset_dir + \"/data_ICM_sub_class.csv\"  # categorization of the item (number)\n\n# Any results you write to the current directory are saved as output.\n","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/dataset/data_UCM_age.csv\n/kaggle/input/dataset/data_ICM_sub_class.csv\n/kaggle/input/dataset/data_ICM_asset.csv\n/kaggle/input/dataset/data_ICM_price.csv\n/kaggle/input/dataset/data_UCM_region.csv\n/kaggle/input/dataset/data_target_users_test.csv\n/kaggle/input/dataset/data_train.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"> ** Load data and preprocess**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data_manager.py\n# ------------------------------------------------------------------\n\n# global vars\nuser_list = []\nitem_list = []\nn_interactions = 0\nn_users = 0\nn_items = 0\nn_subclass = 0\n\n\ndef build_URM():\n    global user_list, item_list, n_interactions\n\n    matrix_tuples = []\n\n    with open(data_train, 'r') as file:  # read file's content\n        next(file)  # skip header row\n        for line in file:\n            if len(line.strip()) != 0:  # ignore lines with only whitespace\n                n_interactions += 1\n\n                # Create a tuple for each interaction (line in the file)\n                matrix_tuples.append(row_split(line))\n\n    # Separate user_id, item_id and rating\n    user_list, item_list, rating_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n\n    # Create lists of all users, items and contents (ratings)\n    user_list = list(user_list)  # row\n    item_list = list(item_list)  # col\n    rating_list = list(rating_list)  # data\n\n    URM = csr_sparse_matrix(rating_list, user_list, item_list)\n    \n    print(\"URM built!\")\n\n    return URM\n\ndef build_ICM():\n    # features = [‘asset’, ’price’, ’subclass’] info about products\n    global n_subclass\n\n    # Load subclass data\n    matrix_tuples = []\n\n    with open(data_ICM_sub_class, 'r') as file:  # read file's content\n        next(file)  # skip header row\n        for line in file:\n            n_subclass += 1\n\n            # Create a tuple for each interaction (line in the file)\n            matrix_tuples.append(row_split(line))\n\n    # Separate user_id, item_id and rating\n    item_list, class_list, col_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n\n    # Convert values to list# Create lists of all users, items and contents (ratings)\n    item_list_icm = list(item_list)\n    class_list_icm = list(class_list)\n    col_list_icm = np.zeros(len(col_list))\n\n    # Number of items that are in the subclass list\n    num_items = max(item_list_icm) + 1\n    ICM_shape = (num_items, 1)\n    ICM_subclass = csr_sparse_matrix(class_list_icm, item_list_icm, col_list_icm, shape=ICM_shape)\n\n    # Load price data\n    matrix_tuples = []\n    n_prices = 0\n\n    with open(data_ICM_price, 'r') as file:  # read file's content\n        next(file)  # skip header row\n        for line in file:\n            n_prices += 1\n\n            # Create a tuple for each interaction (line in the file)\n            matrix_tuples.append(row_split(line))\n\n    # Separate user_id, item_id and rating\n    item_list, col_list, price_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n\n    # Convert values to list# Create lists of all users, items and contents (ratings)\n    item_list_icm = list(item_list)\n    col_list_icm = list(col_list)\n    price_list_icm = list(price_list)\n\n    ICM_price = csr_sparse_matrix(price_list_icm, item_list_icm, col_list_icm)\n\n    # Load asset data\n    matrix_tuples = []\n    n_assets = 0\n\n    with open(data_ICM_asset, 'r') as file:  # read file's content\n        next(file)  # skip header row\n        for line in file:\n            n_assets += 1\n\n            # Create a tuple for each interaction (line in the file)\n            matrix_tuples.append(row_split(line))\n\n    # Separate user_id, item_id and rating\n    item_list, col_list, asset_list = zip(*matrix_tuples)  # join tuples together (zip() to map values)\n\n    # Convert values to list# Create lists of all users, items and contents (ratings)\n    item_list_icm = list(item_list)\n    col_list_icm = list(col_list)\n    asset_list_icm = list(asset_list)\n\n    ICM_asset = csr_sparse_matrix(asset_list_icm, item_list_icm, col_list_icm)\n\n    ICM_all = sps.hstack([ICM_price, ICM_asset, ICM_subclass], format='csr')\n\n    # item_feature_ratios(ICM_all)\n\n    print(\"ICM built!\\n\")\n\n    return ICM_all\n\ndef row_split(row_string):\n    # file format: 0,3568,1.0\n\n    split = row_string.split(\",\")\n    split[2] = split[2].replace(\"\\n\", \"\")\n\n    split[0] = int(split[0])\n    split[1] = int(split[1])\n    split[2] = float(split[2])  # rating is a float\n\n    result = tuple(split)\n    return result\n\ndef csr_sparse_matrix(data, row, col, shape=None):\n    csr_matrix = sps.coo_matrix((data, (row, col)), shape=shape)\n    csr_matrix = csr_matrix.tocsr()\n\n    return csr_matrix\n\n\ndef get_target_users():\n    target_user_id_list = []\n\n    with open(data_target_users, 'r') as file:  # read file's content\n        next(file)  # skip header row\n        for line in file:\n            # each line is a user_id\n            target_user_id_list.append(int(line.strip()))  # remove trailing space\n\n    return target_user_id_list","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Train, validation and test splitting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data_splitter.py\n# ------------------------------------------------------------------\n\n# Random holdout split: take interactions randomly\n# and do not care about which users were involved in that interaction\n\ndef split_train_validation_random_holdout(URM, train_split):\n    number_interactions = URM.nnz  # number of nonzero values\n    URM = URM.tocoo()  # Coordinate list matrix (COO)\n    shape = URM.shape\n\n    #  URM.row: user_list, URM.col: item_list, URM.data: rating_list\n\n    # Sampling strategy: take random samples of data using a boolean mask\n    train_mask = np.random.choice(\n        [True, False],\n        number_interactions,\n        p=[train_split, 1 - train_split])  # train_perc for True, 1-train_perc for False\n\n    URM_train = csr_sparse_matrix(URM.data[train_mask],\n                                  URM.row[train_mask],\n                                  URM.col[train_mask],\n                                  shape=shape)\n\n    test_mask = np.logical_not(train_mask)  # remaining samples\n    URM_test = csr_sparse_matrix(URM.data[test_mask],\n                                 URM.row[test_mask],\n                                 URM.col[test_mask],\n                                 shape=shape)\n\n    return URM_train, URM_test\n\n","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Performance evaluation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Incremental_Training_Early_Stopping.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 06/07/2018\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\nimport time, sys\nimport numpy as np\n# from Base.BaseTempFolder import BaseTempFolder\n# from utils.Evaluation.Utils.seconds_to_biggest_unit import seconds_to_biggest_unit\n\n\nclass Incremental_Training_Early_Stopping(object):\n    \"\"\"\n    This class provides a function which trains a model applying early stopping\n    The term \"incremental\" refers to the model that is updated at every epoch\n    The term \"best\" refers to the incremental model which corresponded to the best validation score\n    The object must implement the following methods:\n    _run_epoch(self, num_epoch)                 : trains the model for one epoch (e.g. calling another object implementing the training cython, pyTorch...)\n    _prepare_model_for_validation(self)         : ensures the recommender being trained can compute the predictions needed for the validation step\n    _update_best_model(self)                    : updates the best model with the current incremental one\n    _train_with_early_stopping(.)               : Function that executes the training, validation and early stopping by using the previously implemented functions\n    \"\"\"\n\n    def __init__(self):\n        super(Incremental_Training_Early_Stopping, self).__init__()\n\n\n    def get_early_stopping_final_epochs_dict(self):\n        \"\"\"\n        This function returns a dictionary to be used as optimal parameters in the .fit() function\n        It provides the flexibility to deal with multiple early-stopping in a single algorithm\n        e.g. in NeuMF there are three model components each with its own optimal number of epochs\n        the return dict would be {\"epochs\": epochs_best_neumf, \"epochs_gmf\": epochs_best_gmf, \"epochs_mlp\": epochs_best_mlp}\n        :return:\n        \"\"\"\n\n        return {\"epochs\": self.epochs_best}\n\n\n    def _run_epoch(self, num_epoch):\n        \"\"\"\n        This function should run a single epoch on the object you train. This may either involve calling a function to do an epoch\n        on a Cython object or a loop on the data points directly in python\n        :param num_epoch:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\n\n    def _prepare_model_for_validation(self):\n        \"\"\"\n        This function is executed before the evaluation of the current model\n        It should ensure the current object \"self\" can be passed to the evaluator object\n        E.G. if the epoch is done via Cython or PyTorch, this function should get the new parameter values from\n        the cython or pytorch objects into the self. pyhon object\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\n\n    def _update_best_model(self):\n        \"\"\"\n        This function is called when the incremental model is found to have better validation score than the current best one\n        So the current best model should be replaced by the current incremental one.\n        Important, remember to clone the objects and NOT to create a pointer-reference, otherwise the best solution will be altered\n        by the next epoch\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\n\n\n    def _train_with_early_stopping(self, epochs_max, epochs_min = 0,\n                                   validation_every_n = None, stop_on_validation = False,\n                                   validation_metric = None, lower_validations_allowed = None, evaluator_object = None,\n                                   algorithm_name = \"Incremental_Training_Early_Stopping\"):\n        \"\"\"\n        :param epochs_max:                  max number of epochs the training will last\n        :param epochs_min:                  min number of epochs the training will last\n        :param validation_every_n:          number of epochs after which the model will be evaluated and a best_model selected\n        :param stop_on_validation:          [True/False] whether to stop the training before the max number of epochs\n        :param validation_metric:           which metric to use when selecting the best model, higher values are better\n        :param lower_validations_allowed:    number of contiguous validation steps required for the tranining to early-stop\n        :param evaluator_object:            evaluator instance used to compute the validation metrics.\n                                                If multiple cutoffs are available, the first one is used\n        :param algorithm_name:              name of the algorithm to be displayed in the output updates\n        :return: -\n        Supported uses:\n        - Train for max number of epochs with no validation nor early stopping:\n            _train_with_early_stopping(epochs_max = 100,\n                                        evaluator_object = None\n                                        epochs_min,                 not used\n                                        validation_every_n,         not used\n                                        stop_on_validation,         not used\n                                        validation_metric,          not used\n                                        lower_validations_allowed,   not used\n                                        )\n        - Train for max number of epochs with validation but NOT early stopping:\n            _train_with_early_stopping(epochs_max = 100,\n                                        evaluator_object = evaluator\n                                        stop_on_validation = False\n                                        validation_every_n = int value\n                                        validation_metric = metric name string\n                                        epochs_min,                 not used\n                                        lower_validations_allowed,   not used\n                                        )\n        - Train for max number of epochs with validation AND early stopping:\n            _train_with_early_stopping(epochs_max = 100,\n                                        epochs_min = int value\n                                        evaluator_object = evaluator\n                                        stop_on_validation = True\n                                        validation_every_n = int value\n                                        validation_metric = metric name string\n                                        lower_validations_allowed = int value\n                                        )\n        \"\"\"\n\n        assert epochs_max >= 0, \"{}: Number of epochs_max must be >= 0, passed was {}\".format(algorithm_name, epochs_max)\n        assert epochs_min >= 0, \"{}: Number of epochs_min must be >= 0, passed was {}\".format(algorithm_name, epochs_min)\n        assert epochs_min <= epochs_max, \"{}: epochs_min must be <= epochs_max, passed are epochs_min {}, epochs_max {}\".format(algorithm_name, epochs_min, epochs_max)\n\n        # Train for max number of epochs with no validation nor early stopping\n        # OR Train for max number of epochs with validation but NOT early stopping\n        # OR Train for max number of epochs with validation AND early stopping\n        assert evaluator_object is None or\\\n               (evaluator_object is not None and not stop_on_validation and validation_every_n is not None and validation_metric is not None) or\\\n               (evaluator_object is not None and stop_on_validation and validation_every_n is not None and validation_metric is not None and lower_validations_allowed is not None),\\\n            \"{}: Inconsistent parameters passed, please check the supported uses\".format(algorithm_name)\n\n\n\n\n        start_time = time.time()\n\n        self.best_validation_metric = None\n        lower_validatons_count = 0\n        convergence = False\n\n        self.epochs_best = 0\n\n        epochs_current = 0\n\n        while epochs_current < epochs_max and not convergence:\n\n            self._run_epoch(epochs_current)\n\n            # If no validation required, always keep the latest\n            if evaluator_object is None:\n\n                self.epochs_best = epochs_current\n\n            # Determine whether a validaton step is required\n            elif (epochs_current + 1) % validation_every_n == 0:\n\n                print(\"{}: Validation begins...\".format(algorithm_name))\n\n                self._prepare_model_for_validation()\n\n                # If the evaluator validation has multiple cutoffs, choose the first one\n                results_run, results_run_string = evaluator_object.evaluateRecommender(self)\n                results_run = results_run[list(results_run.keys())[0]]\n\n                print(\"{}: {}\".format(algorithm_name, results_run_string))\n\n                # Update optimal model\n                current_metric_value = results_run[validation_metric]\n                #\n                # if not np.isfinite(current_metric_value):\n                #     if isinstance(self, BaseTempFolder):\n                #         # If the recommender uses BaseTempFolder, clean the temp folder\n                #         self._clean_temp_folder(temp_file_folder=self.temp_file_folder)\n                #\n                #     assert False, \"{}: metric value is not a finite number, terminating!\".format(self.RECOMMENDER_NAME)\n                #\n\n                if self.best_validation_metric is None or self.best_validation_metric < current_metric_value:\n\n                    print(\"{}: New best model found! Updating.\".format(algorithm_name))\n\n                    self.best_validation_metric = current_metric_value\n\n                    self._update_best_model()\n\n                    self.epochs_best = epochs_current +1\n                    lower_validatons_count = 0\n\n                else:\n                    lower_validatons_count += 1\n\n\n                if stop_on_validation and lower_validatons_count >= lower_validations_allowed and epochs_current >= epochs_min:\n                    convergence = True\n\n                    elapsed_time = time.time() - start_time\n                    new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n\n                    print(\"{}: Convergence reached! Terminating at epoch {}. Best value for '{}' at epoch {} is {:.4f}. Elapsed time {:.2f} {}\".format(\n                        algorithm_name, epochs_current+1, validation_metric, self.epochs_best, self.best_validation_metric, new_time_value, new_time_unit))\n\n\n            elapsed_time = time.time() - start_time\n            new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n\n            print(\"{}: Epoch {} of {}. Elapsed time {:.2f} {}\".format(\n                algorithm_name, epochs_current+1, epochs_max, new_time_value, new_time_unit))\n\n            epochs_current += 1\n\n            sys.stdout.flush()\n            sys.stderr.flush()\n\n        # If no validation required, keep the latest\n        if evaluator_object is None:\n\n            self._prepare_model_for_validation()\n            self._update_best_model()\n\n\n        # Stop when max epochs reached and not early-stopping\n        if not convergence:\n            elapsed_time = time.time() - start_time\n            new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n\n            if evaluator_object is not None and self.best_validation_metric is not None:\n                print(\"{}: Terminating at epoch {}. Best value for '{}' at epoch {} is {:.4f}. Elapsed time {:.2f} {}\".format(\n                    algorithm_name, epochs_current, validation_metric, self.epochs_best, self.best_validation_metric, new_time_value, new_time_unit))\n            else:\n                print(\"{}: Terminating at epoch {}. Elapsed time {:.2f} {}\".format(\n                    algorithm_name, epochs_current, new_time_value, new_time_unit))\n\n                \n        ","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluator.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 26/06/18\n\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\nimport numpy as np\nimport scipy.sparse as sps\nimport time, sys, copy\n\nfrom enum import Enum\n# from utils.Evaluation.Utils.seconds_to_biggest_unit import seconds_to_biggest_unit\n\n# from utils.Evaluation.metrics import roc_auc, precision, precision_recall_min_denominator, recall, MAP, MRR, ndcg, arhr, \\\n#     rmse, \\\n#     Novelty, Coverage_Item, Metrics_Object, Coverage_User, Gini_Diversity, Shannon_Entropy, Diversity_MeanInterList, \\\n#     Diversity_Herfindahl, AveragePopularity\n\n\nclass EvaluatorMetrics(Enum):\n    ROC_AUC = \"ROC_AUC\"\n    PRECISION = \"PRECISION\"\n    PRECISION_RECALL_MIN_DEN = \"PRECISION_RECALL_MIN_DEN\"\n    RECALL = \"RECALL\"\n    MAP = \"MAP\"\n    MRR = \"MRR\"\n    NDCG = \"NDCG\"\n    F1 = \"F1\"\n    HIT_RATE = \"HIT_RATE\"\n    ARHR = \"ARHR\"\n    RMSE = \"RMSE\"\n    NOVELTY = \"NOVELTY\"\n    AVERAGE_POPULARITY = \"AVERAGE_POPULARITY\"\n    DIVERSITY_SIMILARITY = \"DIVERSITY_SIMILARITY\"\n    DIVERSITY_MEAN_INTER_LIST = \"DIVERSITY_MEAN_INTER_LIST\"\n    DIVERSITY_HERFINDAHL = \"DIVERSITY_HERFINDAHL\"\n    COVERAGE_ITEM = \"COVERAGE_ITEM\"\n    COVERAGE_USER = \"COVERAGE_USER\"\n    DIVERSITY_GINI = \"DIVERSITY_GINI\"\n    SHANNON_ENTROPY = \"SHANNON_ENTROPY\"\n\n\ndef create_empty_metrics_dict(n_items, n_users, URM_train, ignore_items, ignore_users, cutoff,\n                              diversity_similarity_object):\n    empty_dict = {}\n\n    # from Base.Evaluation.ResultMetric import ResultMetric\n    # empty_dict = ResultMetric()\n\n    for metric in EvaluatorMetrics:\n        if metric == EvaluatorMetrics.COVERAGE_ITEM:\n            empty_dict[metric.value] = Coverage_Item(n_items, ignore_items)\n\n        elif metric == EvaluatorMetrics.DIVERSITY_GINI:\n            empty_dict[metric.value] = Gini_Diversity(n_items, ignore_items)\n\n        elif metric == EvaluatorMetrics.SHANNON_ENTROPY:\n            empty_dict[metric.value] = Shannon_Entropy(n_items, ignore_items)\n\n        elif metric == EvaluatorMetrics.COVERAGE_USER:\n            empty_dict[metric.value] = Coverage_User(n_users, ignore_users)\n\n        elif metric == EvaluatorMetrics.DIVERSITY_MEAN_INTER_LIST:\n            empty_dict[metric.value] = Diversity_MeanInterList(n_items, cutoff)\n\n        elif metric == EvaluatorMetrics.DIVERSITY_HERFINDAHL:\n            empty_dict[metric.value] = Diversity_Herfindahl(n_items, ignore_items)\n\n        elif metric == EvaluatorMetrics.NOVELTY:\n            empty_dict[metric.value] = Novelty(URM_train)\n\n        elif metric == EvaluatorMetrics.AVERAGE_POPULARITY:\n            empty_dict[metric.value] = AveragePopularity(URM_train)\n\n        elif metric == EvaluatorMetrics.MAP:\n            empty_dict[metric.value] = MAP()\n\n        elif metric == EvaluatorMetrics.MRR:\n            empty_dict[metric.value] = MRR()\n\n        elif metric == EvaluatorMetrics.DIVERSITY_SIMILARITY:\n            if diversity_similarity_object is not None:\n                empty_dict[metric.value] = copy.deepcopy(diversity_similarity_object)\n        else:\n            empty_dict[metric.value] = 0.0\n\n    return empty_dict\n\n\ndef get_result_string(results_run, n_decimals=7):\n    output_str = \"\"\n\n    for cutoff in results_run.keys():\n\n        results_run_current_cutoff = results_run[cutoff]\n\n        output_str += \"CUTOFF: {} - \".format(cutoff)\n\n        for metric in results_run_current_cutoff.keys():\n            output_str += \"{}: {:.{n_decimals}f}, \".format(metric, results_run_current_cutoff[metric],\n                                                           n_decimals=n_decimals)\n        output_str += \"\\n\"\n\n    return output_str\n\n\ndef _remove_item_interactions(URM, item_list):\n    URM = sps.csc_matrix(URM.copy())\n\n    for item_index in item_list:\n        start_pos = URM.indptr[item_index]\n        end_pos = URM.indptr[item_index + 1]\n\n        URM.data[start_pos:end_pos] = np.zeros_like(URM.data[start_pos:end_pos])\n\n    URM.eliminate_zeros()\n    URM = sps.csr_matrix(URM)\n\n    return URM\n\n\nclass Evaluator(object):\n    \"\"\"Abstract Evaluator\"\"\"\n\n    EVALUATOR_NAME = \"Evaluator_Base_Class\"\n\n    def __init__(self, URM_test_list, cutoff_list, minRatingsPerUser=1, exclude_seen=True,\n                 diversity_object=None,\n                 ignore_items=None,\n                 ignore_users=None,\n                 verbose=True):\n\n        super(Evaluator, self).__init__()\n\n        self.verbose = verbose\n\n        if ignore_items is None:\n            self.ignore_items_flag = False\n            self.ignore_items_ID = np.array([])\n        else:\n            self._print(\"Ignoring {} Items\".format(len(ignore_items)))\n            self.ignore_items_flag = True\n            self.ignore_items_ID = np.array(ignore_items)\n\n        self.cutoff_list = cutoff_list.copy()\n        self.max_cutoff = max(self.cutoff_list)\n\n        self.minRatingsPerUser = minRatingsPerUser\n        self.exclude_seen = exclude_seen\n\n        if not isinstance(URM_test_list, list):\n            self.URM_test = URM_test_list.copy()\n            URM_test_list = [URM_test_list]\n        else:\n            raise ValueError(\"List of URM_test not supported\")\n\n        self.diversity_object = diversity_object\n\n        self.n_users, self.n_items = URM_test_list[0].shape\n\n        # Prune users with an insufficient number of ratings\n        # During testing CSR is faster\n        self.URM_test_list = []\n        usersToEvaluate_mask = np.zeros(self.n_users, dtype=np.bool)\n\n        for URM_test in URM_test_list:\n            URM_test = _remove_item_interactions(URM_test, self.ignore_items_ID)\n\n            URM_test = sps.csr_matrix(URM_test)\n            self.URM_test_list.append(URM_test)\n\n            rows = URM_test.indptr\n            numRatings = np.ediff1d(rows)\n            new_mask = numRatings >= minRatingsPerUser\n\n            usersToEvaluate_mask = np.logical_or(usersToEvaluate_mask, new_mask)\n\n        self.usersToEvaluate = np.arange(self.n_users)[usersToEvaluate_mask]\n\n        if ignore_users is not None:\n            self._print(\"Ignoring {} Users\".format(len(ignore_users)))\n            self.ignore_users_ID = np.array(ignore_users)\n            self.usersToEvaluate = set(self.usersToEvaluate) - set(ignore_users)\n        else:\n            self.ignore_users_ID = np.array([])\n\n        self.usersToEvaluate = list(self.usersToEvaluate)\n\n    def _print(self, string):\n\n        if self.verbose:\n            print(\"{}: {}\".format(self.EVALUATOR_NAME, string))\n\n    def evaluateRecommender(self, recommender_object):\n        \"\"\"\n        :param recommender_object: the trained recommender object, a BaseRecommender subclass\n        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n        \"\"\"\n\n        raise NotImplementedError(\"The method evaluateRecommender not implemented for this evaluator class\")\n\n    def get_user_relevant_items(self, user_id):\n\n        assert self.URM_test.getformat() == \"csr\", \"Evaluator_Base_Class: URM_test is not CSR, this will cause errors in getting relevant items\"\n\n        return self.URM_test.indices[self.URM_test.indptr[user_id]:self.URM_test.indptr[user_id + 1]]\n\n    def get_user_test_ratings(self, user_id):\n\n        assert self.URM_test.getformat() == \"csr\", \"Evaluator_Base_Class: URM_test is not CSR, this will cause errors in relevant items ratings\"\n\n        return self.URM_test.data[self.URM_test.indptr[user_id]:self.URM_test.indptr[user_id + 1]]\n\n\nclass EvaluatorHoldout(Evaluator):\n    \"\"\"EvaluatorHoldout\"\"\"\n\n    EVALUATOR_NAME = \"EvaluatorHoldout\"\n\n    def __init__(self, URM_test_list, cutoff_list, minRatingsPerUser=1, exclude_seen=True,\n                 diversity_object=None,\n                 ignore_items=None,\n                 ignore_users=None,\n                 verbose=True):\n\n        super(EvaluatorHoldout, self).__init__(URM_test_list, cutoff_list,\n                                               diversity_object=diversity_object,\n                                               minRatingsPerUser=minRatingsPerUser, exclude_seen=exclude_seen,\n                                               ignore_items=ignore_items, ignore_users=ignore_users,\n                                               verbose=verbose)\n\n    def _run_evaluation_on_selected_users(self, recommender_object, usersToEvaluate, block_size=None):\n\n        if block_size is None:\n            block_size = min(1000, int(1e8 / self.n_items))\n            block_size = min(block_size, len(usersToEvaluate))\n\n        start_time = time.time()\n        start_time_print = time.time()\n\n        results_dict = {}\n\n        for cutoff in self.cutoff_list:\n            results_dict[cutoff] = create_empty_metrics_dict(self.n_items, self.n_users,\n                                                             recommender_object.get_URM_train(),\n                                                             self.ignore_items_ID,\n                                                             self.ignore_users_ID,\n                                                             cutoff,\n                                                             self.diversity_object)\n\n        if self.ignore_items_flag:\n            recommender_object.set_items_to_ignore(self.ignore_items_ID)\n\n        n_users_evaluated = 0\n\n        # Start from -block_size to ensure it to be 0 at the first block\n        user_batch_start = 0\n        user_batch_end = 0\n\n        while user_batch_start < len(usersToEvaluate):\n\n            user_batch_end = user_batch_start + block_size\n            user_batch_end = min(user_batch_end, len(usersToEvaluate))\n\n            test_user_batch_array = np.array(usersToEvaluate[user_batch_start:user_batch_end])\n            user_batch_start = user_batch_end\n\n            # Compute predictions for a batch of users using vectorization, much more efficient than computing it one at a time\n            recommended_items_batch_list, scores_batch = recommender_object.recommend(test_user_batch_array,\n                                                                                      remove_seen_flag=self.exclude_seen,\n                                                                                      cutoff=self.max_cutoff,\n                                                                                      remove_top_pop_flag=False,\n                                                                                      remove_custom_items_flag=self.ignore_items_flag,\n                                                                                      return_scores=True\n                                                                                      )\n\n            assert len(recommended_items_batch_list) == len(\n                test_user_batch_array), \"{}: recommended_items_batch_list contained recommendations for {} users, expected was {}\".format(\n                self.EVALUATOR_NAME, len(recommended_items_batch_list), len(test_user_batch_array))\n\n            assert scores_batch.shape[0] == len(\n                test_user_batch_array), \"{}: scores_batch contained scores for {} users, expected was {}\".format(\n                self.EVALUATOR_NAME, scores_batch.shape[0], len(test_user_batch_array))\n\n            assert scores_batch.shape[\n                       1] == self.n_items, \"{}: scores_batch contained scores for {} items, expected was {}\".format(\n                self.EVALUATOR_NAME, scores_batch.shape[1], self.n_items)\n\n            # Compute recommendation quality for each user in batch\n            for batch_user_index in range(len(recommended_items_batch_list)):\n\n                test_user = test_user_batch_array[batch_user_index]\n\n                relevant_items = self.get_user_relevant_items(test_user)\n                relevant_items_rating = self.get_user_test_ratings(test_user)\n\n                all_items_predicted_ratings = scores_batch[batch_user_index]\n                user_rmse = rmse(all_items_predicted_ratings, relevant_items, relevant_items_rating)\n\n                # Being the URM CSR, the indices are the non-zero column indexes\n                recommended_items = recommended_items_batch_list[batch_user_index]\n                is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n\n                n_users_evaluated += 1\n\n                for cutoff in self.cutoff_list:\n\n                    results_current_cutoff = results_dict[cutoff]\n\n                    is_relevant_current_cutoff = is_relevant[0:cutoff]\n                    recommended_items_current_cutoff = recommended_items[0:cutoff]\n\n                    results_current_cutoff[EvaluatorMetrics.ROC_AUC.value] += roc_auc(is_relevant_current_cutoff)\n                    results_current_cutoff[EvaluatorMetrics.PRECISION.value] += precision(is_relevant_current_cutoff)\n                    results_current_cutoff[\n                        EvaluatorMetrics.PRECISION_RECALL_MIN_DEN.value] += precision_recall_min_denominator(\n                        is_relevant_current_cutoff, len(relevant_items))\n                    results_current_cutoff[EvaluatorMetrics.RECALL.value] += recall(is_relevant_current_cutoff,\n                                                                                    relevant_items)\n                    results_current_cutoff[EvaluatorMetrics.NDCG.value] += ndcg(recommended_items_current_cutoff,\n                                                                                relevant_items,\n                                                                                relevance=self.get_user_test_ratings(\n                                                                                    test_user), at=cutoff)\n                    results_current_cutoff[EvaluatorMetrics.HIT_RATE.value] += is_relevant_current_cutoff.sum()\n                    results_current_cutoff[EvaluatorMetrics.ARHR.value] += arhr(is_relevant_current_cutoff)\n                    results_current_cutoff[EvaluatorMetrics.RMSE.value] += user_rmse\n\n                    results_current_cutoff[EvaluatorMetrics.MRR.value].add_recommendations(is_relevant_current_cutoff)\n                    results_current_cutoff[EvaluatorMetrics.MAP.value].add_recommendations(is_relevant_current_cutoff,\n                                                                                           relevant_items)\n                    results_current_cutoff[EvaluatorMetrics.NOVELTY.value].add_recommendations(\n                        recommended_items_current_cutoff)\n                    results_current_cutoff[EvaluatorMetrics.AVERAGE_POPULARITY.value].add_recommendations(\n                        recommended_items_current_cutoff)\n                    results_current_cutoff[EvaluatorMetrics.DIVERSITY_GINI.value].add_recommendations(\n                        recommended_items_current_cutoff)\n                    results_current_cutoff[EvaluatorMetrics.SHANNON_ENTROPY.value].add_recommendations(\n                        recommended_items_current_cutoff)\n                    results_current_cutoff[EvaluatorMetrics.COVERAGE_ITEM.value].add_recommendations(\n                        recommended_items_current_cutoff)\n                    results_current_cutoff[EvaluatorMetrics.COVERAGE_USER.value].add_recommendations(\n                        recommended_items_current_cutoff, test_user)\n                    results_current_cutoff[EvaluatorMetrics.DIVERSITY_MEAN_INTER_LIST.value].add_recommendations(\n                        recommended_items_current_cutoff)\n                    results_current_cutoff[EvaluatorMetrics.DIVERSITY_HERFINDAHL.value].add_recommendations(\n                        recommended_items_current_cutoff)\n\n                    if EvaluatorMetrics.DIVERSITY_SIMILARITY.value in results_current_cutoff:\n                        results_current_cutoff[EvaluatorMetrics.DIVERSITY_SIMILARITY.value].add_recommendations(\n                            recommended_items_current_cutoff)\n\n                if time.time() - start_time_print > 30 or n_users_evaluated == len(self.usersToEvaluate):\n                    elapsed_time = time.time() - start_time\n                    new_time_value, new_time_unit = seconds_to_biggest_unit(elapsed_time)\n\n                    self._print(\"Processed {} ( {:.2f}% ) in {:.2f} {}. Users per second: {:.0f}\".format(\n                        n_users_evaluated,\n                        100.0 * float(n_users_evaluated) / len(self.usersToEvaluate),\n                        new_time_value, new_time_unit,\n                        float(n_users_evaluated) / elapsed_time))\n\n                    sys.stdout.flush()\n                    sys.stderr.flush()\n\n                    start_time_print = time.time()\n\n        return results_dict, n_users_evaluated\n\n    def evaluateRecommender(self, recommender_object):\n        \"\"\"\n        :param recommender_object: the trained recommender object, a BaseRecommender subclass\n        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n        \"\"\"\n\n        if self.ignore_items_flag:\n            recommender_object.set_items_to_ignore(self.ignore_items_ID)\n\n        results_dict, n_users_evaluated = self._run_evaluation_on_selected_users(recommender_object,\n                                                                                 self.usersToEvaluate)\n\n        if (n_users_evaluated > 0):\n\n            for cutoff in self.cutoff_list:\n\n                results_current_cutoff = results_dict[cutoff]\n\n                for key in results_current_cutoff.keys():\n\n                    value = results_current_cutoff[key]\n\n                    if isinstance(value, Metrics_Object):\n                        results_current_cutoff[key] = value.get_metric_value()\n                    else:\n                        results_current_cutoff[key] = value / n_users_evaluated\n\n                precision_ = results_current_cutoff[EvaluatorMetrics.PRECISION.value]\n                recall_ = results_current_cutoff[EvaluatorMetrics.RECALL.value]\n\n                if precision_ + recall_ != 0:\n                    # F1 micro averaged: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n                    results_current_cutoff[EvaluatorMetrics.F1.value] = 2 * (precision_ * recall_) / (\n                            precision_ + recall_)\n        else:\n            self._print(\"WARNING: No users had a sufficient number of relevant items\")\n\n        results_run_string = get_result_string(results_dict)\n\n        if self.ignore_items_flag:\n            recommender_object.reset_items_to_ignore()\n\n        return (results_dict, results_run_string)\n    \n    \n    \n# metrics.py \n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n@author: Maurizio Ferrari Dacrema, Massimo Quadrana\n\"\"\"\n\nimport numpy as np\nimport unittest\n\nclass Metrics_Object(object):\n    \"\"\"\n    Abstract class that should be used as superclass of all metrics requiring an object, therefore a state, to be computed\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def add_recommendations(self, recommended_items_ids):\n        raise NotImplementedError()\n\n    def get_metric_value(self):\n        raise NotImplementedError()\n\n    def merge_with_other(self, other_metric_object):\n        raise NotImplementedError()\n\n\nclass Coverage_Item(Metrics_Object):\n    \"\"\"\n    Item coverage represents the percentage of the overall items which were recommended\n    https://gab41.lab41.org/recommender-systems-its-not-all-about-the-accuracy-562c7dceeaff\n    \"\"\"\n\n    def __init__(self, n_items, ignore_items):\n        super(Coverage_Item, self).__init__()\n        self.recommended_mask = np.zeros(n_items, dtype=np.bool)\n        self.n_ignore_items = len(ignore_items)\n\n    def add_recommendations(self, recommended_items_ids):\n        if len(recommended_items_ids) > 0:\n            self.recommended_mask[recommended_items_ids] = True\n\n    def get_metric_value(self):\n        return self.recommended_mask.sum()/(len(self.recommended_mask)-self.n_ignore_items)\n\n\n    def merge_with_other(self, other_metric_object):\n        assert other_metric_object is Coverage_Item, \"Coverage_Item: attempting to merge with a metric object of different type\"\n\n        self.recommended_mask = np.logical_or(self.recommended_mask, other_metric_object.recommended_mask)\n\nclass Coverage_User(Metrics_Object):\n    \"\"\"\n    User coverage represents the percentage of the overall users for which we can make recommendations.\n    If there is at least one recommendation the user is considered as covered\n    https://gab41.lab41.org/recommender-systems-its-not-all-about-the-accuracy-562c7dceeaff\n    \"\"\"\n\n    def __init__(self, n_users, ignore_users):\n        super(Coverage_User, self).__init__()\n        self.users_mask = np.zeros(n_users, dtype=np.bool)\n        self.n_ignore_users = len(ignore_users)\n\n    def add_recommendations(self, recommended_items_ids, user_id):\n        self.users_mask[user_id] = len(recommended_items_ids)>0\n\n    def get_metric_value(self):\n        return self.users_mask.sum()/(len(self.users_mask)-self.n_ignore_users)\n\n    def merge_with_other(self, other_metric_object):\n        assert other_metric_object is Coverage_User, \"Coverage_User: attempting to merge with a metric object of different type\"\n\n        self.users_mask = np.logical_or(self.users_mask, other_metric_object.users_mask)\n\nclass MAP(Metrics_Object):\n    \"\"\"\n    Mean Average Precision, defined as the mean of the AveragePrecision over all users\n    \"\"\"\n\n    def __init__(self):\n        super(MAP, self).__init__()\n        self.cumulative_AP = 0.0\n        self.n_users = 0\n\n    def add_recommendations(self, is_relevant, pos_items):\n        self.cumulative_AP += average_precision(is_relevant, pos_items)\n        self.n_users += 1\n\n    def get_metric_value(self):\n        return self.cumulative_AP/self.n_users\n\n    def merge_with_other(self, other_metric_object):\n        assert other_metric_object is MAP, \"MAP: attempting to merge with a metric object of different type\"\n\n        self.cumulative_AP += other_metric_object.cumulative_AP\n        self.n_users += other_metric_object.n_users\n\nclass MRR(Metrics_Object):\n    \"\"\"\n    Mean Reciprocal Rank, defined as the mean of the Reciprocal Rank over all users\n    \"\"\"\n\n    def __init__(self):\n        super(MRR, self).__init__()\n        self.cumulative_RR = 0.0\n        self.n_users = 0\n\n    def add_recommendations(self, is_relevant):\n        self.cumulative_RR += rr(is_relevant)\n        self.n_users += 1\n\n    def get_metric_value(self):\n        return self.cumulative_RR/self.n_users\n\n    def merge_with_other(self, other_metric_object):\n        assert other_metric_object is MAP, \"MRR: attempting to merge with a metric object of different type\"\n\n        self.cumulative_RR += other_metric_object.cumulative_RR\n        self.n_users += other_metric_object.n_users\n\nclass Gini_Diversity(Metrics_Object):\n    \"\"\"\n    Gini diversity index, computed from the Gini Index but with inverted range, such that high values mean higher diversity\n    This implementation ignores zero-occurrence items\n    # From https://github.com/oliviaguest/gini\n    # based on bottom eq: http://www.statsdirect.com/help/content/image/stat0206_wmf.gif\n    # from: http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n    #\n    # http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.8174&rep=rep1&type=pdf\n    \"\"\"\n\n    def __init__(self, n_items, ignore_items):\n        super(Gini_Diversity, self).__init__()\n        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n        self.ignore_items = ignore_items.astype(np.int).copy()\n\n    def add_recommendations(self, recommended_items_ids):\n        if len(recommended_items_ids) > 0:\n            self.recommended_counter[recommended_items_ids] += 1\n\n    def get_metric_value(self):\n\n        recommended_counter = self.recommended_counter.copy()\n\n        recommended_counter_mask = np.ones_like(recommended_counter, dtype = np.bool)\n        recommended_counter_mask[self.ignore_items] = False\n        recommended_counter_mask[recommended_counter == 0] = False\n\n        recommended_counter = recommended_counter[recommended_counter_mask]\n\n        n_items = len(recommended_counter)\n\n        recommended_counter_sorted = np.sort(recommended_counter)       # values must be sorted\n        index = np.arange(1, n_items+1)                                 # index per array element\n\n        #gini_index = (np.sum((2 * index - n_items  - 1) * recommended_counter_sorted)) / (n_items * np.sum(recommended_counter_sorted))\n        gini_diversity = 2*np.sum((n_items + 1 - index)/(n_items+1) * recommended_counter_sorted/np.sum(recommended_counter_sorted))\n\n        return gini_diversity\n\n    def merge_with_other(self, other_metric_object):\n        assert other_metric_object is Gini_Diversity, \"Gini_Diversity: attempting to merge with a metric object of different type\"\n\n        self.recommended_counter += other_metric_object.recommended_counter\n\nclass Diversity_Herfindahl(Metrics_Object):\n    \"\"\"\n    The Herfindahl index is also known as Concentration index, it is used in economy to determine whether the market quotas\n    are such that an excessive concentration exists. It is here used as a diversity index, if high means high diversity.\n    It is known to have a small value range in recommender systems, between 0.9 and 1.0\n    The Herfindahl index is a function of the square of the probability an item has been recommended to any user, hence\n    The Herfindahl index is equivalent to MeanInterList diversity as they measure the same quantity.\n    # http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.8174&rep=rep1&type=pdf\n    \"\"\"\n\n    def __init__(self, n_items, ignore_items):\n        super(Diversity_Herfindahl, self).__init__()\n        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n        self.ignore_items = ignore_items.astype(np.int).copy()\n\n    def add_recommendations(self, recommended_items_ids):\n        if len(recommended_items_ids) > 0:\n            self.recommended_counter[recommended_items_ids] += 1\n\n    def get_metric_value(self):\n\n        recommended_counter = self.recommended_counter.copy()\n\n        recommended_counter_mask = np.ones_like(recommended_counter, dtype = np.bool)\n        recommended_counter_mask[self.ignore_items] = False\n\n        recommended_counter = recommended_counter[recommended_counter_mask]\n\n        if recommended_counter.sum() != 0:\n            herfindahl_index = 1 - np.sum((recommended_counter / recommended_counter.sum()) ** 2)\n        else:\n            herfindahl_index = np.nan\n\n        return herfindahl_index\n\n    def merge_with_other(self, other_metric_object):\n        assert other_metric_object is Diversity_Herfindahl, \"Diversity_Herfindahl: attempting to merge with a metric object of different type\"\n\n        self.recommended_counter += other_metric_object.recommended_counter\n\nclass Shannon_Entropy(Metrics_Object):\n    \"\"\"\n    Shannon Entropy is a well known metric to measure the amount of information of a certain string of data.\n    Here is applied to the global number of times an item has been recommended.\n    It has a lower bound and can reach values over 12.0 for random recommenders.\n    A high entropy means that the distribution is random uniform across all users.\n    Note that while a random uniform distribution\n    (hence all items with SIMILAR number of occurrences)\n    will be highly diverse and have high entropy, a perfectly uniform distribution\n    (hence all items with EXACTLY IDENTICAL number of occurrences)\n    will have 0.0 entropy while being the most diverse possible.\n    \"\"\"\n\n    def __init__(self, n_items, ignore_items):\n        super(Shannon_Entropy, self).__init__()\n        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n        self.ignore_items = ignore_items.astype(np.int).copy()\n\n    def add_recommendations(self, recommended_items_ids):\n        if len(recommended_items_ids) > 0:\n            self.recommended_counter[recommended_items_ids] += 1\n\n    def get_metric_value(self):\n\n        assert np.all(self.recommended_counter >= 0.0), \"Shannon_Entropy: self.recommended_counter contains negative counts\"\n\n        recommended_counter = self.recommended_counter.copy()\n\n        # Ignore from the computation both ignored items and items with zero occurrence.\n        # Zero occurrence items will have zero probability and will not change the result, butt will generate nans if used in the log\n        recommended_counter_mask = np.ones_like(recommended_counter, dtype = np.bool)\n        recommended_counter_mask[self.ignore_items] = False\n        recommended_counter_mask[recommended_counter == 0] = False\n\n        recommended_counter = recommended_counter[recommended_counter_mask]\n\n        n_recommendations = recommended_counter.sum()\n\n        recommended_probability = recommended_counter/n_recommendations\n\n        shannon_entropy = -np.sum(recommended_probability * np.log2(recommended_probability))\n\n        return shannon_entropy\n\n    def merge_with_other(self, other_metric_object):\n        assert other_metric_object is Gini_Diversity, \"Shannon_Entropy: attempting to merge with a metric object of different type\"\n\n        assert np.all(self.recommended_counter >= 0.0), \"Shannon_Entropy: self.recommended_counter contains negative counts\"\n        assert np.all(other_metric_object.recommended_counter >= 0.0), \"Shannon_Entropy: other.recommended_counter contains negative counts\"\n\n        self.recommended_counter += other_metric_object.recommended_counter\n\n\nimport scipy.sparse as sps\n\nclass Novelty(Metrics_Object):\n    \"\"\"\n    Novelty measures how \"novel\" a recommendation is in terms of how popular the item was in the train set.\n    Due to this definition, the novelty of a cold item (i.e. with no interactions in the train set) is not defined,\n    in this implementation cold items are ignored and their contribution to the novelty is 0.\n    A recommender with high novelty will be able to recommend also long queue (i.e. unpopular) items.\n    Mean self-information  (Zhou 2010)\n    \"\"\"\n\n    def __init__(self, URM_train):\n        super(Novelty, self).__init__()\n\n        URM_train = sps.csc_matrix(URM_train)\n        URM_train.eliminate_zeros()\n        self.item_popularity = np.ediff1d(URM_train.indptr)\n\n        self.novelty = 0.0\n        self.n_evaluated_users = 0\n        self.n_items = len(self.item_popularity)\n        self.n_interactions = self.item_popularity.sum()\n\n\n    def add_recommendations(self, recommended_items_ids):\n\n        self.n_evaluated_users += 1\n\n        if len(recommended_items_ids)>0:\n            recommended_items_popularity = self.item_popularity[recommended_items_ids]\n\n            probability = recommended_items_popularity/self.n_interactions\n            probability = probability[probability!=0]\n\n            self.novelty += np.sum(-np.log2(probability)/self.n_items)\n\n    def get_metric_value(self):\n\n        if self.n_evaluated_users == 0:\n            return 0.0\n\n        return self.novelty/self.n_evaluated_users\n\n    def merge_with_other(self, other_metric_object):\n        assert other_metric_object is Novelty, \"Novelty: attempting to merge with a metric object of different type\"\n\n        self.novelty = self.novelty + other_metric_object.novelty\n        self.n_evaluated_users = self.n_evaluated_users + other_metric_object.n_evaluated_users\n\nclass AveragePopularity(Metrics_Object):\n    \"\"\"\n    Average popularity the recommended items have in the train data.\n    The popularity is normalized by setting as 1 the item with the highest popularity in the train data\n    \"\"\"\n\n    def __init__(self, URM_train):\n        super(AveragePopularity, self).__init__()\n\n        URM_train = sps.csc_matrix(URM_train)\n        URM_train.eliminate_zeros()\n        item_popularity = np.ediff1d(URM_train.indptr)\n\n\n        self.cumulative_popularity = 0.0\n        self.n_evaluated_users = 0\n        self.n_items = URM_train.shape[0]\n        self.n_interactions = item_popularity.sum()\n\n        self.item_popularity_normalized = item_popularity/item_popularity.max()\n\n    def add_recommendations(self, recommended_items_ids):\n\n        self.n_evaluated_users += 1\n\n        if len(recommended_items_ids)>0:\n            recommended_items_popularity = self.item_popularity_normalized[recommended_items_ids]\n\n            self.cumulative_popularity += np.sum(recommended_items_popularity)/len(recommended_items_ids)\n\n    def get_metric_value(self):\n\n        if self.n_evaluated_users == 0:\n            return 0.0\n\n        return self.cumulative_popularity/self.n_evaluated_users\n\n    def merge_with_other(self, other_metric_object):\n        assert other_metric_object is Novelty, \"AveragePopularity: attempting to merge with a metric object of different type\"\n\n        self.cumulative_popularity = self.cumulative_popularity + other_metric_object.cumulative_popularity\n        self.n_evaluated_users = self.n_evaluated_users + other_metric_object.n_evaluated_users\n\n\nclass Diversity_similarity(Metrics_Object):\n    \"\"\"\n    Intra list diversity computes the diversity of items appearing in the recommendations received by each single user, by using an item_diversity_matrix.\n    It can be used, for example, to compute the diversity in terms of features for a collaborative recommender.\n    A content-based recommender will have low IntraList diversity if that is computed on the same features the recommender uses.\n    A TopPopular recommender may exhibit high IntraList diversity.\n    \"\"\"\n\n    def __init__(self, item_diversity_matrix):\n        super(Diversity_similarity, self).__init__()\n\n        assert np.all(item_diversity_matrix >= 0.0) and np.all(item_diversity_matrix <= 1.0), \\\n            \"item_diversity_matrix contains value greated than 1.0 or lower than 0.0\"\n\n        self.item_diversity_matrix = item_diversity_matrix\n\n        self.n_evaluated_users = 0\n        self.diversity = 0.0\n\n\n    def add_recommendations(self, recommended_items_ids):\n\n        current_recommended_items_diversity = 0.0\n\n        for item_index in range(len(recommended_items_ids)-1):\n\n            item_id = recommended_items_ids[item_index]\n\n            item_other_diversity = self.item_diversity_matrix[item_id, recommended_items_ids]\n            item_other_diversity[item_index] = 0.0\n\n            current_recommended_items_diversity += np.sum(item_other_diversity)\n\n\n        self.diversity += current_recommended_items_diversity/(len(recommended_items_ids)*(len(recommended_items_ids)-1))\n\n        self.n_evaluated_users += 1\n\n\n    def get_metric_value(self):\n\n        if self.n_evaluated_users == 0:\n            return 0.0\n\n        return self.diversity/self.n_evaluated_users\n\n    def merge_with_other(self, other_metric_object):\n        assert other_metric_object is Diversity_similarity, \"Diversity: attempting to merge with a metric object of different type\"\n\n        self.diversity = self.diversity + other_metric_object.diversity\n        self.n_evaluated_users = self.n_evaluated_users + other_metric_object.n_evaluated_users\n\n\nclass Diversity_MeanInterList(Metrics_Object):\n    \"\"\"\n    MeanInterList diversity measures the uniqueness of different users' recommendation lists.\n    It can be used to measure how \"diversified\" are the recommendations different users receive.\n    While the original proposal called this metric \"Personalization\", we do not use this name since the highest MeanInterList diversity\n    is exhibited by a non personalized Random recommender.\n    It can be demonstrated that this metric does not require to compute the common items all possible couples of users have in common\n    but rather it is only sensitive to the total amount of time each item has been recommended.\n    MeanInterList diversity is a function of the square of the probability an item has been recommended to any user, hence\n    MeanInterList diversity is equivalent to the Herfindahl index as they measure the same quantity.\n    A TopPopular recommender that does not remove seen items will have 0.0 MeanInterList diversity.\n    pag. 3, http://www.pnas.org/content/pnas/107/10/4511.full.pdf\n    @article{zhou2010solving,\n      title={Solving the apparent diversity-accuracy dilemma of recommender systems},\n      author={Zhou, Tao and Kuscsik, Zolt{\\'a}n and Liu, Jian-Guo and Medo, Mat{\\'u}{\\v{s}} and Wakeling, Joseph Rushton and Zhang, Yi-Cheng},\n      journal={Proceedings of the National Academy of Sciences},\n      volume={107},\n      number={10},\n      pages={4511--4515},\n      year={2010},\n      publisher={National Acad Sciences}\n    }\n    # The formula is diversity_cumulative += 1 - common_recommendations(user1, user2)/cutoff\n    # for each couple of users, except the diagonal. It is VERY computationally expensive\n    # We can move the 1 and cutoff outside of the summation. Remember to exclude the diagonal\n    # co_counts = URM_predicted.dot(URM_predicted.T)\n    # co_counts[np.arange(0, n_user, dtype=np.int):np.arange(0, n_user, dtype=np.int)] = 0\n    # diversity = (n_user**2 - n_user) - co_counts.sum()/self.cutoff\n    # If we represent the summation of co_counts separating it for each item, we will have:\n    # co_counts.sum() = co_counts_item1.sum()  + co_counts_item2.sum() ...\n    # If we know how many times an item has been recommended, co_counts_item1.sum() can be computed as how many couples of\n    # users have item1 in common. If item1 has been recommended n times, the number of couples is n*(n-1)\n    # Therefore we can compute co_counts.sum() value as:\n    # np.sum(np.multiply(item-occurrence, item-occurrence-1))\n    # The naive implementation URM_predicted.dot(URM_predicted.T) might require an hour of computation\n    # The last implementation has a negligible computational time even for very big datasets\n    \"\"\"\n\n    def __init__(self, n_items, cutoff):\n        super(Diversity_MeanInterList, self).__init__()\n\n        self.recommended_counter = np.zeros(n_items, dtype=np.float)\n\n        self.n_evaluated_users = 0\n        self.n_items = n_items\n        self.diversity = 0.0\n        self.cutoff = cutoff\n\n\n    def add_recommendations(self, recommended_items_ids):\n\n        assert len(recommended_items_ids) <= self.cutoff, \"Diversity_MeanInterList: recommended list is contains more elements than cutoff\"\n\n        self.n_evaluated_users += 1\n\n        if len(recommended_items_ids) > 0:\n            self.recommended_counter[recommended_items_ids] += 1\n\n    def get_metric_value(self):\n\n        # Requires to compute the number of common elements for all couples of users\n        if self.n_evaluated_users == 0:\n            return 1.0\n\n        cooccurrences_cumulative = np.sum(self.recommended_counter**2) - self.n_evaluated_users*self.cutoff\n\n        # All user combinations except diagonal\n        all_user_couples_count = self.n_evaluated_users**2 - self.n_evaluated_users\n\n        diversity_cumulative = all_user_couples_count - cooccurrences_cumulative/self.cutoff\n\n        self.diversity = diversity_cumulative/all_user_couples_count\n\n        return self.diversity\n\n    def get_theoretical_max(self):\n\n        global_co_occurrence_count = (self.n_evaluated_users*self.cutoff)**2/self.n_items - self.n_evaluated_users*self.cutoff\n\n        mild = 1 - 1/(self.n_evaluated_users**2 - self.n_evaluated_users)*(global_co_occurrence_count/self.cutoff)\n\n        return mild\n\n    def merge_with_other(self, other_metric_object):\n\n        assert other_metric_object is Diversity_MeanInterList, \"Diversity_MeanInterList: attempting to merge with a metric object of different type\"\n\n        assert np.all(self.recommended_counter >= 0.0), \"Diversity_MeanInterList: self.recommended_counter contains negative counts\"\n        assert np.all(other_metric_object.recommended_counter >= 0.0), \"Diversity_MeanInterList: other.recommended_counter contains negative counts\"\n\n        self.recommended_counter += other_metric_object.recommended_counter\n        self.n_evaluated_users += other_metric_object.n_evaluated_users\n\ndef roc_auc(is_relevant):\n\n    ranks = np.arange(len(is_relevant))\n    pos_ranks = ranks[is_relevant]\n    neg_ranks = ranks[~is_relevant]\n    auc_score = 0.0\n\n    if len(neg_ranks) == 0:\n        return 1.0\n\n    if len(pos_ranks) > 0:\n        for pos_pred in pos_ranks:\n            auc_score += np.sum(pos_pred < neg_ranks, dtype=np.float32)\n        auc_score /= (pos_ranks.shape[0] * neg_ranks.shape[0])\n\n    assert 0 <= auc_score <= 1, auc_score\n    return auc_score\n\ndef arhr(is_relevant):\n    # average reciprocal hit-rank (ARHR) of all relevant items\n    # As opposed to MRR, ARHR takes into account all relevant items and not just the first\n    # pag 17\n    # http://glaros.dtc.umn.edu/gkhome/fetch/papers/itemrsTOIS04.pdf\n    # https://emunix.emich.edu/~sverdlik/COSC562/ItemBasedTopTen.pdf\n\n    p_reciprocal = 1/np.arange(1,len(is_relevant)+1, 1.0, dtype=np.float64)\n    arhr_score = is_relevant.dot(p_reciprocal)\n\n    #assert 0 <= arhr_score <= p_reciprocal.sum(), \"arhr_score {} should be between 0 and {}\".format(arhr_score, p_reciprocal.sum())\n    assert not np.isnan(arhr_score), \"ARHR is NaN\"\n    return arhr_score\n\ndef precision(is_relevant):\n\n    if len(is_relevant) == 0:\n        precision_score = 0.0\n    else:\n        precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n\n    assert 0 <= precision_score <= 1, precision_score\n    return precision_score\n\n\ndef precision_recall_min_denominator(is_relevant, n_test_items):\n\n    if len(is_relevant) == 0:\n        precision_score = 0.0\n    else:\n        precision_score = np.sum(is_relevant, dtype=np.float32) / min(n_test_items, len(is_relevant))\n\n    assert 0 <= precision_score <= 1, precision_score\n    return precision_score\n\ndef rmse(all_items_predicted_ratings, relevant_items, relevant_items_rating):\n\n    # Important, some items will have -np.inf score and are treated as if they did not exist\n\n    # RMSE with test items\n    relevant_items_error = (all_items_predicted_ratings[relevant_items]-relevant_items_rating)**2\n\n    finite_prediction_mask = np.isfinite(relevant_items_error)\n\n    if finite_prediction_mask.sum() == 0:\n        rmse = np.nan\n\n    else:\n        relevant_items_error = relevant_items_error[finite_prediction_mask]\n\n        squared_error = np.sum(relevant_items_error)\n\n        # # Second the RMSE against all non-test items assumed having true rating 0\n        # # In order to avoid the need of explicitly indexing all non-relevant items, use a difference\n        # squared_error += np.sum(all_items_predicted_ratings[np.isfinite(all_items_predicted_ratings)]**2) - \\\n        #                  np.sum(all_items_predicted_ratings[relevant_items][np.isfinite(all_items_predicted_ratings[relevant_items])]**2)\n\n        mean_squared_error = squared_error/finite_prediction_mask.sum()\n        rmse = np.sqrt(mean_squared_error)\n\n    return rmse\n\n\ndef recall(is_relevant, pos_items):\n\n    recall_score = np.sum(is_relevant, dtype=np.float32) / pos_items.shape[0]\n\n    assert 0 <= recall_score <= 1, recall_score\n    return recall_score\n\n\ndef rr(is_relevant):\n    # reciprocal rank of the FIRST relevant item in the ranked list (0 if none)\n\n    ranks = np.arange(1, len(is_relevant) + 1)[is_relevant]\n\n    if len(ranks) > 0:\n        return 1. / ranks[0]\n    else:\n        return 0.0\n\n\ndef average_precision(is_relevant, pos_items):\n\n    if len(is_relevant) == 0:\n        a_p = 0.0\n    else:\n        p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n        a_p = np.sum(p_at_k) / np.min([pos_items.shape[0], is_relevant.shape[0]])\n\n    assert 0 <= a_p <= 1, a_p\n    return a_p\n\n\ndef ndcg(ranked_list, pos_items, relevance=None, at=None):\n\n    if relevance is None:\n        relevance = np.ones_like(pos_items)\n    assert len(relevance) == pos_items.shape[0]\n\n    # Create a dictionary associating item_id to its relevance\n    # it2rel[item] -> relevance[item]\n    it2rel = {it: r for it, r in zip(pos_items, relevance)}\n\n    # Creates array of length \"at\" with the relevance associated to the item in that position\n    rank_scores = np.asarray([it2rel.get(it, 0.0) for it in ranked_list[:at]], dtype=np.float32)\n\n    # IDCG has all relevances to 1, up to the number of items in the test set\n    ideal_dcg = dcg(np.sort(relevance)[::-1])\n\n    # DCG uses the relevance of the recommended items\n    rank_dcg = dcg(rank_scores)\n\n    if rank_dcg == 0.0:\n        return 0.0\n\n    ndcg_ = rank_dcg / ideal_dcg\n    # assert 0 <= ndcg_ <= 1, (rank_dcg, ideal_dcg, ndcg_)\n    return ndcg_\n\n\ndef dcg(scores):\n    return np.sum(np.divide(np.power(2, scores) - 1, np.log(np.arange(scores.shape[0], dtype=np.float32) + 2)),\n                  dtype=np.float32)\n\n\nmetrics = ['AUC', 'Precision' 'Recall', 'MAP', 'NDCG']\n\n\ndef pp_metrics(metric_names, metric_values, metric_at):\n    \"\"\"\n    Pretty-prints metric values\n    :param metrics_arr:\n    :return:\n    \"\"\"\n    assert len(metric_names) == len(metric_values)\n    if isinstance(metric_at, int):\n        metric_at = [metric_at] * len(metric_values)\n    return ' '.join(['{}: {:.4f}'.format(mname, mvalue) if mcutoff is None or mcutoff == 0 else\n                     '{}@{}: {:.4f}'.format(mname, mcutoff, mvalue)\n                     for mname, mcutoff, mvalue in zip(metric_names, metric_at, metric_values)])\n\n\nclass TestAUC(unittest.TestCase):\n    def runTest(self):\n        pos_items = np.asarray([2, 4])\n        ranked_list = np.asarray([1, 2, 3, 4, 5])\n        self.assertTrue(np.allclose(roc_auc(ranked_list, pos_items),\n                                    (2. / 3 + 1. / 3) / 2))\n\n\nclass TestRecall(unittest.TestCase):\n    def runTest(self):\n        pos_items = np.asarray([2, 4, 5, 10])\n        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n        self.assertTrue(np.allclose(recall(ranked_list_1, pos_items), 3. / 4))\n        self.assertTrue(np.allclose(recall(ranked_list_2, pos_items), 1.0))\n        self.assertTrue(np.allclose(recall(ranked_list_3, pos_items), 0.0))\n\n        thresholds = [1, 2, 3, 4, 5]\n        values = [0.0, 1. / 4, 1. / 4, 2. / 4, 3. / 4]\n        for at, val in zip(thresholds, values):\n            self.assertTrue(np.allclose(np.asarray(recall(ranked_list_1, pos_items, at=at)), val))\n\n\nclass TestPrecision(unittest.TestCase):\n    def runTest(self):\n        pos_items = np.asarray([2, 4, 5, 10])\n        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n        self.assertTrue(np.allclose(precision(ranked_list_1, pos_items), 3. / 5))\n        self.assertTrue(np.allclose(precision(ranked_list_2, pos_items), 4. / 5))\n        self.assertTrue(np.allclose(precision(ranked_list_3, pos_items), 0.0))\n\n        thresholds = [1, 2, 3, 4, 5]\n        values = [0.0, 1. / 2, 1. / 3, 2. / 4, 3. / 5]\n        for at, val in zip(thresholds, values):\n            self.assertTrue(np.allclose(np.asarray(precision(ranked_list_1, pos_items, at=at)), val))\n\n\nclass TestRR(unittest.TestCase):\n    def runTest(self):\n        pos_items = np.asarray([2, 4, 5, 10])\n        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n        self.assertTrue(np.allclose(rr(ranked_list_1, pos_items), 1. / 2))\n        self.assertTrue(np.allclose(rr(ranked_list_2, pos_items), 1.))\n        self.assertTrue(np.allclose(rr(ranked_list_3, pos_items), 0.0))\n\n        thresholds = [1, 2, 3, 4, 5]\n        values = [0.0, 1. / 2, 1. / 2, 1. / 2, 1. / 2]\n        for at, val in zip(thresholds, values):\n            self.assertTrue(np.allclose(np.asarray(rr(ranked_list_1, pos_items, at=at)), val))\n\n\nclass TestMAP(unittest.TestCase):\n    def runTest(self):\n        pos_items = np.asarray([2, 4, 5, 10])\n        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])\n        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])\n        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])\n        ranked_list_4 = np.asarray([11, 12, 13, 14, 15, 16, 2, 4, 5, 10])\n        ranked_list_5 = np.asarray([2, 11, 12, 13, 14, 15, 4, 5, 10, 16])\n        self.assertTrue(np.allclose(map(ranked_list_1, pos_items), (1. / 2 + 2. / 4 + 3. / 5) / 4))\n        self.assertTrue(np.allclose(map(ranked_list_2, pos_items), 1.0))\n        self.assertTrue(np.allclose(map(ranked_list_3, pos_items), 0.0))\n        self.assertTrue(np.allclose(map(ranked_list_4, pos_items), (1. / 7 + 2. / 8 + 3. / 9 + 4. / 10) / 4))\n        self.assertTrue(np.allclose(map(ranked_list_5, pos_items), (1. + 2. / 7 + 3. / 8 + 4. / 9) / 4))\n\n        thresholds = [1, 2, 3, 4, 5]\n        values = [\n            0.0,\n            1. / 2 / 2,\n            1. / 2 / 3,\n            (1. / 2 + 2. / 4) / 4,\n            (1. / 2 + 2. / 4 + 3. / 5) / 4\n        ]\n        for at, val in zip(thresholds, values):\n            self.assertTrue(np.allclose(np.asarray(map(ranked_list_1, pos_items, at)), val))\n\n\nclass TestNDCG(unittest.TestCase):\n    def runTest(self):\n        pos_items = np.asarray([2, 4, 5, 10])\n        pos_relevances = np.asarray([5, 4, 3, 2])\n        ranked_list_1 = np.asarray([1, 2, 3, 4, 5])  # rel = 0, 5, 0, 4, 3\n        ranked_list_2 = np.asarray([10, 5, 2, 4, 3])  # rel = 2, 3, 5, 4, 0\n        ranked_list_3 = np.asarray([1, 3, 6, 7, 8])  # rel = 0, 0, 0, 0, 0\n        idcg = ((2 ** 5 - 1) / np.log(2) +\n                (2 ** 4 - 1) / np.log(3) +\n                (2 ** 3 - 1) / np.log(4) +\n                (2 ** 2 - 1) / np.log(5))\n        self.assertTrue(np.allclose(dcg(np.sort(pos_relevances)[::-1]), idcg))\n        self.assertTrue(np.allclose(ndcg(ranked_list_1, pos_items, pos_relevances),\n                                    ((2 ** 5 - 1) / np.log(3) +\n                                     (2 ** 4 - 1) / np.log(5) +\n                                     (2 ** 3 - 1) / np.log(6)) / idcg))\n        self.assertTrue(np.allclose(ndcg(ranked_list_2, pos_items, pos_relevances),\n                                    ((2 ** 2 - 1) / np.log(2) +\n                                     (2 ** 3 - 1) / np.log(3) +\n                                     (2 ** 5 - 1) / np.log(4) +\n                                     (2 ** 4 - 1) / np.log(5)) / idcg))\n        self.assertTrue(np.allclose(ndcg(ranked_list_3, pos_items, pos_relevances), 0.0))\n\n\n# if __name__ == '__main__':\n#     unittest.main()\n\n\n# seconds_to_biggest_unit.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 30/03/2019\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\n\ndef seconds_to_biggest_unit(time_in_seconds, data_array = None):\n\n    conversion_factor = [\n        (\"sec\", 60),\n        (\"min\", 60),\n        (\"hour\", 24),\n        (\"day\", 365),\n    ]\n\n    terminate = False\n    unit_index = 0\n\n    new_time_value = time_in_seconds\n    new_time_unit = \"sec\"\n\n    while not terminate:\n\n        next_time = new_time_value/conversion_factor[unit_index][1]\n\n        if next_time >= 1.0:\n            new_time_value = next_time\n\n            if data_array is not None:\n                data_array /= conversion_factor[unit_index][1]\n\n            unit_index += 1\n            new_time_unit = conversion_factor[unit_index][0]\n\n        else:\n            terminate = True\n\n\n    if data_array is not None:\n        return new_time_value, new_time_unit, data_array\n\n    else:\n        return new_time_value, new_time_unit\n","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Hyperparams tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SearchAbstractClass.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 10/03/2018\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\nimport time, os, traceback\n# from utils.Evaluation.Incremental_Training_Early_Stopping import Incremental_Training_Early_Stopping\nimport numpy as np\n# from utils.DataIO import DataIO\n\nclass SearchInputRecommenderArgs(object):\n\n\n    def __init__(self,\n                   # Dictionary of parameters needed by the constructor\n                   CONSTRUCTOR_POSITIONAL_ARGS = None,\n                   CONSTRUCTOR_KEYWORD_ARGS = None,\n\n                   # List containing all positional arguments needed by the fit function\n                   FIT_POSITIONAL_ARGS = None,\n                   FIT_KEYWORD_ARGS = None\n                   ):\n\n\n          super(SearchInputRecommenderArgs, self).__init__()\n\n          if CONSTRUCTOR_POSITIONAL_ARGS is None:\n              CONSTRUCTOR_POSITIONAL_ARGS = []\n\n          if CONSTRUCTOR_KEYWORD_ARGS is None:\n              CONSTRUCTOR_KEYWORD_ARGS = {}\n\n          if FIT_POSITIONAL_ARGS is None:\n              FIT_POSITIONAL_ARGS = []\n\n          if FIT_KEYWORD_ARGS is None:\n              FIT_KEYWORD_ARGS = {}\n\n\n          assert isinstance(CONSTRUCTOR_POSITIONAL_ARGS, list), \"CONSTRUCTOR_POSITIONAL_ARGS must be a list\"\n          assert isinstance(CONSTRUCTOR_KEYWORD_ARGS, dict), \"CONSTRUCTOR_KEYWORD_ARGS must be a dict\"\n\n          assert isinstance(FIT_POSITIONAL_ARGS, list), \"FIT_POSITIONAL_ARGS must be a list\"\n          assert isinstance(FIT_KEYWORD_ARGS, dict), \"FIT_KEYWORD_ARGS must be a dict\"\n\n\n          self.CONSTRUCTOR_POSITIONAL_ARGS = CONSTRUCTOR_POSITIONAL_ARGS\n          self.CONSTRUCTOR_KEYWORD_ARGS = CONSTRUCTOR_KEYWORD_ARGS\n\n          self.FIT_POSITIONAL_ARGS = FIT_POSITIONAL_ARGS\n          self.FIT_KEYWORD_ARGS = FIT_KEYWORD_ARGS\n\n\n    def copy(self):\n\n\n        clone_object = SearchInputRecommenderArgs(\n                            CONSTRUCTOR_POSITIONAL_ARGS = self.CONSTRUCTOR_POSITIONAL_ARGS.copy(),\n                            CONSTRUCTOR_KEYWORD_ARGS = self.CONSTRUCTOR_KEYWORD_ARGS.copy(),\n                            FIT_POSITIONAL_ARGS = self.FIT_POSITIONAL_ARGS.copy(),\n                            FIT_KEYWORD_ARGS = self.FIT_KEYWORD_ARGS.copy()\n                            )\n\n\n        return clone_object\n\n\n\ndef _compute_avg_time_non_none_values(data_list):\n\n    non_none_values = sum([value is not None for value in data_list])\n    total_value = sum([value if value is not None else 0.0 for value in data_list])\n\n    return total_value, \\\n           total_value/non_none_values if non_none_values != 0 else 0.0\n\n\n\ndef get_result_string_evaluate_on_validation(results_run_single_cutoff, n_decimals=7):\n\n    output_str = \"\"\n\n    for metric in results_run_single_cutoff.keys():\n        output_str += \"{}: {:.{n_decimals}f}, \".format(metric, results_run_single_cutoff[metric], n_decimals = n_decimals)\n\n    return output_str\n\n\n\nclass SearchAbstractClass(object):\n\n    ALGORITHM_NAME = \"SearchAbstractClass\"\n\n    # Available values for the save_model attribute\n    _SAVE_MODEL_VALUES = [\"all\", \"best\", \"last\", \"no\"]\n\n\n    # Value to be assigned to invalid configuration or if an Exception is raised\n    INVALID_CONFIG_VALUE = np.finfo(np.float16).max\n\n    def __init__(self, recommender_class,\n                 evaluator_validation = None,\n                 evaluator_test = None,\n                 verbose = True):\n\n        super(SearchAbstractClass, self).__init__()\n\n        self.recommender_class = recommender_class\n        self.verbose = verbose\n        self.log_file = None\n\n        self.results_test_best = {}\n        self.parameter_dictionary_best = {}\n\n        self.evaluator_validation = evaluator_validation\n\n        if evaluator_test is None:\n            self.evaluator_test = None\n        else:\n            self.evaluator_test = evaluator_test\n\n\n    def search(self, recommender_input_args,\n               parameter_search_space,\n               metric_to_optimize = \"MAP\",\n               n_cases = None,\n               output_folder_path = None,\n               output_file_name_root = None,\n               parallelize = False,\n               save_model = \"best\",\n               evaluate_on_test_each_best_solution = True,\n               save_metadata = True,\n               ):\n\n        raise NotImplementedError(\"Function search not implemented for this class\")\n\n\n    def _set_search_attributes(self, recommender_input_args,\n                               recommender_input_args_last_test,\n                               metric_to_optimize,\n                               output_folder_path,\n                               output_file_name_root,\n                               resume_from_saved,\n                               save_metadata,\n                               save_model,\n                               evaluate_on_test_each_best_solution,\n                               n_cases):\n\n\n        if save_model not in self._SAVE_MODEL_VALUES:\n           raise ValueError(\"{}: parameter save_model must be in '{}', provided was '{}'.\".format(self.ALGORITHM_NAME, self._SAVE_MODEL_VALUES, save_model))\n\n        self.output_folder_path = output_folder_path\n        self.output_file_name_root = output_file_name_root\n\n        # If directory does not exist, create\n        if not os.path.exists(self.output_folder_path):\n            os.makedirs(self.output_folder_path)\n\n        self.log_file = open(self.output_folder_path + self.output_file_name_root + \"_{}.txt\".format(self.ALGORITHM_NAME), \"a\")\n\n        if save_model == \"last\" and recommender_input_args_last_test is None:\n            self._write_log(\"{}: parameter save_model is 'last' but no recommender_input_args_last_test provided, saving best model on train data alone.\".format(self.ALGORITHM_NAME))\n            save_model = \"best\"\n\n\n\n        self.recommender_input_args = recommender_input_args\n        self.recommender_input_args_last_test = recommender_input_args_last_test\n        self.metric_to_optimize = metric_to_optimize\n        self.save_model = save_model\n        self.resume_from_saved = resume_from_saved\n        self.save_metadata = save_metadata\n        self.evaluate_on_test_each_best_solution = evaluate_on_test_each_best_solution\n\n        self.model_counter = 0\n        self._init_metadata_dict(n_cases = n_cases)\n\n        if self.save_metadata:\n            self.dataIO = DataIO(folder_path = self.output_folder_path)\n\n\n\n    def _init_metadata_dict(self, n_cases):\n\n        self.metadata_dict = {\"algorithm_name_search\": self.ALGORITHM_NAME,\n                              \"algorithm_name_recommender\": self.recommender_class.RECOMMENDER_NAME,\n                              \"exception_list\": [None]*n_cases,\n\n                              \"hyperparameters_list\": [None]*n_cases,\n                              \"hyperparameters_best\": None,\n                              \"hyperparameters_best_index\": None,\n\n                              \"result_on_validation_list\": [None]*n_cases,\n                              \"result_on_validation_best\": None,\n                              \"result_on_test_list\": [None]*n_cases,\n                              \"result_on_test_best\": None,\n\n                              \"time_on_train_list\": [None]*n_cases,\n                              \"time_on_train_total\": 0.0,\n                              \"time_on_train_avg\": 0.0,\n\n                              \"time_on_validation_list\": [None]*n_cases,\n                              \"time_on_validation_total\": 0.0,\n                              \"time_on_validation_avg\": 0.0,\n\n                              \"time_on_test_list\": [None]*n_cases,\n                              \"time_on_test_total\": 0.0,\n                              \"time_on_test_avg\": 0.0,\n\n                              \"result_on_last\": None,\n                              \"time_on_last_train\": None,\n                              \"time_on_last_test\": None,\n                              }\n\n\n    def _print(self, string):\n\n        if self.verbose:\n            print(string)\n\n\n    def _write_log(self, string):\n\n        self._print(string)\n\n        if self.log_file is not None:\n            self.log_file.write(string)\n            self.log_file.flush()\n\n\n    def _fit_model(self, current_fit_parameters):\n\n        start_time = time.time()\n\n        # Construct a new recommender instance\n        recommender_instance = self.recommender_class(*self.recommender_input_args.CONSTRUCTOR_POSITIONAL_ARGS,\n                                                      **self.recommender_input_args.CONSTRUCTOR_KEYWORD_ARGS)\n\n\n        self._print(\"{}: Testing config: {}\".format(self.ALGORITHM_NAME, current_fit_parameters))\n\n\n        recommender_instance.fit(*self.recommender_input_args.FIT_POSITIONAL_ARGS,\n                                 **self.recommender_input_args.FIT_KEYWORD_ARGS,\n                                 **current_fit_parameters)\n\n        train_time = time.time() - start_time\n\n        return recommender_instance, train_time\n\n\n\n    def _evaluate_on_validation(self, current_fit_parameters):\n\n        recommender_instance, train_time = self._fit_model(current_fit_parameters)\n\n        start_time = time.time()\n\n        # Evaluate recommender and get results for the first cutoff\n        result_dict, _ = self.evaluator_validation.evaluateRecommender(recommender_instance)\n        result_dict = result_dict[list(result_dict.keys())[0]]\n\n        evaluation_time = time.time() - start_time\n\n        result_string = get_result_string_evaluate_on_validation(result_dict, n_decimals=7)\n\n        return result_dict, result_string, recommender_instance, train_time, evaluation_time\n\n\n\n    def _evaluate_on_test(self, recommender_instance, current_fit_parameters_dict, print_log = True):\n\n        start_time = time.time()\n\n        # Evaluate recommender and get results for the first cutoff\n        result_dict, result_string = self.evaluator_test.evaluateRecommender(recommender_instance)\n\n        evaluation_test_time = time.time() - start_time\n\n        if print_log:\n            self._write_log(\"{}: Best config evaluated with evaluator_test. Config: {} - results:\\n{}\\n\".format(self.ALGORITHM_NAME,\n                                                                                                         current_fit_parameters_dict,\n                                                                                                         result_string))\n\n        return result_dict, result_string, evaluation_test_time\n\n\n\n    def _evaluate_on_test_with_data_last(self):\n\n        start_time = time.time()\n\n        # Construct a new recommender instance\n        recommender_instance = self.recommender_class(*self.recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS,\n                                                      **self.recommender_input_args_last_test.CONSTRUCTOR_KEYWORD_ARGS)\n\n        # Check if last was already evaluated\n        if self.resume_from_saved:\n            result_on_last_saved_flag = self.metadata_dict[\"result_on_last\"] is not None and \\\n                                        self.metadata_dict[\"time_on_last_train\"] is not None and \\\n                                        self.metadata_dict[\"time_on_last_test\"] is not None\n\n            if result_on_last_saved_flag:\n                self._print(\"{}: Resuming '{}'... Result on last already available.\".format(self.ALGORITHM_NAME, self.output_file_name_root))\n                return\n\n\n\n        self._print(\"{}: Evaluation with constructor data for final test. Using best config: {}\".format(self.ALGORITHM_NAME, self.metadata_dict[\"hyperparameters_best\"]))\n\n\n        # Use the hyperparameters that have been saved\n        assert self.metadata_dict[\"hyperparameters_best\"] is not None, \"{}: Best hyperparameters not available, the search might have failed.\".format(self.ALGORITHM_NAME)\n        fit_keyword_args = self.metadata_dict[\"hyperparameters_best\"].copy()\n\n\n        recommender_instance.fit(*self.recommender_input_args_last_test.FIT_POSITIONAL_ARGS,\n                                 **fit_keyword_args)\n\n        train_time = time.time() - start_time\n\n        result_dict_test, result_string, evaluation_test_time = self._evaluate_on_test(recommender_instance, fit_keyword_args, print_log = False)\n\n        self._write_log(\"{}: Best config evaluated with evaluator_test with constructor data for final test. Config: {} - results:\\n{}\\n\".format(self.ALGORITHM_NAME,\n                                                                                                                                          self.metadata_dict[\"hyperparameters_best\"],\n                                                                                                                                          result_string))\n\n        self.metadata_dict[\"result_on_last\"] = result_dict_test\n        self.metadata_dict[\"time_on_last_train\"] = train_time\n        self.metadata_dict[\"time_on_last_test\"] = evaluation_test_time\n\n        if self.save_metadata:\n            self.dataIO.save_data(data_dict_to_save = self.metadata_dict.copy(),\n                                  file_name = self.output_file_name_root + \"_metadata\")\n\n        if self.save_model in [\"all\", \"best\", \"last\"]:\n            self._print(\"{}: Saving model in {}\\n\".format(self.ALGORITHM_NAME, self.output_folder_path + self.output_file_name_root))\n            recommender_instance.save_model(self.output_folder_path, file_name =self.output_file_name_root + \"_best_model_last\")\n\n\n    def _objective_function(self, current_fit_parameters_dict):\n\n        try:\n\n            self.metadata_dict[\"hyperparameters_list\"][self.model_counter] = current_fit_parameters_dict.copy()\n\n            result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\n\n            current_result = - result_dict[self.metric_to_optimize]\n\n            # If the recommender uses Earlystopping, get the selected number of epochs\n            if isinstance(recommender_instance, Incremental_Training_Early_Stopping):\n\n                n_epochs_early_stopping_dict = recommender_instance.get_early_stopping_final_epochs_dict()\n                current_fit_parameters_dict = current_fit_parameters_dict.copy()\n\n                for epoch_label in n_epochs_early_stopping_dict.keys():\n\n                    epoch_value = n_epochs_early_stopping_dict[epoch_label]\n                    current_fit_parameters_dict[epoch_label] = epoch_value\n\n\n\n            # Always save best model separately\n            if self.save_model in [\"all\"]:\n                self._print(\"{}: Saving model in {}\\n\".format(self.ALGORITHM_NAME, self.output_folder_path + self.output_file_name_root))\n                recommender_instance.save_model(self.output_folder_path, file_name = self.output_file_name_root + \"_model_{}\".format(self.model_counter))\n\n\n            if self.metadata_dict[\"result_on_validation_best\"] is None:\n                new_best_config_found = True\n            else:\n                best_solution_val = self.metadata_dict[\"result_on_validation_best\"][self.metric_to_optimize]\n                new_best_config_found = best_solution_val < result_dict[self.metric_to_optimize]\n\n\n            if new_best_config_found:\n\n                self._write_log(\"{}: New best config found. Config {}: {} - results: {}\\n\".format(self.ALGORITHM_NAME,\n                                                                                           self.model_counter,\n                                                                                           current_fit_parameters_dict,\n                                                                                           result_string))\n\n                if self.save_model in [\"all\", \"best\"]:\n                    self._print(\"{}: Saving model in {}\\n\".format(self.ALGORITHM_NAME, self.output_folder_path + self.output_file_name_root))\n                    recommender_instance.save_model(self.output_folder_path, file_name =self.output_file_name_root + \"_best_model\")\n\n\n                if self.evaluator_test is not None and self.evaluate_on_test_each_best_solution:\n                    result_dict_test, _, evaluation_test_time = self._evaluate_on_test(recommender_instance, current_fit_parameters_dict, print_log = True)\n\n\n            else:\n                self._write_log(\"{}: Config {} is suboptimal. Config: {} - results: {}\\n\".format(self.ALGORITHM_NAME,\n                                                                                          self.model_counter,\n                                                                                          current_fit_parameters_dict,\n                                                                                          result_string))\n\n\n\n            if current_result >= self.INVALID_CONFIG_VALUE:\n                self._write_log(\"{}: WARNING! Config {} returned a value equal or worse than the default value to be assigned to invalid configurations.\"\n                                \" If no better valid configuration is found, this parameter search may produce an invalid result.\\n\")\n\n\n\n            self.metadata_dict[\"result_on_validation_list\"][self.model_counter] = result_dict.copy()\n\n            self.metadata_dict[\"time_on_train_list\"][self.model_counter] = train_time\n            self.metadata_dict[\"time_on_validation_list\"][self.model_counter] = evaluation_time\n\n            self.metadata_dict[\"time_on_train_total\"], self.metadata_dict[\"time_on_train_avg\"] = \\\n                _compute_avg_time_non_none_values(self.metadata_dict[\"time_on_train_list\"])\n            self.metadata_dict[\"time_on_validation_total\"], self.metadata_dict[\"time_on_validation_avg\"] = \\\n                _compute_avg_time_non_none_values(self.metadata_dict[\"time_on_validation_list\"])\n\n\n            if new_best_config_found:\n                self.metadata_dict[\"hyperparameters_best\"] = current_fit_parameters_dict.copy()\n                self.metadata_dict[\"hyperparameters_best_index\"] = self.model_counter\n                self.metadata_dict[\"result_on_validation_best\"] = result_dict.copy()\n\n                if self.evaluator_test is not None and self.evaluate_on_test_each_best_solution:\n                    self.metadata_dict[\"result_on_test_best\"] = result_dict_test.copy()\n                    self.metadata_dict[\"result_on_test_list\"][self.model_counter] = result_dict_test.copy()\n                    self.metadata_dict[\"time_on_test_list\"][self.model_counter] = evaluation_test_time\n\n                    self.metadata_dict[\"time_on_test_total\"], self.metadata_dict[\"time_on_test_avg\"] = \\\n                        _compute_avg_time_non_none_values(self.metadata_dict[\"time_on_test_list\"])\n\n\n        except (KeyboardInterrupt, SystemExit) as e:\n            # If getting a interrupt, terminate without saving the exception\n            raise e\n\n        except:\n            # Catch any error: Exception, Tensorflow errors etc...\n\n            traceback_string = traceback.format_exc()\n\n            self._write_log(\"{}: Config {} Exception. Config: {} - Exception: {}\\n\".format(self.ALGORITHM_NAME,\n                                                                                  self.model_counter,\n                                                                                  current_fit_parameters_dict,\n                                                                                  traceback_string))\n\n            self.metadata_dict[\"exception_list\"][self.model_counter] = traceback_string\n\n\n            # Assign to this configuration the worst possible score\n            # Being a minimization problem, set it to the max value of a float\n            current_result = + self.INVALID_CONFIG_VALUE\n\n            traceback.print_exc()\n\n\n\n        if self.save_metadata:\n            self.dataIO.save_data(data_dict_to_save = self.metadata_dict.copy(),\n                                  file_name = self.output_file_name_root + \"_metadata\")\n\n        self.model_counter += 1\n\n        return current_result","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SearchBayesianSkopt.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 14/12/18\n@author: Emanuele Chioso, Maurizio Ferrari Dacrema\n\"\"\"\n\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer, Categorical\n\n# from utils.ParameterTuning.SearchAbstractClass import SearchAbstractClass\nimport traceback\n\n\nclass SearchBayesianSkopt(SearchAbstractClass):\n\n    ALGORITHM_NAME = \"SearchBayesianSkopt\"\n\n    def __init__(self, recommender_class, evaluator_validation = None, evaluator_test = None, verbose = True):\n\n        assert evaluator_validation is not None, \"{}: evaluator_validation must be provided\".format(self.ALGORITHM_NAME)\n\n        super(SearchBayesianSkopt, self).__init__(recommender_class,\n                                                  evaluator_validation = evaluator_validation,\n                                                  evaluator_test = evaluator_test,\n                                                  verbose = verbose)\n\n\n\n    def _set_skopt_params(self, n_calls = 70,\n                          n_random_starts = 20,\n                          n_points = 10000,\n                          n_jobs = 1,\n                          # noise = 'gaussian',\n                          noise = 1e-5,\n                          acq_func = 'gp_hedge',\n                          acq_optimizer = 'auto',\n                          random_state = None,\n                          verbose = True,\n                          n_restarts_optimizer = 10,\n                          xi = 0.01,\n                          kappa = 1.96,\n                          x0 = None,\n                          y0 = None):\n        \"\"\"\n        wrapper to change the params of the bayesian optimizator.\n        for further details:\n        https://scikit-optimize.github.io/#skopt.gp_minimize\n        \"\"\"\n        self.n_point = n_points\n        self.n_calls = n_calls\n        self.n_random_starts = n_random_starts\n        self.n_jobs = n_jobs\n        self.acq_func = acq_func\n        self.acq_optimizer = acq_optimizer\n        self.random_state = random_state\n        self.n_restarts_optimizer = n_restarts_optimizer\n        self.verbose = verbose\n        self.xi = xi\n        self.kappa = kappa\n        self.noise = noise\n        self.x0 = x0\n        self.y0 = y0\n\n\n\n    def _resume_from_saved(self):\n\n        try:\n            self.metadata_dict = self.dataIO.load_data(file_name = self.output_file_name_root + \"_metadata\")\n\n        except (KeyboardInterrupt, SystemExit) as e:\n            # If getting a interrupt, terminate without saving the exception\n            raise e\n\n        except FileNotFoundError:\n            self._write_log(\"{}: Resuming '{}' Failed, no such file exists.\\n\".format(self.ALGORITHM_NAME, self.output_file_name_root))\n            self.resume_from_saved = False\n            return None, None\n\n        except Exception as e:\n            self._write_log(\"{}: Resuming '{}' Failed, generic exception: {}.\\n\".format(self.ALGORITHM_NAME, self.output_file_name_root, str(e)))\n            traceback.print_exc()\n            self.resume_from_saved = False\n            return None, None\n\n        # Get hyperparameter list and corresponding result\n        # Make sure that the hyperparameters only contain those given as input and not others like the number of epochs\n        # selected by earlystopping\n        hyperparameters_list_saved = self.metadata_dict['hyperparameters_list']\n        result_on_validation_list_saved = self.metadata_dict['result_on_validation_list']\n\n        hyperparameters_list_input = []\n        result_on_validation_list_input = []\n\n        # The hyperparameters are saved for all cases even if they throw an exception\n        while self.model_counter<len(hyperparameters_list_saved) and hyperparameters_list_saved[self.model_counter] is not None:\n\n            hyperparameters_config_saved = hyperparameters_list_saved[self.model_counter]\n\n            hyperparameters_config_input = []\n\n            # Add only those having a search space, in the correct ordering\n            for index in range(len(self.hyperparams_names)):\n                key = self.hyperparams_names[index]\n                value_saved = hyperparameters_config_saved[key]\n\n                # Check if single value categorical. It is aimed at intercepting\n                # Hyperparameters that are chosen via early stopping and set them as the\n                # maximum value as per hyperparameter search space. If not, the gp_minimize will return an error\n                # as some values will be outside (lower) than the search space\n\n                if isinstance(self.hyperparams_values[index], Categorical) and self.hyperparams_values[index].transformed_size == 1:\n                    value_input = self.hyperparams_values[index].bounds[0]\n                else:\n                    value_input = value_saved\n\n                hyperparameters_config_input.append(value_input)\n\n\n            hyperparameters_list_input.append(hyperparameters_config_input)\n\n            # Check if the hyperparameters have a valid result or an exception\n            validation_result = result_on_validation_list_saved[self.model_counter]\n\n            if validation_result is None:\n                # Exception detected\n                result_on_validation_list_input.append(+ self.INVALID_CONFIG_VALUE)\n\n                assert self.metadata_dict[\"exception_list\"][self.model_counter] is not None, \\\n                    \"{}: Resuming '{}' Failed due to inconsistent data. Invalid validation result found in position {} but no corresponding exception detected.\".format(self.ALGORITHM_NAME, self.output_file_name_root, self.model_counter)\n            else:\n                result_on_validation_list_input.append(- validation_result[self.metric_to_optimize])\n\n\n\n            self.model_counter += 1\n\n\n        self._print(\"{}: Resuming '{}'... Loaded {} configurations.\".format(self.ALGORITHM_NAME, self.output_file_name_root, self.model_counter))\n\n\n        # If the data structure exists but is empty, return None\n        if len(hyperparameters_list_input) == 0:\n            self.resume_from_saved = False\n            return None, None\n\n        # If loaded less configurations than desired ones\n        if self.model_counter < self.n_calls:\n            self.resume_from_saved = False\n\n\n        return hyperparameters_list_input, result_on_validation_list_input\n\n\n    def search(self, recommender_input_args,\n               parameter_search_space,\n               metric_to_optimize = \"MAP\",\n               n_cases = 20,\n               n_random_starts = 5,\n               output_folder_path = None,\n               output_file_name_root = None,\n               save_model = \"best\",\n               save_metadata = True,\n               resume_from_saved = False,\n               recommender_input_args_last_test = None,\n               evaluate_on_test_each_best_solution = True,\n               ):\n        \"\"\"\n        :param recommender_input_args:\n        :param parameter_search_space:\n        :param metric_to_optimize:\n        :param n_cases:\n        :param n_random_starts:\n        :param output_folder_path:\n        :param output_file_name_root:\n        :param save_model:          \"no\"    don't save anything\n                                    \"all\"   save every model\n                                    \"best\"  save the best model trained on train data alone and on last, if present\n                                    \"last\"  save only last, if present\n        :param save_metadata:\n        :param recommender_input_args_last_test:\n        :return:\n        \"\"\"\n\n\n        self._set_skopt_params()    ### default parameters are set here\n\n        self._set_search_attributes(recommender_input_args,\n                                    recommender_input_args_last_test,\n                                    metric_to_optimize,\n                                    output_folder_path,\n                                    output_file_name_root,\n                                    resume_from_saved,\n                                    save_metadata,\n                                    save_model,\n                                    evaluate_on_test_each_best_solution,\n                                    n_cases)\n\n\n        self.parameter_search_space = parameter_search_space\n        self.n_random_starts = n_random_starts\n        self.n_calls = n_cases\n        self.n_jobs = 1\n        self.n_loaded_counter = 0\n\n\n        self.hyperparams = dict()\n        self.hyperparams_names = list()\n        self.hyperparams_values = list()\n\n        skopt_types = [Real, Integer, Categorical]\n\n        for name, hyperparam in self.parameter_search_space.items():\n\n            if any(isinstance(hyperparam, sko_type) for sko_type in skopt_types):\n                self.hyperparams_names.append(name)\n                self.hyperparams_values.append(hyperparam)\n                self.hyperparams[name] = hyperparam\n\n            else:\n                raise ValueError(\"{}: Unexpected parameter type: {} - {}\".format(self.ALGORITHM_NAME, str(name), str(hyperparam)))\n\n\n        if self.resume_from_saved:\n            hyperparameters_list_input, result_on_validation_list_saved = self._resume_from_saved()\n            self.x0 = hyperparameters_list_input\n            self.y0 = result_on_validation_list_saved\n\n            self.n_random_starts = max(0, self.n_random_starts - self.model_counter)\n            self.n_calls = max(0, self.n_calls - self.model_counter)\n            self.n_loaded_counter = self.model_counter\n\n\n\n        self.result = gp_minimize(self._objective_function_list_input,\n                                  self.hyperparams_values,\n                                  base_estimator=None,\n                                  n_calls=self.n_calls,\n                                  n_random_starts=self.n_random_starts,\n                                  acq_func=self.acq_func,\n                                  acq_optimizer=self.acq_optimizer,\n                                  x0=self.x0,\n                                  y0=self.y0,\n                                  random_state=self.random_state,\n                                  verbose=self.verbose,\n                                  callback=None,\n                                  n_points=self.n_point,\n                                  n_restarts_optimizer=self.n_restarts_optimizer,\n                                  xi=self.xi,\n                                  kappa=self.kappa,\n                                  noise=self.noise,\n                                  n_jobs=self.n_jobs)\n\n\n        if self.n_loaded_counter < self.model_counter:\n            self._write_log(\"{}: Search complete. Best config is {}: {}\\n\".format(self.ALGORITHM_NAME,\n                                                                           self.metadata_dict[\"hyperparameters_best_index\"],\n                                                                           self.metadata_dict[\"hyperparameters_best\"]))\n\n\n        if self.recommender_input_args_last_test is not None:\n            self._evaluate_on_test_with_data_last()\n\n\n\n\n\n\n\n    def _objective_function_list_input(self, current_fit_parameters_list_of_values):\n\n        current_fit_parameters_dict = dict(zip(self.hyperparams_names, current_fit_parameters_list_of_values))\n\n        return self._objective_function(current_fit_parameters_dict)\n","execution_count":7,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n  warnings.warn(msg, category=DeprecationWarning)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameter_search.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 22/11/17\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\n# Automate hyperparameter tuning\n# using bayesian optimization with scikit-optimize\n# -------------------------------------------------\n\nimport os, multiprocessing\nfrom functools import partial\n\n######################################################################\nfrom skopt.space import Real, Integer, Categorical\nimport traceback\n# from Utils.PoolWithSubprocess import PoolWithSubprocess\n\n\ndef run_KNNRecommender_on_similarity_type(similarity_type, parameterSearch,\n                                          parameter_search_space,\n                                          recommender_input_args,\n                                          n_cases,\n                                          n_random_starts,\n                                          resume_from_saved,\n                                          save_model,\n                                          output_folder_path,\n                                          output_file_name_root,\n                                          metric_to_optimize,\n                                          allow_weighting=False,\n                                          recommender_input_args_last_test=None):\n\n    original_parameter_search_space = parameter_search_space\n\n    hyperparameters_range_dictionary = {}\n    hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n    hyperparameters_range_dictionary[\"shrink\"] = Integer(0, 1000)\n    hyperparameters_range_dictionary[\"similarity\"] = Categorical([similarity_type])\n    hyperparameters_range_dictionary[\"normalize\"] = Categorical([True, False])\n\n    is_set_similarity = similarity_type in [\"tversky\", \"dice\", \"jaccard\", \"tanimoto\"]\n\n    if similarity_type == \"asymmetric\":\n        hyperparameters_range_dictionary[\"asymmetric_alpha\"] = Real(low=0, high=2, prior='uniform')\n        hyperparameters_range_dictionary[\"normalize\"] = Categorical([True])\n\n    elif similarity_type == \"tversky\":\n        hyperparameters_range_dictionary[\"tversky_alpha\"] = Real(low=0, high=2, prior='uniform')\n        hyperparameters_range_dictionary[\"tversky_beta\"] = Real(low=0, high=2, prior='uniform')\n        hyperparameters_range_dictionary[\"normalize\"] = Categorical([True])\n\n    elif similarity_type == \"euclidean\":\n        hyperparameters_range_dictionary[\"normalize\"] = Categorical([True, False])\n        hyperparameters_range_dictionary[\"normalize_avg_row\"] = Categorical([True, False])\n        hyperparameters_range_dictionary[\"similarity_from_distance_mode\"] = Categorical([\"lin\", \"log\", \"exp\"])\n\n    if not is_set_similarity:\n\n        if allow_weighting:\n            hyperparameters_range_dictionary[\"feature_weighting\"] = Categorical([\"none\", \"BM25\", \"TF-IDF\"])\n\n    local_parameter_search_space = {**hyperparameters_range_dictionary, **original_parameter_search_space}\n\n    parameterSearch.search(recommender_input_args,\n                           parameter_search_space=local_parameter_search_space,\n                           n_cases=n_cases,\n                           n_random_starts=n_random_starts,\n                           resume_from_saved=resume_from_saved,\n                           save_model=save_model,\n                           output_folder_path=output_folder_path,\n                           output_file_name_root=output_file_name_root + \"_\" + similarity_type,\n                           metric_to_optimize=metric_to_optimize,\n                           recommender_input_args_last_test=recommender_input_args_last_test)\n\n\ndef runParameterSearch_Content(recommender_class, URM_train, ICM_object, ICM_name, URM_train_last_test=None,\n                               n_cases=30, n_random_starts=5, resume_from_saved=False, save_model=\"best\",\n                               evaluator_validation=None, evaluator_test=None, metric_to_optimize=\"PRECISION\",\n                               output_folder_path=\"result_experiments/\", parallelizeKNN=False, allow_weighting=True,\n                               similarity_type_list=None):\n    # If directory does not exist, create\n    if not os.path.exists(output_folder_path):\n        os.makedirs(output_folder_path)\n\n    URM_train = URM_train.copy()\n    ICM_object = ICM_object.copy()\n\n    if URM_train_last_test is not None:\n        URM_train_last_test = URM_train_last_test.copy()\n\n    ##########################################################################################################\n\n    output_file_name_root = recommender_class.RECOMMENDER_NAME + \"_{}\".format(ICM_name)\n\n    parameterSearch = SearchBayesianSkopt(recommender_class,\n                                          evaluator_validation=evaluator_validation,\n                                          evaluator_test=evaluator_test)\n\n    if similarity_type_list is None:\n        similarity_type_list = ['cosine', 'jaccard', \"asymmetric\", \"dice\", \"tversky\"]\n\n    recommender_input_args = SearchInputRecommenderArgs(\n        CONSTRUCTOR_POSITIONAL_ARGS=[URM_train, ICM_object],\n        CONSTRUCTOR_KEYWORD_ARGS={},\n        FIT_POSITIONAL_ARGS=[],\n        FIT_KEYWORD_ARGS={}\n    )\n\n    if URM_train_last_test is not None:\n        recommender_input_args_last_test = recommender_input_args.copy()\n        recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n    else:\n        recommender_input_args_last_test = None\n\n    run_KNNCBFRecommender_on_similarity_type_partial = partial(run_KNNRecommender_on_similarity_type,\n                                                               recommender_input_args=recommender_input_args,\n                                                               parameter_search_space={},\n                                                               parameterSearch=parameterSearch,\n                                                               n_cases=n_cases,\n                                                               n_random_starts=n_random_starts,\n                                                               resume_from_saved=resume_from_saved,\n                                                               save_model=save_model,\n                                                               output_folder_path=output_folder_path,\n                                                               output_file_name_root=output_file_name_root,\n                                                               metric_to_optimize=metric_to_optimize,\n                                                               allow_weighting=allow_weighting,\n                                                               recommender_input_args_last_test=recommender_input_args_last_test)\n\n    # if parallelizeKNN:\n    #     pool = multiprocessing.Pool(processes=int(multiprocessing.cpu_count()), maxtasksperchild=1)\n    #     pool.map(run_KNNCBFRecommender_on_similarity_type_partial, similarity_type_list)\n    #\n    #     pool.close()\n    #     pool.join()\n    #\n    # else:\n    #\n    for similarity_type in similarity_type_list:\n        run_KNNCBFRecommender_on_similarity_type_partial(similarity_type)\n\n\ndef runParameterSearch_Collaborative(recommender_class, URM_train, URM_train_last_test=None,\n                                     metric_to_optimize=\"PRECISION\",\n                                     evaluator_validation=None, evaluator_test=None,\n                                     evaluator_validation_earlystopping=None,\n                                     output_folder_path=\"result_experiments/\", parallelizeKNN=True,\n                                     n_cases=35, n_random_starts=5, resume_from_saved=False, save_model=\"best\",\n                                     allow_weighting=True,\n                                     similarity_type_list=None):\n\n    # If directory does not exist, create\n    if not os.path.exists(output_folder_path):\n        os.makedirs(output_folder_path)\n\n    earlystopping_keywargs = {\"validation_every_n\": 5,\n                              \"stop_on_validation\": True,\n                              \"evaluator_object\": evaluator_validation_earlystopping,\n                              \"lower_validations_allowed\": 5,\n                              \"validation_metric\": metric_to_optimize,\n                              }\n\n    URM_train = URM_train.copy()\n\n    if URM_train_last_test is not None:\n        URM_train_last_test = URM_train_last_test.copy()\n\n    try:\n\n        output_file_name_root = recommender_class.RECOMMENDER_NAME\n\n        parameterSearch = SearchBayesianSkopt(recommender_class,\n                                              evaluator_validation=evaluator_validation,\n                                              evaluator_test=evaluator_test)\n\n        # if recommender_class in [TopPop, GlobalEffects, Random]:\n        #     \"\"\"\n        #     TopPop, GlobalEffects and Random have no parameters therefore only one evaluation is needed\n        #     \"\"\"\n        #\n        #     parameterSearch = SearchSingleCase(recommender_class,\n        #                                        evaluator_validation=evaluator_validation,\n        #                                        evaluator_test=evaluator_test)\n        #\n        #     recommender_input_args = SearchInputRecommenderArgs(\n        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n        #         CONSTRUCTOR_KEYWORD_ARGS={},\n        #         FIT_POSITIONAL_ARGS=[],\n        #         FIT_KEYWORD_ARGS={}\n        #     )\n        #\n        #     if URM_train_last_test is not None:\n        #         recommender_input_args_last_test = recommender_input_args.copy()\n        #         recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n        #     else:\n        #         recommender_input_args_last_test = None\n        #\n        #     parameterSearch.search(recommender_input_args,\n        #                            recommender_input_args_last_test=recommender_input_args_last_test,\n        #                            fit_hyperparameters_values={},\n        #                            output_folder_path=output_folder_path,\n        #                            output_file_name_root=output_file_name_root,\n        #                            resume_from_saved=resume_from_saved,\n        #                            save_model=save_model,\n        #                            )\n        #\n        #     return\n\n        ##########################################################################################################\n\n        if recommender_class in [ItemKNNCFRecommender, UserKNNCFRecommender]:\n\n            if similarity_type_list is None:\n                similarity_type_list = ['cosine', 'jaccard', \"asymmetric\", \"dice\", \"tversky\"]\n\n            recommender_input_args = SearchInputRecommenderArgs(\n                CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n                CONSTRUCTOR_KEYWORD_ARGS={},\n                FIT_POSITIONAL_ARGS=[],\n                FIT_KEYWORD_ARGS={}\n            )\n\n            if URM_train_last_test is not None:\n                recommender_input_args_last_test = recommender_input_args.copy()\n                recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n            else:\n                recommender_input_args_last_test = None\n\n            run_KNNCFRecommender_on_similarity_type_partial = partial(run_KNNRecommender_on_similarity_type,\n                                                                      recommender_input_args=recommender_input_args,\n                                                                      parameter_search_space={},\n                                                                      parameterSearch=parameterSearch,\n                                                                      n_cases=n_cases,\n                                                                      n_random_starts=n_random_starts,\n                                                                      resume_from_saved=resume_from_saved,\n                                                                      save_model=save_model,\n                                                                      output_folder_path=output_folder_path,\n                                                                      output_file_name_root=output_file_name_root,\n                                                                      metric_to_optimize=metric_to_optimize,\n                                                                      allow_weighting=allow_weighting,\n                                                                      recommender_input_args_last_test=recommender_input_args_last_test)\n\n            # if parallelizeKNN:\n            #     pool = multiprocessing.Pool(processes=multiprocessing.cpu_count(), maxtasksperchild=1)\n            #     pool.map(run_KNNCFRecommender_on_similarity_type_partial, similarity_type_list)\n            #\n            #     pool.close()\n            #     pool.join()\n            #\n            # else:\n\n            for similarity_type in similarity_type_list:\n                run_KNNCFRecommender_on_similarity_type_partial(similarity_type)\n\n            return\n\n        ##########################################################################################################\n\n        # if recommender_class is P3alphaRecommender:\n        #     hyperparameters_range_dictionary = {}\n        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=0, high=2, prior='uniform')\n        #     hyperparameters_range_dictionary[\"normalize_similarity\"] = Categorical([True, False])\n        #\n        #     recommender_input_args = SearchInputRecommenderArgs(\n        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n        #         CONSTRUCTOR_KEYWORD_ARGS={},\n        #         FIT_POSITIONAL_ARGS=[],\n        #         FIT_KEYWORD_ARGS={}\n        #     )\n\n        ##########################################################################################################\n\n        # if recommender_class is RP3betaRecommender:\n        #     hyperparameters_range_dictionary = {}\n        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=0, high=2, prior='uniform')\n        #     hyperparameters_range_dictionary[\"beta\"] = Real(low=0, high=2, prior='uniform')\n        #     hyperparameters_range_dictionary[\"normalize_similarity\"] = Categorical([True, False])\n        #\n        #     recommender_input_args = SearchInputRecommenderArgs(\n        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n        #         CONSTRUCTOR_KEYWORD_ARGS={},\n        #         FIT_POSITIONAL_ARGS=[],\n        #         FIT_KEYWORD_ARGS={}\n        #     )\n\n        ##########################################################################################################\n\n        # if recommender_class is MatrixFactorization_FunkSVD_Cython:\n        #     hyperparameters_range_dictionary = {}\n        #     hyperparameters_range_dictionary[\"sgd_mode\"] = Categorical([\"sgd\", \"adagrad\", \"adam\"])\n        #     hyperparameters_range_dictionary[\"epochs\"] = Categorical([500])\n        #     hyperparameters_range_dictionary[\"use_bias\"] = Categorical([True, False])\n        #     hyperparameters_range_dictionary[\"batch_size\"] = Categorical([1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024])\n        #     hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n        #     hyperparameters_range_dictionary[\"item_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n        #     hyperparameters_range_dictionary[\"user_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n        #     hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n        #     hyperparameters_range_dictionary[\"negative_interactions_quota\"] = Real(low=0.0, high=0.5, prior='uniform')\n        #\n        #     recommender_input_args = SearchInputRecommenderArgs(\n        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n        #         CONSTRUCTOR_KEYWORD_ARGS={},\n        #         FIT_POSITIONAL_ARGS=[],\n        #         FIT_KEYWORD_ARGS=earlystopping_keywargs\n        #     )\n\n        ##########################################################################################################\n\n        # if recommender_class is MatrixFactorization_AsySVD_Cython:\n        #     hyperparameters_range_dictionary = {}\n        #     hyperparameters_range_dictionary[\"sgd_mode\"] = Categorical([\"sgd\", \"adagrad\", \"adam\"])\n        #     hyperparameters_range_dictionary[\"epochs\"] = Categorical([500])\n        #     hyperparameters_range_dictionary[\"use_bias\"] = Categorical([True, False])\n        #     hyperparameters_range_dictionary[\"batch_size\"] = Categorical([1])\n        #     hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n        #     hyperparameters_range_dictionary[\"item_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n        #     hyperparameters_range_dictionary[\"user_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n        #     hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n        #     hyperparameters_range_dictionary[\"negative_interactions_quota\"] = Real(low=0.0, high=0.5, prior='uniform')\n        #\n        #     recommender_input_args = SearchInputRecommenderArgs(\n        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n        #         CONSTRUCTOR_KEYWORD_ARGS={},\n        #         FIT_POSITIONAL_ARGS=[],\n        #         FIT_KEYWORD_ARGS=earlystopping_keywargs\n        #     )\n\n        ##########################################################################################################\n\n#         if recommender_class is MatrixFactorization_BPR_Cython:\n#             hyperparameters_range_dictionary = {}\n#             hyperparameters_range_dictionary[\"sgd_mode\"] = Categorical([\"sgd\", \"adagrad\", \"adam\"])\n#             hyperparameters_range_dictionary[\"epochs\"] = Categorical([1500]) \n#             hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n#             hyperparameters_range_dictionary[\"batch_size\"] = Categorical([1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024])\n#             hyperparameters_range_dictionary[\"positive_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n#             hyperparameters_range_dictionary[\"negative_reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n#             hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n        \n#             recommender_input_args = SearchInputRecommenderArgs(\n#                 CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n#                 CONSTRUCTOR_KEYWORD_ARGS={},\n#                 FIT_POSITIONAL_ARGS=[],\n#                 FIT_KEYWORD_ARGS={**earlystopping_keywargs,\n#                                   \"positive_threshold_BPR\": None}\n#             )\n\n        ##########################################################################################################\n\n        # if recommender_class is IALSRecommender:\n        #     hyperparameters_range_dictionary = {}\n        #     hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 200)\n        #     hyperparameters_range_dictionary[\"confidence_scaling\"] = Categorical([\"linear\", \"log\"])\n        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=1e-3, high=50.0, prior='log-uniform')\n        #     hyperparameters_range_dictionary[\"epsilon\"] = Real(low=1e-3, high=10.0, prior='log-uniform')\n        #     hyperparameters_range_dictionary[\"reg\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n        #\n        #     recommender_input_args = SearchInputRecommenderArgs(\n        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n        #         CONSTRUCTOR_KEYWORD_ARGS={},\n        #         FIT_POSITIONAL_ARGS=[],\n        #         FIT_KEYWORD_ARGS=earlystopping_keywargs\n        #     )\n\n        ##########################################################################################################\n\n        if recommender_class is PureSVDRecommender:\n            hyperparameters_range_dictionary = {}\n            hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 350)\n\n            recommender_input_args = SearchInputRecommenderArgs(\n                CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n                CONSTRUCTOR_KEYWORD_ARGS={},\n                FIT_POSITIONAL_ARGS=[],\n                FIT_KEYWORD_ARGS={}\n            )\n\n        ##########################################################################################################\n\n        # if recommender_class is NMFRecommender:\n        #     hyperparameters_range_dictionary = {}\n        #     hyperparameters_range_dictionary[\"num_factors\"] = Integer(1, 350)\n        #     hyperparameters_range_dictionary[\"solver\"] = Categorical([\"coordinate_descent\", \"multiplicative_update\"])\n        #     hyperparameters_range_dictionary[\"init_type\"] = Categorical([\"random\", \"nndsvda\"])\n        #     hyperparameters_range_dictionary[\"beta_loss\"] = Categorical([\"frobenius\", \"kullback-leibler\"])\n        #\n        #     recommender_input_args = SearchInputRecommenderArgs(\n        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n        #         CONSTRUCTOR_KEYWORD_ARGS={},\n        #         FIT_POSITIONAL_ARGS=[],\n        #         FIT_KEYWORD_ARGS={}\n        #     )\n\n        #########################################################################################################\n\n        # if recommender_class is SLIM_BPR_Cython:\n        #     hyperparameters_range_dictionary = {}\n        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n        #     hyperparameters_range_dictionary[\"epochs\"] = Categorical([1500])\n        #     hyperparameters_range_dictionary[\"symmetric\"] = Categorical([True, False])\n        #     hyperparameters_range_dictionary[\"sgd_mode\"] = Categorical([\"sgd\", \"adagrad\", \"adam\"])\n        #     hyperparameters_range_dictionary[\"lambda_i\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n        #     hyperparameters_range_dictionary[\"lambda_j\"] = Real(low=1e-5, high=1e-2, prior='log-uniform')\n        #     hyperparameters_range_dictionary[\"learning_rate\"] = Real(low=1e-4, high=1e-1, prior='log-uniform')\n        #\n        #     recommender_input_args = SearchInputRecommenderArgs(\n        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n        #         CONSTRUCTOR_KEYWORD_ARGS={},\n        #         FIT_POSITIONAL_ARGS=[],\n        #         FIT_KEYWORD_ARGS={**earlystopping_keywargs,\n        #                           \"positive_threshold_BPR\": None,\n        #                           'train_with_sparse_weights': None}\n        #     )\n\n        ##########################################################################################################\n        #\n        # if recommender_class is SLIMElasticNetRecommender:\n        #     hyperparameters_range_dictionary = {}\n        #     hyperparameters_range_dictionary[\"topK\"] = Integer(5, 1000)\n        #     hyperparameters_range_dictionary[\"l1_ratio\"] = Real(low=1e-5, high=1.0, prior='log-uniform')\n        #     hyperparameters_range_dictionary[\"alpha\"] = Real(low=1e-3, high=1.0, prior='uniform')\n        #\n        #     recommender_input_args = SearchInputRecommenderArgs(\n        #         CONSTRUCTOR_POSITIONAL_ARGS=[URM_train],\n        #         CONSTRUCTOR_KEYWORD_ARGS={},\n        #         FIT_POSITIONAL_ARGS=[],\n        #         FIT_KEYWORD_ARGS={}\n        #     )\n\n        #########################################################################################################\n\n        if URM_train_last_test is not None:\n            recommender_input_args_last_test = recommender_input_args.copy()\n            recommender_input_args_last_test.CONSTRUCTOR_POSITIONAL_ARGS[0] = URM_train_last_test\n        else:\n            recommender_input_args_last_test = None\n\n        ## Final step, after the hyperparameter range has been defined for each type of algorithm\n        parameterSearch.search(recommender_input_args,\n                               parameter_search_space=hyperparameters_range_dictionary,\n                               n_cases=n_cases,\n                               n_random_starts=n_random_starts,\n                               resume_from_saved=resume_from_saved,\n                               save_model=save_model,\n                               output_folder_path=output_folder_path,\n                               output_file_name_root=output_file_name_root,\n                               metric_to_optimize=metric_to_optimize,\n                               recommender_input_args_last_test=recommender_input_args_last_test)\n\n    except Exception as e:\n\n        print(\"On recommender {} Exception {}\".format(recommender_class, str(e)))\n        traceback.print_exc()\n\n        error_file = open(output_folder_path + \"ErrorLog.txt\", \"a\")\n        error_file.write(\"On recommender {} Exception {}\\n\".format(recommender_class, str(e)))\n        error_file.close()\n","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Data IO**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataIO.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 27/04/2019\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\nimport os, json, zipfile, shutil, platform\n\nimport scipy.sparse as sps\nfrom pandas import DataFrame\nimport pandas as pd\nimport numpy as np\n\n\ndef json_not_serializable_handler(o):\n    \"\"\"\n    Json cannot serialize automatically some data types, for example numpy integers (int32).\n    This may be a limitation of numpy-json interfaces for Python 3.6 and may not occur in Python 3.7\n    :param o:\n    :return:\n    \"\"\"\n\n    if isinstance(o, np.integer):\n        return int(o)\n\n    raise TypeError(\"json_not_serializable_handler: object '{}' is not serializable.\".format(type(o)))\n\n\nclass DataIO(object):\n    \"\"\" DataIO\"\"\"\n\n    _DEFAULT_TEMP_FOLDER = \".temp_DataIO_\"\n\n    # _MAX_PATH_LENGTH_LINUX = 4096\n    _MAX_PATH_LENGTH_WINDOWS = 255\n\n    def __init__(self, folder_path):\n        super(DataIO, self).__init__()\n\n        self._is_windows = platform.system() == \"Windows\"\n\n        self.folder_path = folder_path\n        self._key_string_alert_done = False\n\n        # if self._is_windows:\n        #     self.folder_path = \"\\\\\\\\?\\\\\" + self.folder_path\n\n\n    def _print(self, message):\n        print(\"{}: {}\".format(\"DataIO\", message))\n\n\n    def _get_temp_folder(self, file_name):\n        \"\"\"\n        Creates a temporary folder to be used during the data saving\n        :return:\n        \"\"\"\n\n        # Ignore the .zip extension\n        file_name = file_name[:-4]\n\n        current_temp_folder = \"{}{}_{}/\".format(self.folder_path, self._DEFAULT_TEMP_FOLDER, file_name)\n\n        if os.path.exists(current_temp_folder):\n            self._print(\"Folder {} already exists, could be the result of a previous failed save attempt or multiple saver are active in parallel. \" \\\n            \"Folder will be removed.\".format(current_temp_folder))\n\n            shutil.rmtree(current_temp_folder, ignore_errors=True)\n\n        os.makedirs(current_temp_folder)\n\n        return current_temp_folder\n\n\n    def _check_dict_key_type(self, dict_to_save):\n        \"\"\"\n        Check whether the keys of the dictionary are string. If not, transforms them into strings\n        :param dict_to_save:\n        :return:\n        \"\"\"\n\n        all_keys_are_str = all(isinstance(key, str) for key in dict_to_save.keys())\n\n        if all_keys_are_str:\n            return dict_to_save\n\n        if not self._key_string_alert_done:\n            self._print(\"Json dumps supports only 'str' as dictionary keys. Transforming keys to string, note that this will alter the mapper content.\")\n            self._key_string_alert_done = True\n\n        dict_to_save_key_str = {str(key):val for (key,val) in dict_to_save.items()}\n\n        assert all(dict_to_save_key_str[str(key)] == val for (key,val) in dict_to_save.items()), \\\n            \"DataIO: Transforming dictionary keys into strings altered its content. Duplicate keys may have been produced.\"\n\n        return dict_to_save_key_str\n\n\n    def save_data(self, file_name, data_dict_to_save):\n\n        # If directory does not exist, create with .temp_model_folder\n        if not os.path.exists(self.folder_path):\n            os.makedirs(self.folder_path)\n\n        if file_name[-4:] != \".zip\":\n            file_name += \".zip\"\n\n\n        current_temp_folder = self._get_temp_folder(file_name)\n\n        attribute_to_file_name = {}\n        attribute_to_json_file = {}\n\n        for attrib_name, attrib_data in data_dict_to_save.items():\n\n            current_file_path = current_temp_folder + attrib_name\n\n            if isinstance(attrib_data, DataFrame):\n                attrib_data.to_csv(current_file_path, index=False)\n                attribute_to_file_name[attrib_name] = attrib_name + \".csv\"\n\n            elif isinstance(attrib_data, sps.spmatrix):\n                sps.save_npz(current_file_path, attrib_data)\n                attribute_to_file_name[attrib_name] = attrib_name + \".npz\"\n\n            elif isinstance(attrib_data, np.ndarray):\n                # allow_pickle is FALSE to prevent using pickle and ensure portability\n                np.save(current_file_path, attrib_data, allow_pickle=False)\n                attribute_to_file_name[attrib_name] = attrib_name + \".npy\"\n\n            else:\n                attribute_to_json_file[attrib_name] = attrib_data\n                attribute_to_file_name[attrib_name] = attrib_name + \".json\"\n\n\n        # Save list objects\n        attribute_to_json_file[\".DataIO_attribute_to_file_name\"] = attribute_to_file_name.copy()\n\n        for attrib_name, attrib_data in attribute_to_json_file.items():\n\n            current_file_path = current_temp_folder + attrib_name\n            attribute_to_file_name[attrib_name] = attrib_name + \".json\"\n\n            # if self._is_windows and len(current_file_path + \".json\") >= self._MAX_PATH_LENGTH_WINDOWS:\n            #     current_file_path = \"\\\\\\\\?\\\\\" + current_file_path\n\n            absolute_path = current_file_path + \".json\" if current_file_path.startswith(os.getcwd()) else os.getcwd() + current_file_path + \".json\"\n\n            assert not self._is_windows or (self._is_windows and len(absolute_path) <= self._MAX_PATH_LENGTH_WINDOWS), \\\n                \"DataIO: Path of file exceeds {} characters, which is the maximum allowed under standard paths for Windows.\".format(self._MAX_PATH_LENGTH_WINDOWS)\n\n\n            with open(current_file_path + \".json\", 'w') as outfile:\n\n                if isinstance(attrib_data, dict):\n                    attrib_data = self._check_dict_key_type(attrib_data)\n\n                json.dump(attrib_data, outfile, default=json_not_serializable_handler)\n\n\n\n        with zipfile.ZipFile(self.folder_path + file_name, 'w', compression=zipfile.ZIP_DEFLATED) as myzip:\n\n            for file_name in attribute_to_file_name.values():\n                myzip.write(current_temp_folder + file_name, arcname = file_name)\n\n\n\n        shutil.rmtree(current_temp_folder, ignore_errors=True)\n\n\n    def load_data(self, file_name):\n\n        if file_name[-4:] != \".zip\":\n            file_name += \".zip\"\n\n        dataFile = zipfile.ZipFile(self.folder_path + file_name)\n\n        dataFile.testzip()\n\n        current_temp_folder = self._get_temp_folder(file_name)\n\n        try:\n\n            try:\n                attribute_to_file_name_path = dataFile.extract(\".DataIO_attribute_to_file_name.json\", path = current_temp_folder)\n            except KeyError:\n                attribute_to_file_name_path = dataFile.extract(\"__DataIO_attribute_to_file_name.json\", path = current_temp_folder)\n\n\n            with open(attribute_to_file_name_path, \"r\") as json_file:\n                attribute_to_file_name = json.load(json_file)\n\n            data_dict_loaded = {}\n\n            for attrib_name, file_name in attribute_to_file_name.items():\n\n                attrib_file_path = dataFile.extract(file_name, path = current_temp_folder)\n                attrib_data_type = file_name.split(\".\")[-1]\n\n                if attrib_data_type == \"csv\":\n                    attrib_data = pd.read_csv(attrib_file_path, index_col=False)\n\n                elif attrib_data_type == \"npz\":\n                    attrib_data = sps.load_npz(attrib_file_path)\n\n                elif attrib_data_type == \"npy\":\n                    # allow_pickle is FALSE to prevent using pickle and ensure portability\n                    attrib_data = np.load(attrib_file_path, allow_pickle=False)\n\n                elif attrib_data_type == \"json\":\n                    with open(attrib_file_path, \"r\") as json_file:\n                        attrib_data = json.load(json_file)\n\n                else:\n                    raise Exception(\"Attribute type not recognized for: '{}' of class: '{}'\".format(attrib_file_path, attrib_data_type))\n\n                data_dict_loaded[attrib_name] = attrib_data\n\n\n        except Exception as exec:\n\n            shutil.rmtree(current_temp_folder, ignore_errors=True)\n            raise exec\n\n        shutil.rmtree(current_temp_folder, ignore_errors=True)\n\n\n        return data_dict_loaded","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Create submission file**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create_submission_file.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n#  -*- coding: utf-8 -*-\n\n\"\"\"\n\tcreate_submission_file.py: module for creating the formatted csv submission file.\n\"\"\"\n\nimport os\nfrom datetime import datetime\n\n\ndef create_csv(top_10_items, recommender):\n    print(\"\\nGenerating submission csv ... \")\n\n    # save on a different dir according to the recommender used\n    submissions_dir = './submissions/' + recommender\n\n    # If directory for the recommender does not exist, create\n    if not os.path.exists(submissions_dir):\n        os.makedirs(submissions_dir)\n\n    csv_fname = 'submission_'\n    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n\n    file_path = os.path.join(submissions_dir, csv_fname)\n\n    with open(file_path, 'w') as f:\n\n        fieldnames = 'user_id,item_list'\n        f.write(fieldnames + '\\n')\n\n        for user_id, item_list in top_10_items.items():\n            row = str(user_id) + ',' + str(item_list) + '\\n'\n            f.write(row.replace('[', '').replace(']', ''))  # remove '[' ']' from item_list string\n","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Compute similarity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute_similarity.py\n# ------------------------------------------------------------------\n\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 23/10/17\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\nimport numpy as np\nimport time, sys\nimport scipy.sparse as sps\n# from utils import data_manager as dp\n\n\nclass Compute_Similarity_Python:\n\n    def __init__(self, dataMatrix, topK=100, shrink=0, normalize=True,\n                 asymmetric_alpha=0.5, tversky_alpha=1.0, tversky_beta=1.0,\n                 similarity=\"cosine\", row_weights=None):\n        \"\"\"\n        Computes the cosine similarity on the columns of dataMatrix\n        If it is computed on URM=|users|x|items|, pass the URM as is.\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n        :param dataMatrix:\n        :param topK:\n        :param shrink:\n        :param normalize:           If True divide the dot product by the product of the norms\n        :param row_weights:         Multiply the values in each row by a specified value. Array\n        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\n        :param similarity:  \"cosine\"        computes Cosine similarity\n                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\n                            \"asymmetric\"    computes Asymmetric Cosine\n                            \"pearson\"       computes Pearson Correlation, removing the average of the items\n                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\n                            \"dice\"          computes Dice similarity for binary interactions\n                            \"tversky\"       computes Tversky similarity for binary interactions\n                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\n        \"\"\"\n\n        super(Compute_Similarity_Python, self).__init__()\n\n        self.shrink = shrink\n        self.normalize = normalize\n\n        self.n_rows, self.n_columns = dataMatrix.shape\n        self.TopK = min(topK, self.n_columns)\n\n        self.asymmetric_alpha = asymmetric_alpha\n        self.tversky_alpha = tversky_alpha\n        self.tversky_beta = tversky_beta\n\n        self.dataMatrix = dataMatrix.copy()\n\n        self.adjusted_cosine = False\n        self.asymmetric_cosine = False\n        self.pearson_correlation = False\n        self.tanimoto_coefficient = False\n        self.dice_coefficient = False\n        self.tversky_coefficient = False\n\n        if similarity == \"adjusted\":\n            self.adjusted_cosine = True\n        elif similarity == \"asymmetric\":\n            self.asymmetric_cosine = True\n        elif similarity == \"pearson\":\n            self.pearson_correlation = True\n        elif similarity == \"jaccard\" or similarity == \"tanimoto\":\n            self.tanimoto_coefficient = True\n            # Tanimoto has a specific kind of normalization\n            self.normalize = False\n\n        elif similarity == \"dice\":\n            self.dice_coefficient = True\n            self.normalize = False\n\n        elif similarity == \"tversky\":\n            self.tversky_coefficient = True\n            self.normalize = False\n\n        elif similarity == \"cosine\":\n            pass\n        else:\n            raise ValueError(\"Cosine_Similarity: value for parameter 'mode' not recognized.\"\n                             \" Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',\"\n                             \"dice, tversky.\"\n                             \" Passed value was '{}'\".format(similarity))\n\n        self.use_row_weights = False\n\n        if row_weights is not None:\n\n            if dataMatrix.shape[0] != len(row_weights):\n                raise ValueError(\"Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.\"\n                                 \"Col_weights has {} columns, dataMatrix has {}.\".format(len(row_weights),\n                                                                                         dataMatrix.shape[0]))\n\n            self.use_row_weights = True\n            self.row_weights = row_weights.copy()\n            self.row_weights_diag = sps.diags(self.row_weights)\n\n            self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T\n\n    def applyAdjustedCosine(self):\n        \"\"\"\n        Remove from every data point the average for the corresponding row\n        :return:\n        \"\"\"\n\n        self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n\n        interactionsPerRow = np.diff(self.dataMatrix.indptr)\n\n        nonzeroRows = interactionsPerRow > 0\n        sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n\n        rowAverage = np.zeros_like(sumPerRow)\n        rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n\n        # Split in blocks to avoid duplicating the whole data structure\n        start_row = 0\n        end_row = 0\n\n        blockSize = 1000\n\n        while end_row < self.n_rows:\n            end_row = min(self.n_rows, end_row + blockSize)\n\n            self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= \\\n                np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n\n            start_row += blockSize\n\n    def applyPearsonCorrelation(self):\n        \"\"\"\n        Remove from every data point the average for the corresponding column\n        :return:\n        \"\"\"\n\n        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n\n        interactionsPerCol = np.diff(self.dataMatrix.indptr)\n\n        nonzeroCols = interactionsPerCol > 0\n        sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n\n        colAverage = np.zeros_like(sumPerCol)\n        colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n\n        # Split in blocks to avoid duplicating the whole data structure\n        start_col = 0\n        end_col = 0\n\n        blockSize = 1000\n\n        while end_col < self.n_columns:\n            end_col = min(self.n_columns, end_col + blockSize)\n\n            self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= \\\n                np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n\n            start_col += blockSize\n\n    def useOnlyBooleanInteractions(self):\n\n        # Split in blocks to avoid duplicating the whole data structure\n        start_pos = 0\n        end_pos = 0\n\n        blockSize = 1000\n\n        while end_pos < len(self.dataMatrix.data):\n            end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n\n            self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos - start_pos)\n\n            start_pos += blockSize\n\n    def compute_similarity(self, start_col=None, end_col=None, block_size=100):\n        \"\"\"\n        Compute the similarity for the given dataset\n        :param self:\n        :param start_col: column to begin with\n        :param end_col: column to stop before, end_col is excluded\n        :return:\n        \"\"\"\n\n        values = []\n        rows = []\n        cols = []\n\n        start_time = time.time()\n        start_time_print_batch = start_time\n        processedItems = 0\n\n        if self.adjusted_cosine:\n            self.applyAdjustedCosine()\n\n        elif self.pearson_correlation:\n            self.applyPearsonCorrelation()\n\n        elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n            self.useOnlyBooleanInteractions()\n\n        # We explore the matrix column-wise\n        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n\n        # Compute sum of squared values to be used in normalization\n        sumOfSquared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n\n        # Tanimoto does not require the square root to be applied\n        if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n            sumOfSquared = np.sqrt(sumOfSquared)\n\n        if self.asymmetric_cosine:\n            sumOfSquared_to_1_minus_alpha = np.power(sumOfSquared, 2 * (1 - self.asymmetric_alpha))\n            sumOfSquared_to_alpha = np.power(sumOfSquared, 2 * self.asymmetric_alpha)\n\n        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n\n        start_col_local = 0\n        end_col_local = self.n_columns\n\n        if start_col is not None and start_col > 0 and start_col < self.n_columns:\n            start_col_local = start_col\n\n        if end_col is not None and end_col > start_col_local and end_col < self.n_columns:\n            end_col_local = end_col\n\n        start_col_block = start_col_local\n\n        this_block_size = 0\n\n        # Compute all similarities for each item using vectorization\n        while start_col_block < end_col_local:\n\n            end_col_block = min(start_col_block + block_size, end_col_local)\n            this_block_size = end_col_block - start_col_block\n\n            # All data points for a given item\n            item_data = self.dataMatrix[:, start_col_block:end_col_block]\n            item_data = item_data.toarray().squeeze()\n\n            # If only 1 feature avoid last dimension to disappear\n            if item_data.ndim == 1:\n                item_data = np.atleast_2d(item_data)\n\n            if self.use_row_weights:\n                this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n\n            else:\n                # Compute item similarities => using dot product\n                this_block_weights = self.dataMatrix.T.dot(item_data)\n\n            for col_index_in_block in range(this_block_size):\n\n                if this_block_size == 1:\n                    this_column_weights = this_block_weights\n                else:\n                    this_column_weights = this_block_weights[:, col_index_in_block]\n\n                columnIndex = col_index_in_block + start_col_block\n                this_column_weights[columnIndex] = 0.0\n\n                # Apply normalization and shrinkage, ensure denominator != 0\n                if self.normalize:\n\n                    if self.asymmetric_cosine:\n                        denominator = sumOfSquared_to_alpha[\n                                          columnIndex] * sumOfSquared_to_1_minus_alpha + self.shrink + 1e-6\n                    else:\n                        denominator = sumOfSquared[columnIndex] * sumOfSquared + self.shrink + 1e-6\n\n                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n\n\n                # Apply the specific denominator for Tanimoto\n                elif self.tanimoto_coefficient:\n                    denominator = sumOfSquared[columnIndex] + sumOfSquared - this_column_weights + self.shrink + 1e-6\n                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n\n                elif self.dice_coefficient:\n                    denominator = sumOfSquared[columnIndex] + sumOfSquared + self.shrink + 1e-6\n                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n\n                elif self.tversky_coefficient:\n                    denominator = this_column_weights + \\\n                                  (sumOfSquared[columnIndex] - this_column_weights) * self.tversky_alpha + \\\n                                  (sumOfSquared - this_column_weights) * self.tversky_beta + self.shrink + 1e-6\n                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n\n                # If no normalization or tanimoto is selected, apply only shrink\n                elif self.shrink != 0:\n                    this_column_weights = this_column_weights / self.shrink\n\n                # this_column_weights = this_column_weights.toarray().ravel()\n\n                # Sort indices and select TopK\n                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n                # - Partition the data to extract the set of relevant items\n                # - Sort only the relevant items\n                # - Get the original item index\n                relevant_items_partition = (-this_column_weights).argpartition(self.TopK - 1)[0:self.TopK]\n                relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n                top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n\n                # Incrementally build sparse matrix, do not add zeros\n                notZerosMask = this_column_weights[top_k_idx] != 0.0\n                numNotZeros = np.sum(notZerosMask)\n\n                values.extend(this_column_weights[top_k_idx][notZerosMask])\n                rows.extend(top_k_idx[notZerosMask])\n                cols.extend(np.ones(numNotZeros) * columnIndex)\n\n            # Add previous block size\n            processedItems += this_block_size\n\n            if time.time() - start_time_print_batch >= 30 or end_col_block == end_col_local:\n                columnPerSec = processedItems / (time.time() - start_time + 1e-9)\n\n                print(\"Similarity column {} ( {:2.0f} % ), {:.2f} column/sec, elapsed time {:.2f} min\".format(\n                    processedItems, processedItems / (end_col_local - start_col_local) * 100, columnPerSec,\n                                    (time.time() - start_time) / 60))\n\n                sys.stdout.flush()\n                sys.stderr.flush()\n\n                start_time_print_batch = time.time()\n\n            start_col_block += block_size\n\n        # End while on columns\n\n        W_sparse = sps.csr_matrix((values, (rows, cols)),\n                                  shape=(self.n_columns, self.n_columns),\n                                  dtype=np.float32)\n\n        return W_sparse\n\n\ndef similarityMatrixTopK(item_weights, forceSparseOutput=True, k=100, verbose=False, inplace=True):\n    \"\"\"\n    The function selects the TopK most similar elements, column-wise\n\n    :param item_weights:\n    :param forceSparseOutput:\n    :param k:\n    :param verbose:\n    :param inplace: Default True, WARNING matrix will be modified\n    :return:\n    \"\"\"\n\n    assert (item_weights.shape[0] == item_weights.shape[1]), \"selectTopK: ItemWeights is not a square matrix\"\n\n    start_time = time.time()\n\n    if verbose:\n        print(\"Generating topK matrix\")\n\n    nitems = item_weights.shape[1]\n    k = min(k, nitems)\n\n    # for each column, keep only the top-k scored items\n    sparse_weights = not isinstance(item_weights, np.ndarray)\n\n    if not sparse_weights:\n\n        idx_sorted = np.argsort(item_weights, axis=0)  # sort data inside each column\n\n        if inplace:\n            W = item_weights\n        else:\n            W = item_weights.copy()\n\n        # index of the items that don't belong to the top-k similar items of each column\n        not_top_k = idx_sorted[:-k, :]\n        # use numpy fancy indexing to zero-out the values in sim without using a for loop\n        W[not_top_k, np.arange(nitems)] = 0.0\n\n        if forceSparseOutput:\n            W_sparse = sps.csr_matrix(W, shape=(nitems, nitems))\n\n            if verbose:\n                print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n\n            return W_sparse\n\n        if verbose:\n            print(\"Dense TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n\n        return W\n\n    else:\n        # iterate over each column and keep only the top-k similar items\n        data, rows_indices, cols_indptr = [], [], []\n\n        item_weights = check_matrix(item_weights, format='csc', dtype=np.float32)\n\n        for item_idx in range(nitems):\n            cols_indptr.append(len(data))\n\n            start_position = item_weights.indptr[item_idx]\n            end_position = item_weights.indptr[item_idx + 1]\n\n            column_data = item_weights.data[start_position:end_position]\n            column_row_index = item_weights.indices[start_position:end_position]\n\n            non_zero_data = column_data != 0\n\n            idx_sorted = np.argsort(column_data[non_zero_data])  # sort by column\n            top_k_idx = idx_sorted[-k:]\n\n            data.extend(column_data[non_zero_data][top_k_idx])\n            rows_indices.extend(column_row_index[non_zero_data][top_k_idx])\n\n        cols_indptr.append(len(data))\n\n        # During testing CSR is faster\n        W_sparse = sps.csc_matrix((data, rows_indices, cols_indptr), shape=(nitems, nitems), dtype=np.float32)\n        W_sparse = W_sparse.tocsr()\n\n        if verbose:\n            print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n\n        return W_sparse\n\n\n# Transforms matrix into a specific format\ndef check_matrix(X, format='csc', dtype=np.float32):\n    \"\"\"\n        This function takes a matrix as input and transforms it into the specified format.\n        The matrix in input can be either sparse or ndarray.\n        If the matrix in input has already the desired format, it is returned as-is\n        the dtype parameter is always applied and the default is np.float32\n        :param X:\n        :param format:\n        :param dtype:\n        :return:\n    \"\"\"\n\n    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n        return X.tocsc().astype(dtype)  # Compressed Sparse Column format\n    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n        return X.tocsr().astype(dtype)  # Compressed Sparse Row format\n    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n        return X.tocoo().astype(dtype)\n    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n        return X.todok().astype(dtype)\n    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n        return X.tobsr().astype(dtype)\n    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n        return X.todia().astype(dtype)\n    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n        return X.tolil().astype(dtype)\n    elif isinstance(X, np.ndarray):\n        X = sps.csr_matrix(X, dtype=dtype)\n        X.eliminate_zeros()\n        return check_matrix(X, format=format, dtype=dtype)\n    else:\n        return X.astype(dtype)\n\n\n\nfrom enum import Enum\n\nclass SimilarityFunction(Enum):\n    COSINE = \"cosine\"\n    PEARSON = \"pearson\"\n    JACCARD = \"jaccard\"\n    TANIMOTO = \"tanimoto\"\n    ADJUSTED_COSINE = \"adjusted\"\n    EUCLIDEAN = \"euclidean\"\n\n\nclass Compute_Similarity:\n\n\n    def __init__(self, dataMatrix, use_implementation = \"density\", similarity = None, **args):\n        \"\"\"\n        Interface object that will call the appropriate similarity implementation\n        :param dataMatrix:\n        :param use_implementation:      \"density\" will choose the most efficient implementation automatically\n                                        \"cython\" will use the cython implementation, if available. Most efficient for sparse matrix\n                                        \"python\" will use the python implementation. Most efficent for dense matrix\n        :param similarity:              the type of similarity to use, see SimilarityFunction enum\n        :param args:                    other args required by the specific similarity implementation\n        \"\"\"\n\n        assert np.all(np.isfinite(dataMatrix.data)), \\\n            \"Compute_Similarity: Data matrix contains {} non finite values\".format(np.sum(np.logical_not(np.isfinite(dataMatrix.data))))\n\n        self.dense = False\n\n        if similarity == \"euclidean\":\n            # This is only available here\n            self.compute_similarity_object = Compute_Similarity_Euclidean(dataMatrix, **args)\n\n        else:\n\n            assert not (dataMatrix.shape[0] == 1 and dataMatrix.nnz == dataMatrix.shape[1]),\\\n                \"Compute_Similarity: data has only 1 feature (shape: {}) with dense values,\" \\\n                \" vector and set based similarities are not defined on 1-dimensional dense data,\" \\\n                \" use Euclidean similarity instead.\".format(dataMatrix.shape)\n\n            if similarity is not None:\n                args[\"similarity\"] = similarity\n\n\n            if use_implementation == \"density\":\n\n                if isinstance(dataMatrix, np.ndarray):\n                    self.dense = True\n\n                elif isinstance(dataMatrix, sps.spmatrix):\n                    shape = dataMatrix.shape\n\n                    num_cells = shape[0]*shape[1]\n\n                    sparsity = dataMatrix.nnz/num_cells\n\n                    self.dense = sparsity > 0.5\n\n                else:\n                    print(\"Compute_Similarity: matrix type not recognized, calling default...\")\n                    use_implementation = \"python\"\n\n                if self.dense:\n                    print(\"Compute_Similarity: detected dense matrix\")\n                    use_implementation = \"python\"\n                else:\n                    use_implementation = \"cython\"\n\n\n            if use_implementation == \"cython\":\n\n                try:\n                    from Base.Similarity.Cython.Compute_Similarity_Cython import Compute_Similarity_Cython\n                    self.compute_similarity_object = Compute_Similarity_Cython(dataMatrix, **args)\n\n                except ImportError:\n                    print(\"Unable to load Cython Compute_Similarity, reverting to Python\")\n                    self.compute_similarity_object = Compute_Similarity_Python(dataMatrix, **args)\n\n\n            elif use_implementation == \"python\":\n                self.compute_similarity_object = Compute_Similarity_Python(dataMatrix, **args)\n\n            else:\n\n                raise  ValueError(\"Compute_Similarity: value for argument 'use_implementation' not recognized\")\n\n\n    def compute_similarity(self,  **args):\n\n        return self.compute_similarity_object.compute_similarity(**args)\n\n\nclass Compute_Similarity_Euclidean:\n\n    def __init__(self, dataMatrix, topK=100, shrink = 0, normalize=False, normalize_avg_row=False,\n                 similarity_from_distance_mode =\"lin\", row_weights = None, **args):\n        \"\"\"\n        Computes the euclidean similarity on the columns of dataMatrix\n        If it is computed on URM=|users|x|items|, pass the URM as is.\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n        :param dataMatrix:\n        :param topK:\n        :param normalize\n        :param row_weights:         Multiply the values in each row by a specified value. Array\n        :param similarity_from_distance_mode:       \"exp\"   euclidean_similarity = 1/(e ^ euclidean_distance)\n                                                    \"lin\"        euclidean_similarity = 1/(1 + euclidean_distance)\n                                                    \"log\"        euclidean_similarity = 1/(1 + euclidean_distance)\n        :param args:                accepts other parameters not needed by the current object\n        \"\"\"\n\n        super(Compute_Similarity_Euclidean, self).__init__()\n\n        self.shrink = shrink\n        self.normalize = normalize\n        self.normalize_avg_row = normalize_avg_row\n\n        self.n_rows, self.n_columns = dataMatrix.shape\n        self.TopK = min(topK, self.n_columns)\n\n        self.dataMatrix = dataMatrix.copy()\n\n        self.similarity_is_exp = False\n        self.similarit_is_lin = False\n        self.similarity_is_log = False\n\n        if similarity_from_distance_mode == \"exp\":\n            self.similarity_is_exp = True\n        elif similarity_from_distance_mode == \"lin\":\n            self.similarity_is_lin = True\n        elif similarity_from_distance_mode == \"log\":\n            self.similarity_is_log = True\n        else:\n            raise ValueError(\"Compute_Similarity_Euclidean: value for parameter 'mode' not recognized.\"\n                             \" Allowed values are: 'exp', 'lin', 'log'.\"\n                             \" Passed value was '{}'\".format(similarity_from_distance_mode))\n\n        self.use_row_weights = False\n\n        if row_weights is not None:\n\n            if dataMatrix.shape[0] != len(row_weights):\n                raise ValueError(\"Compute_Similarity_Euclidean: provided row_weights and dataMatrix have different number of rows.\"\n                                 \"row_weights has {} rows, dataMatrix has {}.\".format(len(row_weights), dataMatrix.shape[0]))\n\n            self.use_row_weights = True\n            self.row_weights = row_weights.copy()\n            self.row_weights_diag = sps.diags(self.row_weights)\n\n            self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Recommenders**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# BaseRecommender.py\n# ------------------------------------------------------------------\n\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\nimport numpy as np\n\n# from utils.compute_similarity import check_matrix\n\nclass BaseRecommender(object):\n    \"\"\"Abstract BaseRecommender\"\"\"\n\n    RECOMMENDER_NAME = \"Recommender_Base_Class\"\n\n    def __init__(self, URM_train, verbose=True):\n\n        super(BaseRecommender, self).__init__()\n\n        self.URM_train = check_matrix(URM_train.copy(), 'csr', dtype=np.float32)\n        self.URM_train.eliminate_zeros()\n\n        self.n_users, self.n_items = self.URM_train.shape\n        self.verbose = verbose\n\n        self.filterTopPop = False\n        self.filterTopPop_ItemsID = np.array([], dtype=np.int)\n\n        self.items_to_ignore_flag = False\n        self.items_to_ignore_ID = np.array([], dtype=np.int)\n\n        self._cold_user_mask = np.ediff1d(self.URM_train.indptr) == 0\n\n        if self._cold_user_mask.any():\n            self._print(\"URM Detected {} ({:.2f} %) cold users.\".format(\n                self._cold_user_mask.sum(), self._cold_user_mask.sum()/self.n_users*100))\n\n\n        self._cold_item_mask = np.ediff1d(self.URM_train.tocsc().indptr) == 0\n\n        if self._cold_item_mask.any():\n            self._print(\"URM Detected {} ({:.2f} %) cold items.\".format(\n                self._cold_item_mask.sum(), self._cold_item_mask.sum()/self.n_items*100))\n\n\n    def _get_cold_user_mask(self):\n        return self._cold_user_mask\n\n    def _get_cold_item_mask(self):\n        return self._cold_item_mask\n\n\n    def _print(self, string):\n        if self.verbose:\n            print(\"{}: {}\".format(self.RECOMMENDER_NAME, string))\n\n    def fit(self):\n        pass\n\n    def get_URM_train(self):\n        return self.URM_train.copy()\n\n    def set_items_to_ignore(self, items_to_ignore):\n        self.items_to_ignore_flag = True\n        self.items_to_ignore_ID = np.array(items_to_ignore, dtype=np.int)\n\n    def reset_items_to_ignore(self):\n        self.items_to_ignore_flag = False\n        self.items_to_ignore_ID = np.array([], dtype=np.int)\n\n\n    #########################################################################################################\n    ##########                                                                                     ##########\n    ##########                     COMPUTE AND FILTER RECOMMENDATION LIST                          ##########\n    ##########                                                                                     ##########\n    #########################################################################################################\n\n\n    def _remove_TopPop_on_scores(self, scores_batch):\n        scores_batch[:, self.filterTopPop_ItemsID] = -np.inf\n        return scores_batch\n\n\n    def _remove_custom_items_on_scores(self, scores_batch):\n        scores_batch[:, self.items_to_ignore_ID] = -np.inf\n        return scores_batch\n\n\n    def _remove_seen_on_scores(self, user_id, scores):\n\n        assert self.URM_train.getformat() == \"csr\", \"Recommender_Base_Class: URM_train is not CSR, this will cause errors in filtering seen items\"\n\n        seen = self.URM_train.indices[self.URM_train.indptr[user_id]:self.URM_train.indptr[user_id + 1]]\n\n        scores[seen] = -np.inf\n        return scores\n\n\n    def _compute_item_score(self, user_id_array, items_to_compute = None):\n        \"\"\"\n        :param user_id_array:       array containing the user indices whose recommendations need to be computed\n        :param items_to_compute:    array containing the items whose scores are to be computed.\n                                        If None, all items are computed, otherwise discarded items will have as score -np.inf\n        :return:                    array (len(user_id_array), n_items) with the score.\n        \"\"\"\n        raise NotImplementedError(\"BaseRecommender: compute_item_score not assigned for current recommender, unable to compute prediction scores\")\n\n\n    def recommend(self, user_id_array, cutoff = None, remove_seen_flag=True, items_to_compute = None,\n                  remove_top_pop_flag = False, remove_custom_items_flag = False, return_scores = False):\n\n        # If is a scalar transform it in a 1-cell array\n        if np.isscalar(user_id_array):\n            user_id_array = np.atleast_1d(user_id_array)\n            single_user = True\n        else:\n            single_user = False\n\n        if cutoff is None:\n            cutoff = self.URM_train.shape[1] - 1\n\n        # Compute the scores using the model-specific function\n        # Vectorize over all users in user_id_array\n        scores_batch = self._compute_item_score(user_id_array, items_to_compute=items_to_compute)\n\n\n        for user_index in range(len(user_id_array)):\n\n            user_id = user_id_array[user_index]\n\n            if remove_seen_flag:\n                scores_batch[user_index,:] = self._remove_seen_on_scores(user_id, scores_batch[user_index, :])\n\n            # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n            # - Partition the data to extract the set of relevant items\n            # - Sort only the relevant items\n            # - Get the original item index\n            # relevant_items_partition = (-scores_user).argpartition(cutoff)[0:cutoff]\n            # relevant_items_partition_sorting = np.argsort(-scores_user[relevant_items_partition])\n            # ranking = relevant_items_partition[relevant_items_partition_sorting]\n            #\n            # ranking_list.append(ranking)\n\n\n        if remove_top_pop_flag:\n            scores_batch = self._remove_TopPop_on_scores(scores_batch)\n\n        if remove_custom_items_flag:\n            scores_batch = self._remove_custom_items_on_scores(scores_batch)\n\n        # relevant_items_partition is block_size x cutoff\n        relevant_items_partition = (-scores_batch).argpartition(cutoff, axis=1)[:,0:cutoff]\n\n        # Get original value and sort it\n        # [:, None] adds 1 dimension to the array, from (block_size,) to (block_size,1)\n        # This is done to correctly get scores_batch value as [row, relevant_items_partition[row,:]]\n        relevant_items_partition_original_value = scores_batch[np.arange(scores_batch.shape[0])[:, None], relevant_items_partition]\n        relevant_items_partition_sorting = np.argsort(-relevant_items_partition_original_value, axis=1)\n        ranking = relevant_items_partition[np.arange(relevant_items_partition.shape[0])[:, None], relevant_items_partition_sorting]\n\n        ranking_list = [None] * ranking.shape[0]\n\n        # Remove from the recommendation list any item that has a -inf score\n        # Since -inf is a flag to indicate an item to remove\n        for user_index in range(len(user_id_array)):\n            user_recommendation_list = ranking[user_index]\n            user_item_scores = scores_batch[user_index, user_recommendation_list]\n\n            not_inf_scores_mask = np.logical_not(np.isinf(user_item_scores))\n\n            user_recommendation_list = user_recommendation_list[not_inf_scores_mask]\n            ranking_list[user_index] = user_recommendation_list.tolist()\n\n\n\n        # Return single list for one user, instead of list of lists\n        if single_user:\n            ranking_list = ranking_list[0]\n\n\n        if return_scores:\n            return ranking_list, scores_batch\n\n        else:\n            return ranking_list\n","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BaseSimilarityMatrixRecommender.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 16/09/2017\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\n# from recommenders.BaseRecommender import BaseRecommender\n# from utils.DataIO import DataIO\nimport numpy as np\n\n\nclass BaseSimilarityMatrixRecommender(BaseRecommender):\n    \"\"\"\n    This class refers to a BaseRecommender KNN which uses a similarity matrix, it provides two function to compute item's score\n    bot for user-based and Item-based models as well as a function to save the W_matrix\n    \"\"\"\n\n    def __init__(self, URM_train, verbose=True):\n        super(BaseSimilarityMatrixRecommender, self).__init__(URM_train, verbose = verbose)\n\n        self._URM_train_format_checked = False\n        self._W_sparse_format_checked = False\n\n\n\n    def _check_format(self):\n\n        if not self._URM_train_format_checked:\n\n            if self.URM_train.getformat() != \"csr\":\n                self._print(\"PERFORMANCE ALERT compute_item_score: {} is not {}, this will significantly slow down the computation.\".format(\"URM_train\", \"csr\"))\n\n            self._URM_train_format_checked = True\n\n        if not self._W_sparse_format_checked:\n\n            if self.W_sparse.getformat() != \"csr\":\n                self._print(\"PERFORMANCE ALERT compute_item_score: {} is not {}, this will significantly slow down the computation.\".format(\"W_sparse\", \"csr\"))\n\n            self._W_sparse_format_checked = True\n\n\n    def save_model(self, folder_path, file_name = None):\n\n        if file_name is None:\n            file_name = self.RECOMMENDER_NAME\n\n        self._print(\"Saving model in file '{}'\".format(folder_path + file_name))\n\n        data_dict_to_save = {\"W_sparse\": self.W_sparse}\n\n        dataIO = DataIO(folder_path=folder_path)\n        dataIO.save_data(file_name=file_name, data_dict_to_save = data_dict_to_save)\n\n        self._print(\"Saving complete\")\n\n\n\n    #########################################################################################################\n    ##########                                                                                     ##########\n    ##########                               COMPUTE ITEM SCORES                                   ##########\n    ##########                                                                                     ##########\n    #########################################################################################################\n\n\nclass BaseItemSimilarityMatrixRecommender(BaseSimilarityMatrixRecommender):\n\n    def _compute_item_score(self, user_id_array, items_to_compute=None):\n        \"\"\"\n        URM_train and W_sparse must have the same format, CSR\n        :param user_id_array:\n        :param items_to_compute:\n        :return:\n        \"\"\"\n\n        self._check_format()\n\n        user_profile_array = self.URM_train[user_id_array]\n\n        if items_to_compute is not None:\n            item_scores = - np.ones((len(user_id_array), self.URM_train.shape[1]), dtype=np.float32)*np.inf\n            item_scores_all = user_profile_array.dot(self.W_sparse).toarray()\n            item_scores[:, items_to_compute] = item_scores_all[:, items_to_compute]\n        else:\n            item_scores = user_profile_array.dot(self.W_sparse).toarray()\n\n        return item_scores\n\n\nclass BaseUserSimilarityMatrixRecommender(BaseSimilarityMatrixRecommender):\n\n    def _compute_item_score(self, user_id_array, items_to_compute=None):\n        \"\"\"\n        URM_train and W_sparse must have the same format, CSR\n        :param user_id_array:\n        :param items_to_compute:\n        :return:\n        \"\"\"\n\n        self._check_format()\n\n        user_weights_array = self.W_sparse[user_id_array]\n\n        if items_to_compute is not None:\n            item_scores = - np.ones((len(user_id_array), self.URM_train.shape[1]), dtype=np.float32)*np.inf\n            item_scores_all = user_weights_array.dot(self.URM_train).toarray()\n            item_scores[:, items_to_compute] = item_scores_all[:, items_to_compute]\n        else:\n            item_scores = user_weights_array.dot(self.URM_train).toarray()\n\n        return item_scores","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# UserKNNCFRecommender.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 23/10/17\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\n# from utils.compute_similarity import check_matrix\n# from recommenders.BaseSimilarityMatrixRecommender import BaseUserSimilarityMatrixRecommender\n\n# from Base.IR_feature_weighting import okapi_BM_25, TF_IDF\nimport numpy as np\n\n# from utils.compute_similarity import Compute_Similarity\n\n\nclass UserKNNCFRecommender(BaseUserSimilarityMatrixRecommender):\n    \"\"\" UserKNN recommender\"\"\"\n\n    RECOMMENDER_NAME = \"UserKNNCFRecommender\"\n\n    FEATURE_WEIGHTING_VALUES = [\"BM25\", \"TF-IDF\", \"none\"]\n\n    def __init__(self, URM_train, verbose = True):\n        super(UserKNNCFRecommender, self).__init__(URM_train, verbose = verbose)\n\n\n\n    def fit(self, topK=50, shrink=100, similarity='cosine', normalize=True, feature_weighting = \"none\", **similarity_args):\n\n        self.topK = topK\n        self.shrink = shrink\n\n        if feature_weighting not in self.FEATURE_WEIGHTING_VALUES:\n            raise ValueError(\"Value for 'feature_weighting' not recognized. Acceptable values are {}, provided was '{}'\".format(self.FEATURE_WEIGHTING_VALUES, feature_weighting))\n\n\n        # if feature_weighting == \"BM25\":\n        #     self.URM_train = self.URM_train.astype(np.float32)\n        #     self.URM_train = okapi_BM_25(self.URM_train.T).T\n        #     self.URM_train = check_matrix(self.URM_train, 'csr')\n        #\n        # elif feature_weighting == \"TF-IDF\":\n        #     self.URM_train = self.URM_train.astype(np.float32)\n        #     self.URM_train = TF_IDF(self.URM_train.T).T\n        #     self.URM_train = check_matrix(self.URM_train, 'csr')\n\n        similarity = Compute_Similarity(self.URM_train.T, shrink=shrink, topK=topK, normalize=normalize, similarity = similarity, **similarity_args)\n\n        self.W_sparse = similarity.compute_similarity()\n        self.W_sparse = check_matrix(self.W_sparse, format='csr')","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ItemKNNCFRecommender.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 23/10/17\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\n# from utils.compute_similarity import check_matrix\n# from recommenders.BaseSimilarityMatrixRecommender import BaseItemSimilarityMatrixRecommender\n\n# from Base.IR_feature_weighting import okapi_BM_25, TF_IDF\nimport numpy as np\n\n# from utils.compute_similarity import Compute_Similarity\n\n\nclass ItemKNNCFRecommender(BaseItemSimilarityMatrixRecommender):\n    \"\"\" ItemKNN recommender\"\"\"\n\n    RECOMMENDER_NAME = \"ItemKNNCFRecommender\"\n\n    FEATURE_WEIGHTING_VALUES = [\"BM25\", \"TF-IDF\", \"none\"]\n\n    def __init__(self, URM_train, verbose = True):\n        super(ItemKNNCFRecommender, self).__init__(URM_train, verbose = verbose)\n\n\n    def fit(self, topK=50, shrink=100, similarity='cosine', normalize=True, feature_weighting = \"none\", **similarity_args):\n\n        self.topK = topK\n        self.shrink = shrink\n\n        if feature_weighting not in self.FEATURE_WEIGHTING_VALUES:\n            raise ValueError(\"Value for 'feature_weighting' not recognized. Acceptable values are {}, provided was '{}'\".format(self.FEATURE_WEIGHTING_VALUES, feature_weighting))\n\n\n        # if feature_weighting == \"BM25\":\n        #     self.URM_train = self.URM_train.astype(np.float32)\n        #     self.URM_train = okapi_BM_25(self.URM_train.T).T\n        #     self.URM_train = check_matrix(self.URM_train, 'csr')\n        #\n        # elif feature_weighting == \"TF-IDF\":\n        #     self.URM_train = self.URM_train.astype(np.float32)\n        #     self.URM_train = TF_IDF(self.URM_train.T).T\n        #     self.URM_train = check_matrix(self.URM_train, 'csr')\n\n        similarity = Compute_Similarity(self.URM_train, shrink=shrink, topK=topK, normalize=normalize, similarity = similarity, **similarity_args)\n\n\n        self.W_sparse = similarity.compute_similarity()\n        self.W_sparse = check_matrix(self.W_sparse, format='csr')","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ItemKNNCBFRecommender.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 23/10/17\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\n# from utils.compute_similarity import check_matrix\n# from recommenders.BaseSimilarityMatrixRecommender import BaseItemSimilarityMatrixRecommender\n\n# from Base.IR_feature_weighting import okapi_BM_25, TF_IDF\nimport numpy as np\n\n# from utils.compute_similarity import Compute_Similarity\n\n\nclass ItemKNNCBFRecommender(BaseItemSimilarityMatrixRecommender):\n    \"\"\" ItemKNN recommender\"\"\"\n\n    RECOMMENDER_NAME = \"ItemKNNCBFRecommender\"\n\n    FEATURE_WEIGHTING_VALUES = [\"BM25\", \"TF-IDF\", \"none\"]\n\n    def __init__(self, URM_train, ICM_train, verbose=True):\n        super(ItemKNNCBFRecommender, self).__init__(URM_train, verbose=verbose)\n\n        self.ICM_train = ICM_train\n\n    def fit(self, topK=50, shrink=100, similarity='cosine', normalize=True, feature_weighting=\"none\",\n            **similarity_args):\n\n        self.topK = topK\n        self.shrink = shrink\n\n        if feature_weighting not in self.FEATURE_WEIGHTING_VALUES:\n            raise ValueError(\n                \"Value for 'feature_weighting' not recognized. Acceptable values are {}, provided was '{}'\".format(\n                    self.FEATURE_WEIGHTING_VALUES, feature_weighting))\n\n        # if feature_weighting == \"BM25\":\n        #     self.ICM_train = self.ICM_train.astype(np.float32)\n        #     self.ICM_train = okapi_BM_25(self.ICM_train)\n        #\n        # elif feature_weighting == \"TF-IDF\":\n        #     self.ICM_train = self.ICM_train.astype(np.float32)\n        #     self.ICM_train = TF_IDF(self.ICM_train)\n\n        similarity = Compute_Similarity(self.ICM_train.T, shrink=shrink, topK=topK, normalize=normalize,\n                                        similarity=similarity, **similarity_args)\n\n        self.W_sparse = similarity.compute_similarity()\n        self.W_sparse = check_matrix(self.W_sparse, format='csr')","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BaseMatrixFactorizationRecommender.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 16/09/2017\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\n# from recommenders.BaseRecommender import BaseRecommender\n# from KNN.ItemKNNCustomSimilarityRecommender import ItemKNNCustomSimilarityRecommender\n\nimport numpy as np\n\n\nclass BaseMatrixFactorizationRecommender(BaseRecommender):\n    \"\"\"\n    This class refers to a BaseRecommender KNN which uses matrix factorization,\n    it provides functions to compute item's score as well as a function to save the W_matrix\n    The prediction for cold users will always be -inf for ALL items\n    \"\"\"\n\n    def __init__(self, URM_train, verbose=True):\n        super(BaseMatrixFactorizationRecommender, self).__init__(URM_train, verbose=verbose)\n\n        self.use_bias = False\n\n    #########################################################################################################\n    ##########                                                                                     ##########\n    ##########                               COMPUTE ITEM SCORES                                   ##########\n    ##########                                                                                     ##########\n    #########################################################################################################\n\n\n    def _compute_item_score(self, user_id_array, items_to_compute = None):\n        \"\"\"\n        USER_factors is n_users x n_factors\n        ITEM_factors is n_items x n_factors\n        The prediction for cold users will always be -inf for ALL items\n        :param user_id_array:\n        :param items_to_compute:\n        :return:\n        \"\"\"\n\n        assert self.USER_factors.shape[1] == self.ITEM_factors.shape[1], \\\n            \"{}: User and Item factors have inconsistent shape\".format(self.RECOMMENDER_NAME)\n\n        assert self.USER_factors.shape[0] > np.max(user_id_array),\\\n                \"{}: Cold users not allowed. Users in trained model are {}, requested prediction for users up to {}\".format(\n                self.RECOMMENDER_NAME, self.USER_factors.shape[0], np.max(user_id_array))\n\n        if items_to_compute is not None:\n            item_scores = - np.ones((len(user_id_array), self.ITEM_factors.shape[0]), dtype=np.float32)*np.inf\n            item_scores[:, items_to_compute] = np.dot(self.USER_factors[user_id_array], self.ITEM_factors[items_to_compute,:].T)\n\n        else:\n            item_scores = np.dot(self.USER_factors[user_id_array], self.ITEM_factors.T)\n\n\n        # No need to select only the specific negative items or warm users because the -inf score will not change\n        if self.use_bias:\n            item_scores += self.ITEM_bias + self.GLOBAL_bias\n            item_scores = (item_scores.T + self.USER_bias[user_id_array]).T\n\n        return item_scores\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PureSVDRecommender.py\n# ------------------------------------------------------------------\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on 14/06/18\n@author: Maurizio Ferrari Dacrema\n\"\"\"\n\n# from recommenders.BaseMatrixFactorizationRecommender import BaseMatrixFactorizationRecommender\n\nfrom sklearn.utils.extmath import randomized_svd\nimport scipy.sparse as sps\n\n\nclass PureSVDRecommender(BaseMatrixFactorizationRecommender):\n    \"\"\" PureSVDRecommender\"\"\"\n\n    RECOMMENDER_NAME = \"PureSVDRecommender\"\n\n    def __init__(self, URM_train, verbose = True):\n        super(PureSVDRecommender, self).__init__(URM_train, verbose = verbose)\n\n\n    def fit(self, num_factors=100, random_seed = None):\n\n        self._print(\"Computing SVD decomposition...\")\n\n        U, Sigma, VT = randomized_svd(self.URM_train,\n                                      n_components=num_factors,\n                                      #n_iter=5,\n                                      random_state = random_seed)\n\n        s_Vt = sps.diags(Sigma)*VT\n\n        self.USER_factors = U\n        self.ITEM_factors = s_Vt.T\n\n        self._print(\"Computing SVD decomposition... Done!\")","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# main.py\n# ------------------------------------------------------------------\n\nimport os, traceback\nimport numpy as np\n\nURM_all = build_URM()\nICM_all = build_ICM()\n# data_manager.get_statistics_URM(URM)\n\n\n######################################################################\n##########                                                  ##########\n##########      TRAINING, EVALUATION AND PREDICTIONS        ##########\n##########                                                  ##########\n######################################################################\n\n\n# URM train/validation/test splitting\n# -----------------------------------\n\n# from Data_manager.Movielens1M.Movielens1MReader import Movielens1MReader\n# from Data_manager.DataSplitter_k_fold_stratified import DataSplitter_Warm_k_fold\n\n# dataset_object = Movielens1MReader()\n# dataSplitter = DataSplitter_Warm_k_fold(dataset_object)\n# dataSplitter.load_data()\n# URM_train, URM_validation, URM_test = dataSplitter.get_holdout_split()\n\nURM_train, URM_test = split_train_validation_random_holdout(URM_all, train_split=0.8)\nURM_train, URM_validation = split_train_validation_random_holdout(URM_train, train_split=0.9)\n\n\n# Tuning parameters\n# -----------------\n\nmetric_to_optimize = \"MAP\"\n\nevaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])\nevaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10, 15])\nevaluator_validation_earlystopping = EvaluatorHoldout(URM_train, cutoff_list=[10], exclude_seen = False) # None \noutput_folder_path = \"result_experiments/\"\n\nn_cases = 8 # 2\nn_random_starts = 5  # 1\n\nsave_model = \"no\"\nallow_weighting = False\nsimilarity_type_list = [\"cosine\"]\n\nICM_name = \"ICM_all\"\n\n# Collaborative recommenders\ncollaborative_algorithm_list = [\n    # Random,\n    # TopPop,\n    # P3alphaRecommender,\n    # RP3betaRecommender,\n    ItemKNNCFRecommender,\n    UserKNNCFRecommender,\n#     MatrixFactorization_BPR_Cython,\n    # MatrixFactorization_FunkSVD_Cython,\n    PureSVDRecommender,\n    # SLIM_BPR_Cython,\n    # SLIMElasticNetRecommender\n]\n\n# Content-based recommenders\ncontent_algorithm_list = [\n    ItemKNNCBFRecommender\n]\n\nrecommender_list = [\n    # Random,\n    # TopPop,\n    # P3alphaRecommender,\n    # RP3betaRecommender,\n    ItemKNNCFRecommender,\n    UserKNNCFRecommender,\n#     MatrixFactorization_BPR_Cython,\n    # MatrixFactorization_FunkSVD_Cython,\n    PureSVDRecommender,\n    # SLIM_BPR_Cython,\n    # SLIMElasticNetRecommender,\n    ItemKNNCBFRecommender\n]\n\n\n# from Utils.PoolWithSubprocess import PoolWithSubprocess\n# import multiprocessing\n#\n# pool = PoolWithSubprocess(processes=int(multiprocessing.cpu_count()), maxtasksperchild=1)\n# resultList = pool.map(runParameterSearch_Collaborative_partial, collaborative_algorithm_list)\n# pool.close()\n# pool.join()\n\n\nprint('\\nRecommender Systems: ')\nfor i, recomm_type in enumerate(recommender_list, start=1):\n    print('{}. {}'.format(i, recomm_type.RECOMMENDER_NAME))\n\nwhile True:\n    try:\n        selected = int(input('\\nSelect a recommender system: '.format(i)))\n        recommender_class = recommender_list[selected - 1]\n        print('\\n ... {} ... '.format(recommender_class.RECOMMENDER_NAME))\n\n        output_file_name_root = \"{}_metadata.zip\".format(recommender_class.RECOMMENDER_NAME)\n\n\n        # Hyperparameters tuning\n        apply_hyperparams_tuning = False\n        \n        if apply_hyperparams_tuning :\n            \n            if recommender_class in collaborative_algorithm_list:\n\n                try:\n                    runParameterSearch_Collaborative(recommender_class=recommender_class,\n                                                     URM_train=URM_train,\n                                                     metric_to_optimize=metric_to_optimize,\n                                                     evaluator_validation=evaluator_validation,\n                                                     evaluator_test=evaluator_test,\n                                                     evaluator_validation_earlystopping=evaluator_validation_earlystopping,\n                                                     output_folder_path=output_folder_path,\n                                                     n_cases=n_cases,\n                                                     n_random_starts=n_random_starts,\n                                                     save_model=save_model,\n                                                     allow_weighting=allow_weighting,\n                                                     similarity_type_list=similarity_type_list)\n\n                    if recommender_class in [ItemKNNCFRecommender, UserKNNCFRecommender]:\n                        similarity_type = similarity_type_list[0]  # KNN Recommenders on similarity_type\n                        output_file_name_root = \"{}_{}_metadata.zip\".format(recommender_class.RECOMMENDER_NAME,\n                                                                            similarity_type)\n\n                except Exception as e:\n                    print(\"On recommender {} Exception {}\".format(recommender_class, str(e)))\n                    traceback.print_exc()\n\n\n            if recommender_class in content_algorithm_list:\n\n                try:\n                    runParameterSearch_Content(recommender_class=recommender_class,\n                                               URM_train=URM_train,\n                                               ICM_object=ICM_all,\n                                               ICM_name=ICM_name,\n                                               n_cases=n_cases,\n                                               n_random_starts=n_random_starts,\n                                               save_model=save_model,\n                                               evaluator_validation=evaluator_validation,\n                                               evaluator_test=evaluator_test,\n                                               metric_to_optimize=metric_to_optimize,\n                                               output_folder_path=output_folder_path,\n                                               allow_weighting=allow_weighting,\n                                               similarity_type_list=similarity_type_list)\n\n                    similarity_type = similarity_type_list[0]  # KNN Recommenders on similarity_type\n                    output_file_name_root = \"{}_{}_{}_metadata.zip\".format(recommender_class.RECOMMENDER_NAME,\n                                                                           ICM_name, similarity_type)\n\n                except Exception as e:\n                    print(\"On recommender {} Exception {}\".format(recommender_class, str(e)))\n                    traceback.print_exc()\n\n                # Load best_parameters for training\n                data_loader = DataIO(folder_path=output_folder_path)\n                search_metadata = data_loader.load_data(output_file_name_root)\n                best_parameters = search_metadata[\"hyperparameters_best\"]  # dictionary with all the fit parameters\n                print(\"best_parameters {}\".format(best_parameters))\n        \n        else:\n           \n            best_parameters = {'topK': 5, 'shrink': 1000, 'similarity': 'cosine', 'normalize': True}\n            \n            \n        # Fit the recommender with the parameters we just learned\n        if recommender_class in content_algorithm_list:\n            # todo: ICM_all or ICM_train?\n            recommender = recommender_class(URM_train, ICM_all)\n        else:\n            recommender = recommender_class(URM_train)\n        \n        recommender.fit(**best_parameters)\n\n        # Evaluate model\n        evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])\n        result_dict, _ = evaluator_test.evaluateRecommender(recommender)\n\n        print(\"{} result_dict MAP {}\".format(recommender_class.RECOMMENDER_NAME, result_dict[10][\"MAP\"]))\n\n        # Make predictions\n        predictions = input('\\nCompute and save top10 predictions?: y - Yes  n - No\\n')\n\n        if predictions == 'y':\n            # Train the model on the whole dataset using tuned params\n            if recommender_class in content_algorithm_list:\n                # todo: ICM_all or ICM_train?\n                recommender = recommender_class(URM_all, ICM_all)\n            else:\n                recommender = recommender_class(URM_all)\n\n            recommender.fit(**best_parameters)\n\n            top_10_items = {}\n            target_user_id_list = get_target_users()\n\n            for user_id in target_user_id_list:\n                item_list = ''\n                for item in range(10):  # recommended_items\n                    item_list = recommender.recommend(user_id, cutoff=10)\n                    item_list = np.array(item_list)  # list to np.array\n                    top_10_items[user_id] = item_list  # .strip() # remove trailing space\n\n            # save predictions on csv file\n            create_csv(top_10_items, recommender_class.RECOMMENDER_NAME)\n\n        break\n\n    except (ValueError, IndexError):\n        print('Error. Please enter number between 1 and {}'.format(i))\n","execution_count":19,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"expected an indented block (<ipython-input-19-4be8d475dfdf>, line 117)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-4be8d475dfdf>\"\u001b[0;36m, line \u001b[0;32m117\u001b[0m\n\u001b[0;31m    try:\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"]}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}